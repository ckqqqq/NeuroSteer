{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "from typing import Tuple\n",
    "from sympy import false\n",
    "from tqdm import tqdm  # For progress bars\n",
    "from log import setup_logging\n",
    "import logging\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer\n",
    "from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "from data_preprocess import load_and_prepare_triple_dataset,load_and_prepare_COT_dataset,load_and_prepare_debate_triple_dataset, load_and_prepare_polite_dataset\n",
    "from utils import load_environment\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with TOPK = 150, LAYER = 6\n"
     ]
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "# 初始化字典\n",
    "args = {\n",
    "    \"task\": \"sentiment\",  # 默认任务为 \"sentiment\"\n",
    "    \"layer\": 6,  # 这里假设LAYER的值为6，可以根据实际循环值动态设置\n",
    "    \"LLM\": \"gpt2-small\",  # 使用 gpt2-small 作为默认 LLM\n",
    "    \"seed\": 42,  # 默认随机种子\n",
    "    \"data_size\": -1,  # 默认数据大小为 -1\n",
    "    \"device\": \"cuda\",  # 使用cuda设备\n",
    "    \"alpha\": 203,  # 默认 alpha 值为 203（根据原脚本设定）\n",
    "    \"method\": \"val_mul\",  # 默认方法为 val_mul\n",
    "    \"topk_cnt\": None,  # TOPK值为 None，后续可以在循环中赋值\n",
    "    \"batch_size\": 32,  # 默认 batch size\n",
    "    \"source\": \"pos\",  # 默认 source 为 \"pos\"\n",
    "    \"target\": \"neg\",  # 默认 target 为 \"neg\"\n",
    "    \"prompt_source\": \"pos\",  # 默认 prompt source 为 \"pos\"\n",
    "    \"prompt_data_size\": 500,  # 默认 prompt 数据大小\n",
    "    \"mean_type\": \"dif_mean\",  # 默认 mean 类型\n",
    "    \"steer_type\": \"last\",  # 默认 steer 类型\n",
    "    \"output_dir\": \"./results/wyh_analysis/\",  # 默认输出目录\n",
    "    \"dataset_path\": \"/home/ckqsudo/code2024/0dataset/baseline-acl/data/sentiment/sst5\",  # 默认数据集路径\n",
    "    \"prompt_path\": \"/home/ckqsudo/code2024/0dataset/baseline-acl/data/prompts/sentiment_prompts-10k\",  # 默认 prompt 路径\n",
    "    \"env_path\": \"/home/ckqsudo/code2024/CKQ_ACL2024/NeuroSteer/.env\",  # 默认环境路径\n",
    "    \"save_no_steer\": 0,  # 默认不保存 steer\n",
    "    \"debug\": 0,  # 默认 debug 为 0\n",
    "    \"use_cache\": 1,  # 默认启用缓存\n",
    "    \"repeat_num\": 1,  # 默认重复次数\n",
    "    \"gen_batch_size\": 16,  # 默认生成 batch size\n",
    "    \"example_prompt\": \"But the lack of financial aid would| I feel\",  # 默认示例 prompt\n",
    "    \"temperature\": 0.9,  # 新增参数: 温度\n",
    "    \"top_p\": 0.3,  # 新增参数: top_p\n",
    "    \"freq_penalty\": 1.0,  # 新增参数: 频率惩罚\n",
    "}\n",
    "\n",
    "# 用循环遍历 TOPK_VALUES\n",
    "# TOPK_VALUES = [600, 1200]\n",
    "for TOPK in [150]:\n",
    "    for LAYER in [6]:  # 共12层，从 0 到 11\n",
    "        # 动态修改字典中的值\n",
    "        args[\"topk_cnt\"] = TOPK\n",
    "        args[\"layer\"] = LAYER\n",
    "\n",
    "        # 打印当前参数设置\n",
    "        print(f\"Running with TOPK = {TOPK}, LAYER = {LAYER}\")\n",
    "        # 在这里调用训练代码，例如：train_model(args)\n",
    "\n",
    "args[\"use_cache\"]=1\n",
    "from argparse import Namespace\n",
    "\n",
    "# 将字典转换为 Namespace 对象\n",
    "args = Namespace(**args)\n",
    "\n",
    "sampling_kwargs = {\n",
    "    \"temperature\": args.temperature,\n",
    "    \"top_p\": args.top_p,\n",
    "    \"freq_penalty\": args.freq_penalty,\n",
    "}\n",
    "sampling_kwargs['verbose']=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 13:32:17,566 [INFO] Logging initialized. Logs will be saved to ./results/sentiment_analysis/sentiment_grid_analysis_2_8/gpt2-small_sentiment_layer_6_datasize_ALL_batchsize32_topK_150/alpha_203_from_pos_to_neg_prompt_pos_mean_dif_mean_steertype_last_device_cuda/execution.log\n",
      "2025-02-23 13:32:17,568 [INFO] Show Hyperparameters: \n",
      "\n",
      "\n",
      "2025-02-23 13:32:17,569 [INFO]   task: sentiment\n",
      "2025-02-23 13:32:17,570 [INFO]   layer: 6\n",
      "2025-02-23 13:32:17,571 [INFO]   LLM: gpt2-small\n",
      "2025-02-23 13:32:17,573 [INFO]   seed: 42\n",
      "2025-02-23 13:32:17,573 [INFO]   data_size: -1\n",
      "2025-02-23 13:32:17,574 [INFO]   device: cuda\n",
      "2025-02-23 13:32:17,575 [INFO]   alpha: 203\n",
      "2025-02-23 13:32:17,576 [INFO]   method: val_mul\n",
      "2025-02-23 13:32:17,578 [INFO]   topk_cnt: 150\n",
      "2025-02-23 13:32:17,578 [INFO]   batch_size: 32\n",
      "2025-02-23 13:32:17,579 [INFO]   source: pos\n",
      "2025-02-23 13:32:17,581 [INFO]   target: neg\n",
      "2025-02-23 13:32:17,582 [INFO]   prompt_source: pos\n",
      "2025-02-23 13:32:17,583 [INFO]   prompt_data_size: 500\n",
      "2025-02-23 13:32:17,584 [INFO]   mean_type: dif_mean\n",
      "2025-02-23 13:32:17,585 [INFO]   steer_type: last\n",
      "2025-02-23 13:32:17,585 [INFO]   output_dir: ./results/sentiment_analysis/sentiment_grid_analysis_2_8\n",
      "2025-02-23 13:32:17,586 [INFO]   dataset_path: /home/ckqsudo/code2024/0dataset/baseline-acl/data/sentiment/sst5\n",
      "2025-02-23 13:32:17,587 [INFO]   prompt_path: /home/ckqsudo/code2024/0dataset/baseline-acl/data/prompts/sentiment_prompts-10k\n",
      "2025-02-23 13:32:17,588 [INFO]   env_path: /home/ckqsudo/code2024/CKQ_ACL2024/Control_Infer/SAE-simple/.env\n",
      "2025-02-23 13:32:17,589 [INFO]   save_no_steer: 0\n",
      "2025-02-23 13:32:17,590 [INFO]   debug: 0\n",
      "2025-02-23 13:32:17,590 [INFO]   use_cache: 1\n",
      "2025-02-23 13:32:17,591 [INFO]   repeat_num: 1\n",
      "2025-02-23 13:32:17,592 [INFO]   gen_batch_size: 16\n",
      "2025-02-23 13:32:17,593 [INFO]   example_prompt: But the lack of financial aid would| I feel\n",
      "2025-02-23 13:32:17,594 [INFO]   temperature: 0.9\n",
      "2025-02-23 13:32:17,594 [INFO]   top_p: 0.3\n",
      "2025-02-23 13:32:17,711 [INFO]   freq_penalty: 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "load_environment(args.env_path)\n",
    "#################################################################################### Experiment Setup \n",
    "# sampling_kwargs = dict(temperature=args.temperature,top_p=0.3, freq_penalty=1.0)\n",
    "# 将 sampling_kwargs 直接构建为字典\n",
    "\n",
    "EXAMPLE_PROMPT_LIST=str(args.example_prompt).split(\"|\")\n",
    "assert isinstance(EXAMPLE_PROMPT_LIST,list) and isinstance(EXAMPLE_PROMPT_LIST[0],str),\"EXAMPLE_PROMPT_LIST必须是list[str]\"\n",
    "assert args.example_prompt!=\"\",\"输入测试prompt\"\n",
    "logging.info(f\"Example prompt: {EXAMPLE_PROMPT_LIST}\")\n",
    "\n",
    "\n",
    "TASK =args.task\n",
    "STEER_TYPE=args.steer_type\n",
    "SOURCE=args.source\n",
    "TARGET=args.target\n",
    "# 调整样本正负性在这里调整 从负样本到正样本还是从正样本()到负样本\n",
    "# pos 代表积极情绪\n",
    "# neg 代表消极情绪\n",
    "ALPHA=args.alpha\n",
    "MAX_NEW_TOKENS=50\n",
    "GEN_BATCH_SIZE = args.gen_batch_size\n",
    "\n",
    "SAVE_NO_STEER=args.save_no_steer\n",
    "DATA_SIZE=args.data_size\n",
    "if DATA_SIZE==-1:\n",
    "    logging.info(\"select all data for activation engineering\")\n",
    "    DATA_SIZE=\"ALL\"\n",
    "\n",
    "CACHE_DIR=os.path.join(args.output_dir,f\"{args.LLM}_{TASK}_layer_{args.layer}_datasize_{DATA_SIZE}_batchsize{args.batch_size}_topK_{args.topk_cnt}\")\n",
    "OUTPUT_DIR=os.path.join(CACHE_DIR,f\"alpha_{args.alpha}_from_{args.source}_to_{args.target}_prompt_{args.prompt_source}_mean_{args.mean_type}_steertype_{args.steer_type}_device_{args.device}\")\n",
    "## 注意，一定要小心CACHE_DIR的路径！！\n",
    "os.makedirs(OUTPUT_DIR,exist_ok=True)\n",
    "\n",
    "# Setup logging\n",
    "setup_logging(OUTPUT_DIR)\n",
    "# Save hyperparameters\n",
    "from utils import params_to_dict\n",
    "hyperparams = params_to_dict(args,is_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 13:45:16,293 [INFO] dataset path /home/ckqsudo/code2024/0dataset/baseline-acl/data/sentiment/sst5\n",
      "2025-02-23 13:45:16,295 [INFO] Loading dataset from ****/home/ckqsudo/code2024/0dataset/baseline-acl/data/sentiment/sst5***\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "2025-02-23 13:45:16,297 [WARNING] Repo card metadata block was not found. Setting CardData to empty.\n",
      "2025-02-23 13:45:16,502 [INFO] Filtering dataset for negative, positive, and neutral samples\n",
      "2025-02-23 13:45:16,510 [INFO] 检查数据量 Selected 3310 negative, 1624 positive, and 3610 neutral samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load Environment Variables\n",
    "# %%\n",
    "####################################################################### LOAD DATASET\n",
    "logging.info(\"dataset path \"+args.dataset_path)\n",
    "if TASK==\"sentiment\":\n",
    "    neg_train_set, pos_train_set, neu_train_set,val_set,test_set = load_and_prepare_triple_dataset(\n",
    "        dataset_path=args.dataset_path, \n",
    "        seed=args.seed, \n",
    "        dataset_name=\"sst5\",\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\"No Supported\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM=args.LLM\n",
    "layer=args.layer\n",
    "device=args.device\n",
    "\n",
    "if \"gpt2-small\" in LLM:\n",
    "    # logging.info(f\"Loading model: {args.LLM} SAE gpt2-small-res-jb\")\n",
    "    sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "        release=f\"{LLM}-res-jb\",\n",
    "        sae_id=f\"blocks.{layer}.hook_resid_pre\",\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    # 设置填充标记为 EOS token\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = HookedTransformer.from_pretrained(LLM, device=device,tokenizer=tokenizer)\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(args.LLM)\n",
    "else:\n",
    "    raise ValueError(\"No Supported\")\n",
    "# return model,sae,tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steer_info[\"pos\"] = neuron_selection(pos_train_set[\"text\"][:DATA_SIZE])·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "####################################################################### LOAD SAE LM TOKENIZER\n",
    "\n",
    "########################################################################## Neuron Selection\n",
    "start_train_time=time.time()\n",
    "def neuron_statistics(batch_latents: Tensor) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "    \"\"\"用于去噪和推理\n",
    "\n",
    "    Args:\n",
    "        batch_latents (Tensor): _description_\n",
    "        top_k_mean (int, optional): _description_. Defaults to 100.\n",
    "        top_k_cnt (int, optional): _description_. Defaults to 100.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Tensor, Tensor, Tensor]: _description_\n",
    "    \"\"\"\n",
    "    SAE_LATENT_SIZE=sae.W_dec.shape[0]\n",
    "    \n",
    "    # logging.info(\"Computing non-zero element counts\") \n",
    "    lat_freq = (batch_latents != 0).sum(dim=(0, 1))# 计算非0激活在对应latent位置的激活频率\n",
    "    # 第一维度 batch_size 第二维度 seq_len 第三维度 sae_latent_size \n",
    "    # logging.info(\"Computing sum of non-zero elements\")\n",
    "    lat_val_sum = batch_latents.sum(dim=(0, 1))# 计算非0激活在对应latent位置的激活值的和（注意这里不能计算均值）\n",
    "    # 第一维度 batch_size 第二维度 seq_len 第三维度 sae_latent_size\n",
    "    # logging.info(\"Computing mean of non-zero elements\")\n",
    "    assert batch_latents.shape[-1]==SAE_LATENT_SIZE==lat_val_sum.shape[0]==lat_freq.shape[0], \"Latent dimension mismatch\"\n",
    "    return {\"latent_frequency\":lat_freq,\"latent_value_sum\":lat_val_sum}\n",
    "def SAE_encoding(sae: SAE, model: HookedTransformer, texts: list, hook_point: str, device: str, batch_size: int) -> list:\n",
    "    \"\"\"\n",
    "    计算 latents，支持批次处理。\n",
    "    Args:\n",
    "        sae (SAE): SAE 实例。\n",
    "        model (HookedTransformer): Transformer 模型实例。\n",
    "        texts (list): 文本列表。\n",
    "        hook_point (str): 钩子点名称。\n",
    "        device (str): 计算设备。\n",
    "        batch_size (int): 每个批次的大小。\n",
    "\n",
    "    Returns:\n",
    "        list: 包含每个批次 latents 的张量列表。\n",
    "    \"\"\"\n",
    "    SAE_LATENT_SIZE=sae.W_dec.shape[0]\n",
    "    logging.info(\"Running model with cache to obtain hidden states\")\n",
    "    lat_freq,lat_val_sum=torch.zeros(SAE_LATENT_SIZE).to(\"cpu\"),torch.zeros(SAE_LATENT_SIZE).to(\"cpu\")# 避免OOM\n",
    "    # 使用 tqdm 显示进度条\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Processing batches\"):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            # logging.info(f\"Batch {i // batch_size + 1}: batch_size {batch_size}\")\n",
    "            try:\n",
    "                sv_logits, cache = model.run_with_cache(batch_texts, prepend_bos=True, device=args.device)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing batch {i // batch_size + 1}: {e}\")\n",
    "                raise ValueError(str([len(i) for i in batch_texts]))\n",
    "            batch_hidden_states = cache[hook_point]\n",
    "            # logging.info(f\"Batch {i // batch_size + 1}: Hidden states shape: {batch_hidden_states.shape}\")\n",
    "            # batch statistics\n",
    "            batch_latents = sae.encode(batch_hidden_states)  # 形状: (batch_size, latent_dim)\n",
    "            batch_info=neuron_statistics(batch_latents)\n",
    "            lat_freq=lat_freq+batch_info[\"latent_frequency\"].to(\"cpu\")\n",
    "            lat_val_sum=lat_val_sum+batch_info[\"latent_value_sum\"].to(\"cpu\")\n",
    "    lat_val_mean=torch.where(lat_freq != 0, lat_val_sum / lat_freq, torch.tensor(0.0, device=\"cpu\"))\n",
    "    logging.info(f\"Total non-zero element shape: {lat_freq.shape}\")\n",
    "    \n",
    "    assert lat_freq.shape[0]==lat_val_mean.shape[0]==sae.W_dec.shape[0], \"sae latent dimension mismatch\"\n",
    "    return {\"latent_frequency\":lat_freq.to(device),\"latent_value_mean\":lat_val_mean.to(device)}\n",
    "# %%\n",
    "# args.steer\n",
    "\n",
    "def neuron_selection(texts:list):\n",
    "    hook_point = sae.cfg.hook_name\n",
    "    # Compute latents with batch processing\n",
    "    lat_info=SAE_encoding(sae, model, texts, hook_point, args.device, args.batch_size)\n",
    "    return {\"latent_value_mean\":lat_info[\"latent_value_mean\"],\"latent_frequency\":lat_info[\"latent_frequency\"]}\n",
    "\n",
    "# %% [markdown]\n",
    "# 26000(SAE稀疏神经元)对应的非零激活神经元激活统计信息，和激活值统计信息\n",
    "\n",
    "\n",
    "# 示例使用\n",
    "def compute_neuron_info():\n",
    "    global DATA_SIZE\n",
    "    # 这里是原先的代码逻辑，计算 steer_info\n",
    "    steer_info = {}\n",
    "    steer_polar_list=[str(args.source),(args.target)]# eg ['pos','neu']\n",
    "    steer_lens={\"pos\":len(pos_train_set[\"text\"]),\"neg\":len(neg_train_set[\"text\"])}\n",
    "    if TASK == 'polite' or TASK == \"sentiment\":\n",
    "        logging.info(f\":>> {TASK} : from \" + args.source + \" to \" + args.target)\n",
    "        # 加入neu\n",
    "        steer_lens[\"neu\"]=len(neu_train_set[\"text\"])\n",
    "        if DATA_SIZE==\"ALL\":\n",
    "            DATA_SIZE=min([steer_lens[steer] for steer in steer_polar_list])\n",
    "        \n",
    "            \n",
    "        if 'pos' in steer_polar_list:\n",
    "            logging.info(f\"positive\")\n",
    "            steer_info[\"pos\"] = neuron_selection(pos_train_set[\"text\"][:DATA_SIZE])\n",
    "        if 'neg' in steer_polar_list:\n",
    "            logging.info(f\"negative\")\n",
    "            steer_info[\"neg\"] = neuron_selection(neg_train_set[\"text\"][:DATA_SIZE])\n",
    "        if 'neu' in steer_polar_list:\n",
    "            logging.info(f\"neutral\")\n",
    "            steer_info[\"neu\"] = neuron_selection(neu_train_set[\"text\"][:DATA_SIZE])\n",
    "    else:\n",
    "        # 不包含neu的处理\n",
    "        if DATA_SIZE==\"ALL\":\n",
    "            DATA_SIZE=min([steer_lens[steer] for steer in steer_polar_list])\n",
    "            \n",
    "        if  TASK == 'debate':\n",
    "            logging.info(\":> Debate :from\" + args.source + \"to\" + args.target)\n",
    "            logging.info(f\"support\")\n",
    "            steer_info[\"pos\"]=neuron_selection(pos_train_set[\"text\"][:DATA_SIZE])\n",
    "            logging.info(f\"oppose\")\n",
    "            steer_info[\"neg\"]=neuron_selection(neg_train_set[\"text\"][:DATA_SIZE])\n",
    "        elif TASK == 'toxicity':\n",
    "            logging.info(f\":) Toxicity :from\" + args.source + \"to\" + args.target)\n",
    "\n",
    "            steer_info[\"pos\"] = neuron_selection(pos_train_set[\"text\"][:DATA_SIZE])\n",
    "            steer_info[\"neg\"] = neuron_selection(neg_train_set[\"text\"][:DATA_SIZE])\n",
    "        elif \"cot\" == TASK:\n",
    "            logging.info(\":) COT: from\" + args.source + \"to\" + args.target)\n",
    "            steer_info[\"pos\"] = neuron_selection(pos_train_set[\"text\"][:DATA_SIZE])\n",
    "            steer_info[\"neg\"] = neuron_selection(neg_train_set[\"text\"][:DATA_SIZE])\n",
    "        else:\n",
    "            raise ValueError(\"Non support Task\")\n",
    "        \n",
    "    assert DATA_SIZE>100,\"训练数据量太少了，看看数据集\"\n",
    "    setattr(args, 'real_data_size_for_train', DATA_SIZE)\n",
    "    return steer_info\n",
    "\n",
    "# 缓存到文件里面\n",
    "from utils import load_or_cache_neuron_info\n",
    "steer_info = load_or_cache_neuron_info(CACHE_DIR=CACHE_DIR,args=args,cache_filename=f\"steer_info_cache_of_{args.LLM}_l{args.layer}.pkl\", compute_func=compute_neuron_info)\n",
    "\n",
    "end_train_time=time.time()\n",
    "\n",
    "# %%\n",
    "assert TARGET in steer_info.keys() and SOURCE in steer_info.keys(),str(steer_info.keys())+\"请检查source和target是否正确\"\n",
    "assert bool(torch.all((steer_info[TARGET][\"latent_value_mean\"]-steer_info[SOURCE][\"latent_value_mean\"])==0))==False,\"数据库读取有问题,请检查很可能neg pos写混了\"\n",
    "assert torch.all(steer_info[TARGET][\"latent_value_mean\"]>=0),\"所有SAE的激活需要大于d等于0（maybe）\"\n",
    "\n",
    "logging.info(f\"转向方向 dif_{TARGET}-{SOURCE}_relu\")\n",
    "\n",
    "\n",
    "############################################################### denoising\n",
    "\"\"\"\n",
    "nz_cnt: 神经元被激活的次数\n",
    "nz_mean: 神经元被激活后的平均值\n",
    "nz_mean_pos: 正样本神经元被激活后的平均值\n",
    "\"\"\"\n",
    "steer_info[f\"dif_{TARGET}-{SOURCE}_relu\"]={\"latent_frequency\":torch.relu(steer_info[TARGET][\"latent_frequency\"]-steer_info[SOURCE][\"latent_frequency\"]),\"latent_value_mean\":torch.relu(steer_info[TARGET][\"latent_value_mean\"]-steer_info[SOURCE][\"latent_value_mean\"]),\"target_nz_mean\":torch.relu(steer_info[TARGET][\"latent_value_mean\"])}\n",
    "top_k=args.topk_cnt\n",
    "_,steer_indices=torch.topk(steer_info[f\"dif_{TARGET}-{SOURCE}_relu\"][\"latent_frequency\"],top_k)\n",
    "\n",
    "################################################################ Delta h\n",
    "\n",
    "# %%\n",
    "# 假设 steer_info[f\"dif_{b}-{a}_relu\"][\"latent_frequency\"] 是一个 NumPy 数组\n",
    "# lat_freq = steer_info[f\"dif_{TARGET}-{SOURCE}_relu\"][\"latent_frequency\"]\n",
    "# 先获取非零元素的索引\n",
    "# lat_acti_indices = np.nonzero(lat_freq)\n",
    "assert torch.all(steer_info[f\"dif_{TARGET}-{SOURCE}_relu\"][\"latent_frequency\"] == 0)==False,\"latent_frequency全为0元素读取有问题\"\n",
    "logging.info(\"sae cfg.hook_name 挂载名称: \"+str(sae.cfg.hook_name)) #挂载名称\n",
    "# %%\n",
    "steer_info[f\"dif_{TARGET}-{SOURCE}_relu\"][\"latent_frequency\"].shape\n",
    "steer_info[f\"dif_{TARGET}-{SOURCE}_relu\"][\"latent_value_mean\"][steer_indices]# 这里有0,没有负数比较正常\n",
    "\n",
    "\n",
    "def SAE_decoding(sae: SAE, indices: Tensor, latent_value_mean: Tensor, method: str = \"val_mul\",is_norm:int=0) -> Tensor:\n",
    "    assert is_norm in [0,1] and method in [\"val_mul\"], \"Invalid arguments\"\n",
    "    delta_h = torch.zeros(sae.W_dec.shape[1], device=sae.W_dec.device)\n",
    "    for idx in indices:\n",
    "        delta_h += latent_value_mean[idx].item() * sae.W_dec[idx]\n",
    "    # logging.info(f\"Steering vectors computed with shape: {delta_matrix.shape}\")\n",
    "    if is_norm==1:\n",
    "        norm = torch.norm(delta_h, p='fro')  # 计算 Frobenius 范数\n",
    "        delta_h = delta_h / norm\n",
    "        logging.info(f\"Steering vectors normalized with L2 norm: {norm} 归一化矩阵大小\")\n",
    "    return delta_h\n",
    "\n",
    "if args.mean_type==\"dif_mean\":\n",
    "    delta_h=SAE_decoding(sae,indices=steer_indices,latent_value_mean=steer_info[f\"dif_{TARGET}-{SOURCE}_relu\"][\"latent_value_mean\"],method=\"val_mul\",is_norm=args.is_norm_delta_matrix)\n",
    "elif args.mean_type==\"tar_mean\":\n",
    "    delta_h=SAE_decoding(sae,indices=steer_indices,latent_value_mean=steer_info[f\"dif_{TARGET}-{SOURCE}_relu\"][\"target_nz_mean\"],method=\"val_mul\",is_norm=args.is_norm_delta_matrix)\n",
    "else:\n",
    "    raise ValueError(\"Unsupported\")\n",
    "\n",
    "\n",
    "# %%\n",
    "if args.debug==1:\n",
    "    logging.info(\"delta_matrix: \"+str(delta_h[:5])) #理论上这里有正有负比较正常\n",
    "# %% [markdown]\n",
    "# # 这里得到的就是delta_matricx\n",
    "# Example prompt from the selected set\n",
    "# example_prompt = \"What really matters is that they know\"\n",
    "# example_prompt=\"\"\" Q: Cody goes to the store and buys $40 worth of stuff.  The taxes were 5%.  After taxes, he got an $8 discount.  Cody and his friend split the final price equally. How much did Cody pay?\n",
    "# A:\"\"\"\n",
    "\n",
    "\n",
    "########################################################### apply_intervention (add delta h when generate)\n",
    "# Generate without steering\n",
    "\n",
    "from intervention_generation import run_generate\n",
    "logging.info(\"Generating texts **without** steering... \")\n",
    "logging.info(\"无转向结果\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_texts_no_steer = run_generate(\n",
    "        prompts=EXAMPLE_PROMPT_LIST, \n",
    "        sampling_kwargs=sampling_kwargs,\n",
    "        sae=sae,\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        MAX_NEW_TOKENS=MAX_NEW_TOKENS,\n",
    "        repeat_num=3,\n",
    "        steer_type=\"\",\n",
    "        steer_on=False,\n",
    "        alpha=0,\n",
    "        delta_h=None,\n",
    "        show_res=True)\n",
    "logging.info(\"干预之后的结果\")\n",
    "# bef,aft=args.steer.split(\"-\")\n",
    "logging.info(f\"干预方向{SOURCE}->{TARGET},礼貌任务下，neg=impolite，情感任务下 pos=积极情感\")\n",
    "logging.info(\"** Generating texts with steering... Target **\")\n",
    "logging.info(f\"form {SOURCE} to {TARGET}\")\n",
    "logging.info(\"转向结果\")\n",
    "generated_texts_with_steer = run_generate(\n",
    "    prompts=EXAMPLE_PROMPT_LIST, \n",
    "    sampling_kwargs=sampling_kwargs,\n",
    "    sae=sae,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    MAX_NEW_TOKENS=MAX_NEW_TOKENS,\n",
    "    repeat_num=3,\n",
    "    steer_on=True,\n",
    "    alpha=ALPHA,\n",
    "    steer_type=STEER_TYPE,\n",
    "    delta_h=delta_h,\n",
    "    show_res=True)\n",
    "\n",
    "# Combine generated texts\n",
    "# all_generated_texts = generated_texts_no_steer + generated_texts_with_steer\n",
    "\n",
    "# %% [markdown]\n",
    "# # 总结小批量实验进展\n",
    "# * 不礼貌的输出应该有很多疑问句？例如what？ ha？why？\n",
    "# * 而礼貌的输出应该有很多正常的词语\n",
    "# * 积极情感和不积极情感同理\n",
    "# * 从目前的实验来看，负向情感干预+礼貌情感干预表现比较好，可以拿这个做可解释性\n",
    "# * 频率很重要，我选取的latents选了前100频次的激活神经元\n",
    "# * 如果对[0:-1]的区间进行干预，效果异常优秀，生成比较连贯，但是如果对[-1:length]的区间进行干预，效果就很差，生成的词语很零碎\n",
    "# %% [markdown]\n",
    "# # 下面进行的是扭转实验，使用prompt对模型进行诱导，再进行转向\n",
    "\n",
    "# %%\n",
    "################################################################### EVAL ON FULL DATASET\n",
    "\n",
    "import copy\n",
    "# Example prompt from the selected set\n",
    "import jsonlines\n",
    "\n",
    "def eval_on_full_data():\n",
    "    \"\"\"跑全量实验\n",
    "    Raises:\n",
    "        NotImplementedError: _description_\n",
    "    \"\"\"\n",
    "    logging.info(\"Running on full data\")\n",
    "    \n",
    "    if TASK==\"sentiment\":\n",
    "        logging.info(\"Out of Domain: Calculate at A dataset, Evaluate at B dataset\")\n",
    "        from data_preprocess import load_and_prepare_sentiment_prompts\n",
    "        prompts=load_and_prepare_sentiment_prompts(prompt_path=args.prompt_path,task=TASK)\n",
    "    elif TASK==\"polite\":\n",
    "        logging.info(\"In Domain: Calculate at A train dataset, Evaluate at A test dataset\")\n",
    "        from data_preprocess import load_and_prepare_polite_prompts\n",
    "        prompts=load_and_prepare_polite_prompts(test_set=polite_test_set)\n",
    "    elif TASK==\"toxicity\":\n",
    "        logging.info(\"Out of Domain: Calculate at A dataset, Evaluate at B dataset\")\n",
    "        from data_preprocess import load_and_prepare_toxicity_prompts \n",
    "        prompts=load_and_prepare_toxicity_prompts(prompt_path=args.prompt_path,task=TASK)\n",
    "    elif TASK==\"debate\":\n",
    "        logging.info(\"Out of Domain: Calculate at A dataset, Evaluate at B dataset\")\n",
    "        from data_preprocess import load_and_prepare_debate_prompts\n",
    "        prompts=load_and_prepare_debate_prompts(prompt_path=args.prompt_path,task=TASK)\n",
    "    else:\n",
    "        raise NotImplementedError(\"No Supported Task\")    \n",
    "    print(prompts)\n",
    "    assert \"neg\" in prompts,\"prompt steer source (pos/neg) not in prompts, please check the data_preprocess section\"\n",
    "    if args.prompt_source!=\"\":\n",
    "        logging.info(f\"prompt的极性是{args.prompt_source}\")\n",
    "        prompts=prompts[args.prompt_source]\n",
    "    else:\n",
    "        prompts=prompts[SOURCE]\n",
    "    \n",
    "    param={**vars(args),**sampling_kwargs,\"max_new_tokens\":50,\"steer\":f\"from {SOURCE} to {TARGET}\"}\n",
    "    param[\"alpha_recheck\"]=ALPHA\n",
    "    logging.info(f\"Running with alpha: {ALPHA}\")\n",
    "    logging.info(f\"Running with prompt_type: \"+str(param[\"steer\"]))\n",
    "\n",
    "    \n",
    "    # 打开文件（模式为追加模式 'a'）\n",
    "    with jsonlines.open(os.path.join(OUTPUT_DIR,\"params.jsonl\"), mode='w') as writer:\n",
    "        writer.write(param)  # 逐行写入\n",
    "    if args.prompt_data_size!=-1:# 用于小批量网格搜索\n",
    "        prompts = prompts.select(range(args.prompt_data_size))  # Correct slicing for datase\n",
    "        logging.warning(\"截取prompt_datasize\"+str(len(prompts)))\n",
    "    \n",
    "    # 新增批次大小参数（假设从args获取）\n",
    "\n",
    "    with jsonlines.open(os.path.join(OUTPUT_DIR,\"no_steer_gen_res.jsonl\"), mode='w') as no_steer_f:\n",
    "        with jsonlines.open(os.path.join(OUTPUT_DIR,\"steer_gen_res.jsonl\"), mode='w') as steer_f: \n",
    "            # 分批次处理prompts\n",
    "            for batch_idx in tqdm(range(0, len(prompts), GEN_BATCH_SIZE)):\n",
    "                # batch = prompts[batch_idx:batch_idx+GEN_BATCH_SIZE]\n",
    "                batch = prompts.select(range(batch_idx, min(batch_idx + GEN_BATCH_SIZE, len(prompts))))\n",
    "                batch_prompts = [item[\"prompt\"][\"text\"] for item in batch]\n",
    "                \n",
    "                # 无转向生成\n",
    "                if SAVE_NO_STEER == 1:\n",
    "                    with torch.no_grad():\n",
    "                        no_steer_gen_texts_batch = run_generate(\n",
    "                            prompts=batch_prompts,  # 传入批次prompts\n",
    "                            sampling_kwargs=sampling_kwargs,\n",
    "                            sae=sae,\n",
    "                            model=model,\n",
    "                            tokenizer=tokenizer,\n",
    "                            MAX_NEW_TOKENS=MAX_NEW_TOKENS,\n",
    "                            repeat_num=args.repeat_num,\n",
    "                            steer_type=\"\",\n",
    "                            steer_on=False,\n",
    "                            alpha=0,\n",
    "                            delta_h=None,\n",
    "                            show_res=False\n",
    "                        )\n",
    "                    \n",
    "                    # 写入无转向结果\n",
    "                    for i, item in enumerate(batch):\n",
    "                        no_steer_item = copy.deepcopy(item)\n",
    "                        no_steer_item[\"generations\"] = [{\"text\": text} for text in no_steer_gen_texts_batch[i]]\n",
    "                        no_steer_f.write(no_steer_item)\n",
    "\n",
    "                # 转向生成\n",
    "                steered_texts_batch = run_generate(\n",
    "                    prompts=batch_prompts, \n",
    "                    sampling_kwargs=sampling_kwargs,\n",
    "                    sae=sae,\n",
    "                    model=model,\n",
    "                    tokenizer=tokenizer,\n",
    "                    MAX_NEW_TOKENS=MAX_NEW_TOKENS,\n",
    "                    repeat_num=args.repeat_num,\n",
    "                    steer_on=True,\n",
    "                    alpha=ALPHA,\n",
    "                    steer_type=STEER_TYPE,\n",
    "                    delta_h=delta_h,\n",
    "                    show_res=False\n",
    "                )\n",
    "                \n",
    "                # 写入转向结果\n",
    "                for i, item in enumerate(batch):# 遍历每个batch中的元素\n",
    "                    steer_item = copy.deepcopy(item)\n",
    "                    steer_item[\"generations\"] = [{\"text\": text} for text in steered_texts_batch[i]]\n",
    "                    global_idx = batch_idx + i\n",
    "                    if global_idx % 20 == 0:\n",
    "                        logging.info(f\"{TASK}: from {SOURCE} to {TARGET} prompt_set: {SOURCE}\")\n",
    "                        logging.info(steer_item)\n",
    "                    steer_f.write(steer_item)\n",
    "\n",
    "if args.debug==1:\n",
    "    logging.info(f\"debug mode,show example, no full dataset eval\")\n",
    "elif args.debug==0:\n",
    "    if SAVE_NO_STEER==1:\n",
    "        logging.info(\"Provide No Steer Result 提供无干预对照样本\")\n",
    "    eval_on_full_data()\n",
    "else:\n",
    "    raise ValueError(\"debug must be 0 or 1\")\n",
    "# %%\n",
    "logging.info(\"训练时间\"+str(end_train_time-start_train_time))\n",
    "params = params_to_dict(args, is_print=True)\n",
    "logging.info(f\"{TASK}:{SOURCE}->{TARGET}\")\n",
    "with jsonlines.open(os.path.join(OUTPUT_DIR, \"params.jsonl\"), mode='w') as param_file:\n",
    "    param_file.write(params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
