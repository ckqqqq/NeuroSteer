{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,hidden_size,num_heads):\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        self.num_heads=num_heads\n",
    "        self.hidden_size=hidden_size\n",
    "        \n",
    "        ### 初始化 QKV 对应的投影矩阵\n",
    "        self.q_linear= nn.Linear(hidden_size,hidden_size)\n",
    "        self.k_linear= nn.Linear(hidden_size,hidden_size)\n",
    "        self.v_linear= nn.Linear(hidden_size,hidden_size)\n",
    "        ## 输出线性层\n",
    "        self.out_linear=nn.Linear(hidden_size,hidden_size)\n",
    "        \n",
    "    def forward(self,hidden_state,mask=None):\n",
    "        batch_size= hidden_state.size(0)\n",
    "        # 传入一个隐藏层向量\n",
    "        query=self.q_linear(hidden_state)\n",
    "        key=self.k_linear(hidden_state)\n",
    "        value=self.v_linear(hidden_state)\n",
    "        # 分割头部\n",
    "        query=einops.rearrange(query,\"b s (h d) -> b h s d\",h=self.num_heads)\n",
    "        key=einops.rearrange(key,\"b s (h d) -> b h s d\")\n",
    "        value=einops.rearrange(value,\"b s (h d) -> b h s d\")\n",
    "        ## 计算注意力分数 q*k/sqrt(d_k)\n",
    "        # attention score \n",
    "        attention_scores= torch.matmul(query,key.transpose(-2,-1))/torch.sqrt(torch.tensor(self.hidden_size))\n",
    "        \n",
    "        if mask is not None:\n",
    "            attention_scores=attention_scores.masked_fill(mask==0,-1e9)\n",
    "        attention_probs=torch.softmax(attention_scores,dim=-1)# 对最后一层进行处理\n",
    "        output =torch.matmul(attention_probs,value)\n",
    "        output=einops.rearrange(output,\"b h s d -> b s (h d)\")\n",
    "        return self.out_linear(output)\n",
    "         \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
