2025-02-16 11:18:44,950 [INFO] Logging initialized. Logs will be saved to ./results/stance/gpt2-small_debate_layer_6_datasize_ALL_batchsize32_topK_100/alpha_20.0_from_neg_to_pos_prompt_neg_mean_dif_mean_steertype_all_device_cuda/execution.log
2025-02-16 11:18:44,950 [INFO] Show Hyperparameters: 


2025-02-16 11:18:44,950 [INFO]   task: debate
2025-02-16 11:18:44,950 [INFO]   layer: 6
2025-02-16 11:18:44,950 [INFO]   LLM: gpt2-small
2025-02-16 11:18:44,951 [INFO]   seed: 42
2025-02-16 11:18:44,951 [INFO]   data_size: -1
2025-02-16 11:18:44,951 [INFO]   device: cuda
2025-02-16 11:18:44,951 [INFO]   alpha: 20.0
2025-02-16 11:18:44,951 [INFO]   method: val_mul
2025-02-16 11:18:44,951 [INFO]   topk_mean: 100
2025-02-16 11:18:44,951 [INFO]   topk_cnt: 100
2025-02-16 11:18:44,951 [INFO]   batch_size: 32
2025-02-16 11:18:44,951 [INFO]   source: neg
2025-02-16 11:18:44,951 [INFO]   target: pos
2025-02-16 11:18:44,951 [INFO]   prompt_source: neg
2025-02-16 11:18:44,951 [INFO]   prompt_data_size: -1
2025-02-16 11:18:44,951 [INFO]   mean_type: dif_mean
2025-02-16 11:18:44,951 [INFO]   steer_type: all
2025-02-16 11:18:44,951 [INFO]   output_dir: ./results/stance
2025-02-16 11:18:44,951 [INFO]   dataset_path: /home/ckqsudo/code2024/0dataset/baseline-acl/data/debate/StanceSentences
2025-02-16 11:18:44,951 [INFO]   prompt_path: /home/ckqsudo/code2024/0dataset/baseline-acl/data/debate/ibm_debate
2025-02-16 11:18:44,951 [INFO]   env_path: /home/ckqsudo/code2024/CKQ_ACL2024/Control_Infer/SAE-simple/.env
2025-02-16 11:18:44,951 [INFO]   temperature: 0.9
2025-02-16 11:18:44,951 [INFO]   top_p: 0.3
2025-02-16 11:18:44,951 [INFO]   freq_penalty: 1.0
2025-02-16 11:18:44,951 [INFO]   example_prompt: But the lack of financial aid would| The passage of the AI Act will
2025-02-16 11:18:44,951 [INFO]   debug: 1
2025-02-16 11:18:44,951 [INFO]   save_no_steer: 0
2025-02-16 11:18:44,951 [INFO]   is_norm_delta_matrix: 0
2025-02-16 11:18:44,951 [INFO]   use_cache: 0
2025-02-16 11:18:44,951 [INFO]   repeat_num: 2
2025-02-16 11:18:44,951 [INFO]   gen_batch_size: 16
2025-02-16 11:18:44,952 [INFO] HF_ENDPOINT: https://hf-mirror.com
2025-02-16 11:18:44,952 [INFO] dataset path /home/ckqsudo/code2024/0dataset/baseline-acl/data/debate/StanceSentences
2025-02-16 11:18:44,952 [INFO] debatedebatedebatedebatedebatedebatedebatedebatedebatedebate
2025-02-16 11:18:44,952 [INFO] Loading dataset from /home/ckqsudo/code2024/0dataset/baseline-acl/data/debate/StanceSentences
2025-02-16 11:18:45,088 [INFO] Filtering dataset for support and oppose samples
2025-02-16 11:18:45,091 [INFO] Selected 486 support and 486 oppose samples
2025-02-16 11:18:45,091 [INFO] suppoert==positive opposite==negative
2025-02-16 11:18:45,091 [INFO] Loading Model Loading SAE for layer 6 gpt2-small
2025-02-16 11:18:45,091 [INFO] Loading model: gpt2-small SAE gpt2-small-res-jb
2025-02-16 11:18:50,353 [INFO] model architecture for gpt2-small HookedTransformer(
  (embed): Embed()
  (hook_embed): HookPoint()
  (pos_embed): PosEmbed()
  (hook_pos_embed): HookPoint()
  (blocks): ModuleList(
    (0-11): 12 x TransformerBlock(
      (ln1): LayerNormPre(
        (hook_scale): HookPoint()
        (hook_normalized): HookPoint()
      )
      (ln2): LayerNormPre(
        (hook_scale): HookPoint()
        (hook_normalized): HookPoint()
      )
      (attn): Attention(
        (hook_k): HookPoint()
        (hook_q): HookPoint()
        (hook_v): HookPoint()
        (hook_z): HookPoint()
        (hook_attn_scores): HookPoint()
        (hook_pattern): HookPoint()
        (hook_result): HookPoint()
      )
      (mlp): MLP(
        (hook_pre): HookPoint()
        (hook_post): HookPoint()
      )
      (hook_attn_in): HookPoint()
      (hook_q_input): HookPoint()
      (hook_k_input): HookPoint()
      (hook_v_input): HookPoint()
      (hook_mlp_in): HookPoint()
      (hook_attn_out): HookPoint()
      (hook_mlp_out): HookPoint()
      (hook_resid_pre): HookPoint()
      (hook_resid_mid): HookPoint()
      (hook_resid_post): HookPoint()
    )
  )
  (ln_final): LayerNormPre(
    (hook_scale): HookPoint()
    (hook_normalized): HookPoint()
  )
  (unembed): Unembed()
) GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	50256: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),
}
)
2025-02-16 11:18:50,353 [INFO] 缓存 ./results/stance/gpt2-small_debate_layer_6_datasize_ALL_batchsize32_topK_100/steer_info_cache_of_gpt2-small_l6.pkl 不存在，缓存 steer_info
2025-02-16 11:18:50,357 [INFO] :> Debate :fromnegtopos
2025-02-16 11:18:50,357 [INFO] support
2025-02-16 11:18:50,358 [INFO] Running model with cache to obtain hidden states
2025-02-16 11:18:51,058 [INFO] Total non-zero element shape: torch.Size([24576])
2025-02-16 11:18:51,058 [INFO] oppose
2025-02-16 11:18:51,061 [INFO] Running model with cache to obtain hidden states
2025-02-16 11:18:51,610 [INFO] Total non-zero element shape: torch.Size([24576])
2025-02-16 11:18:51,612 [INFO] steer_info 已保存到缓存 ./results/stance/gpt2-small_debate_layer_6_datasize_ALL_batchsize32_topK_100/steer_info_cache_of_gpt2-small_l6.pkl
2025-02-16 11:18:51,620 [INFO] 转向方向 dif_pos-neg_relu
2025-02-16 11:18:51,642 [INFO] sae cfg.hook_name 挂载名称: blocks.6.hook_resid_pre
