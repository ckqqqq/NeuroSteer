2025-01-22 12:49:34,510 [INFO] Logging initialized. Logs will be saved to ./results/alpha_30_from_sup_to_opp_datasize_ALL_layer_6_mean_dif_mean_steertype_last_device_cuda_batchsize32/execution.log
2025-01-22 12:49:34,510 [INFO] Hyperparameters:
2025-01-22 12:49:34,510 [INFO]   task: debate
2025-01-22 12:49:34,510 [INFO]   layer: 6
2025-01-22 12:49:34,510 [INFO]   LLM: gpt2-small
2025-01-22 12:49:34,510 [INFO]   seed: 42
2025-01-22 12:49:34,510 [INFO]   data_size: ALL
2025-01-22 12:49:34,510 [INFO]   device: cuda
2025-01-22 12:49:34,510 [INFO]   alpha: 30
2025-01-22 12:49:34,510 [INFO]   steer: opp-sup
2025-01-22 12:49:34,510 [INFO]   method: val_mul
2025-01-22 12:49:34,510 [INFO]   topk_mean: 100
2025-01-22 12:49:34,510 [INFO]   topk_cnt: 100
2025-01-22 12:49:34,510 [INFO]   topp: 0.1
2025-01-22 12:49:34,510 [INFO]   temperature: 1.0
2025-01-22 12:49:34,510 [INFO]   batch_size: 32
2025-01-22 12:49:34,510 [INFO]   source: sup
2025-01-22 12:49:34,510 [INFO]   target: opp
2025-01-22 12:49:34,510 [INFO]   mean_type: dif_mean
2025-01-22 12:49:34,511 [INFO]   steer_type: last
2025-01-22 12:49:34,511 [INFO]   debug: False
2025-01-22 12:49:34,511 [INFO]   output_dir: ./results
2025-01-22 12:49:34,511 [INFO]   dataset_path: /home/ckqsudo/code2024/0dataset/baseline-acl/data/debate/StanceSentences
2025-01-22 12:49:34,511 [INFO]   prompt_path: /home/ckqsudo/code2024/0dataset/baseline-acl/data/debate/ibm_debate
2025-01-22 12:49:34,511 [INFO]   env_path: /home/ckqsudo/code2024/CKQ_ACL2024/Control_Infer/SAE-simple/.env
2025-01-22 12:49:34,511 [INFO]   save_compared: False
2025-01-22 12:49:34,511 [INFO] HF_ENDPOINT: https://hf-mirror.com
2025-01-22 12:49:34,511 [INFO] dataset path /home/ckqsudo/code2024/0dataset/baseline-acl/data/debate/StanceSentences
2025-01-22 12:49:34,511 [INFO] debatedebatedebatedebatedebatedebatedebatedebatedebatedebate
2025-01-22 12:49:34,511 [INFO] Loading dataset from /home/ckqsudo/code2024/0dataset/baseline-acl/data/debate/StanceSentences
2025-01-22 12:49:34,673 [INFO] Filtering dataset for negative, positive, and neutral samples
2025-01-22 12:49:34,677 [INFO] Selected 486 support and 486 oppose samples
2025-01-22 12:49:34,678 [INFO] Loading model: gpt2-small
2025-01-22 12:49:51,779 [INFO] Loading SAE for layer 6
2025-01-22 12:49:53,666 [INFO] fromsuptoopp
2025-01-22 12:49:53,667 [INFO] support
2025-01-22 12:49:53,669 [INFO] Running model with cache to obtain hidden states
2025-01-22 12:49:53,670 [INFO] Batch 1: batch_size 32
2025-01-22 12:49:53,837 [ERROR] Error processing batch 1: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 12.50 MiB is free. Process 4036162 has 15.14 GiB memory in use. Process 422339 has 6.09 GiB memory in use. Including non-PyTorch memory, this process has 2.38 GiB memory in use. Of the allocated memory 1.80 GiB is allocated by PyTorch, and 130.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
