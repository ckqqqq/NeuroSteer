{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "import argparse\n",
    "import os\n",
    "import logging\n",
    "from typing import Tuple\n",
    "import json\n",
    "from log import setup_logging\n",
    "from tqdm import tqdm  # For progress bars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE\n",
    "# from sae_lens.analysis.neuronpedia_integration import get_neuronpedia_quick_list\n",
    "# from sae_lens.analysis.feature_statistics import (\n",
    "#     get_all_stats_dfs,\n",
    "#     get_W_U_W_dec_stats_df,\n",
    "# )\n",
    "# from sae_lens.analysis.tsea import (\n",
    "#     get_enrichment_df,\n",
    "#     manhattan_plot_enrichment_scores,\n",
    "#     plot_top_k_feature_projections_by_token_and_category,\n",
    "#     get_baby_name_sets,\n",
    "#     get_letter_gene_sets,\n",
    "#     generate_pos_sets,\n",
    "#     get_test_gene_sets,\n",
    "#     get_gene_set_from_regex,\n",
    "# )\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "# import plotly_express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 进行基础情感干预实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "task=\"sentiment\"\n",
    "if task==\"sentiment\":\n",
    "    args_dict = {\n",
    "        \"layer\": 6,  # Example layer number to analyze\n",
    "        \"LLM\": \"gpt2-small\",\n",
    "        \"dataset_path\": \"/home/ckqsudo/code2024/0dataset/baseline-acl/data/sentiment/sst5\",\n",
    "        \"prompt_path\":\"/home/ckqsudo/code2024/0dataset/baseline-acl/data/prompts/sentiment_prompts-10k\",\n",
    "        \"output_dir\": \"./results\",\n",
    "        \"env_path\": \"/home/ckqsudo/code2024/CKQ_ACL2024/Control_Infer/SAE-simple/.env\",\n",
    "        \"task\":\"sentiment\",# “sentiment”,\"cot\",\"polite\"\n",
    "        \"seed\": 42,\n",
    "        \"data_size\": 1000,\n",
    "        \"device\": \"cpu\",  # Options: \"cpu\", \"cuda\", \"mps\", \"auto\"\n",
    "        \"alpha\": 100, # 这个alpha后面慢慢调节\n",
    "        \"steer\": \"pos-neg\",  # Options: \"pos\", \"neg\", \"neu\",\"pos-neg\",\"cot-direct\"\n",
    "        \"method\": \"val_mul\",  # Options: \"mean\", \"val_mul\" 用val_mul会比较好\n",
    "        \"topk_mean\": 100, # 选取前topk 个均值激活，这个效果一般，会导致很多如what？why？这种被激活\n",
    "        \"topk_cnt\": 100, # 选取前topk个频率激活，目前默认这个，效果很好\n",
    "        \"batch_size\": 32 # 这个好像没用上\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 进行COT相关实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "if task==\"cot\":\n",
    "    args_dict = {\n",
    "        \"layer\": 6,  # Example layer number to analyze\n",
    "        \"LLM\": \"gpt2-small\",\n",
    "        \"dataset_path\": \"/home/ckqsudo/code2024/0dataset/ACL_useful_dataset/math/COT_GSM8k\",\n",
    "        \"output_dir\": \"./results\",\n",
    "        \"env_path\": \"/home/ckqsudo/code2024/CKQ_ACL2024/Control_Infer/SAE-simple/.env\",\n",
    "        \"seed\": 42,\n",
    "        \"data_size\": 1000,\n",
    "        \"device\": \"cpu\",  # Options: \"cpu\", \"cuda\", \"mps\", \"auto\"\n",
    "        \"alpha\": 100,\n",
    "        \"steer\": \"cot-direct\",  # Options: \"pos\", \"neg\", \"neu\",\"pos-neg\",\"cot-direct\"\n",
    "        \"method\": \"val_mul\",  # Options: \"mean\", \"val_mul\"\n",
    "        \"topk_mean\": 100,\n",
    "        \"topk_cnt\": 100,\n",
    "        \"batch_size\": 32\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 进行礼貌实验\n",
    "/home/ckqsudo/code2024/0dataset/ACL_useful_dataset/style_transfer/politeness-corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "if task==\"polite\":\n",
    "    args_dict = {\n",
    "        \"layer\": 6,  # Example layer number to analyze\n",
    "        \"LLM\": \"gpt2-small\",\n",
    "        \"dataset_path\": \"/home/ckqsudo/code2024/0dataset/ACL_useful_dataset/style_transfer/politeness-corpus\",\n",
    "        \"output_dir\": \"./results\",\n",
    "        \"env_path\": \"/home/ckqsudo/code2024/CKQ_ACL2024/Control_Infer/SAE-simple/.env\",\n",
    "        \"seed\": 42,\n",
    "        \"data_size\": 1000,\n",
    "        \"device\": \"cpu\",  # Options: \"cpu\", \"cuda\", \"mps\", \"auto\"\n",
    "        \"alpha\": 100,\n",
    "        \"steer\": \"polite-impolite\",  # Options: \"pos\", \"neg\", \"neu\",\"pos-neg\",\"cot-direct\"\n",
    "        \"method\": \"val_mul\",  # Options: \"mean\", \"val_mul\"\n",
    "        \"topk_mean\": 100,\n",
    "        \"topk_cnt\": 100,\n",
    "        \"batch_size\": 32\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration and Hyperparameters\n",
    "# 将字典转换为 argparse.Namespace 对象\n",
    "args = argparse.Namespace(**args_dict)\n",
    "# 测试访问属性\n",
    "print(args.layer)  # 输出: 10\n",
    "print(args.LLM)  # 输出: gpt2-small\n",
    "print(args.output_dir)  # 输出: ./results\n",
    "print(args.steer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging Setup\n",
    "import os\n",
    "from log import setup_logging\n",
    "import logging\n",
    "# Create output directory base path\n",
    "output_dir_base = os.path.join(\n",
    "    args.output_dir,\n",
    "    f\"LLM_{args.LLM}_layer_{args.layer}_steer_{args.steer}_alpha_{args.alpha}_cnt_{args.topk_cnt}_mean{args.topk_mean}\"\n",
    ")\n",
    "\n",
    "# Setup logging\n",
    "setup_logging(output_dir_base)\n",
    "\n",
    "# Save hyperparameters\n",
    "hyperparams = args_dict\n",
    "\n",
    "# Log hyperparameters\n",
    "logging.info(\"Hyperparameters:\")\n",
    "for key, value in hyperparams.items():\n",
    "    logging.info(f\"  {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Environment Variables\n",
    " \n",
    "def load_environment(env_path: str):\n",
    "    load_dotenv(env_path)\n",
    "    hf_endpoint = os.getenv('HF_ENDPOINT', 'https://hf-mirror.com')\n",
    "    logging.info(f\"HF_ENDPOINT: {hf_endpoint}\")\n",
    "\n",
    "load_environment(args.env_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def load_and_prepare_triple_dataset(dataset_path: str,dataset_name:str, seed: int, num_samples: int):\n",
    "    \"\"\"\n",
    "    支持positive\\neutral\\negative三元组数据类型，例如 sst5，polite数据集和multi-class数据集\n",
    "\n",
    "    Args:\n",
    "        dataset_path (str): _description_\n",
    "        dataset_name : \"sst5\",\"multiclass\",\"polite\"\n",
    "        seed (int): _description_\n",
    "        num_samples (int): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    assert dataset_name in [\"sst5\",\"multiclass\",\"polite\"]\n",
    "    if dataset_name in [\"sst5\"]:\n",
    "        neu_label=2 # 中性情感对应的label\n",
    "        assert \"sst5\" in dataset_path\n",
    "    elif  dataset_name in [\"polite\",\"multiclass\"]:\n",
    "        neu_label=1\n",
    "    logging.info(f\"Loading dataset from ****{dataset_path}***\")\n",
    "    dataset = load_dataset(dataset_path)\n",
    "    dataset[\"train\"] = dataset['train'].shuffle(seed=seed)\n",
    "\n",
    "    logging.info(\"Filtering dataset for negative, positive, and neutral samples\")\n",
    "    neg_train_set = dataset['train'].filter(lambda example: example['label'] < neu_label).select(range(num_samples))\n",
    "    pos_train_set = dataset['train'].filter(lambda example: example['label'] == neu_label).select(range(num_samples))\n",
    "    neu_train_set = dataset['train'].filter(lambda example: example['label'] > neu_label ).select(range(num_samples))\n",
    "\n",
    "    logging.info(f\"Selected {len(neg_train_set)} negative, {len(pos_train_set)} positive, and {len(neu_train_set)} neutral samples\")\n",
    "    print(dataset)\n",
    "    if dataset_name in [\"sst5\"]:\n",
    "        val_set=dataset['validation']\n",
    "    else:\n",
    "        raise ValueError(\"没写呢\")\n",
    "    test_set=dataset[\"test\"]\n",
    "    return neg_train_set, pos_train_set, neu_train_set,val_set,test_set\n",
    "def load_and_prepare_COT_dataset(dataset_path:str,seed:int,num_samples:int):\n",
    "    logging.info(f\"Loading dataset from {dataset_path}\")\n",
    "    dataset = load_dataset(dataset_path)\n",
    "    dataset[\"train\"] = dataset['train'].shuffle(seed=seed)\n",
    "    logging.info(\"Filtering dataset for COT\")\n",
    "    # 定义一个函数来提取答案\n",
    "    def extract_answer(text):\n",
    "        # 使用正则表达式提取答案\n",
    "        match = re.search(r'#### ([-+]?\\d*\\.?\\d+/?\\d*)', text)\n",
    "        if match:\n",
    "            label=match.group(1)\n",
    "            return label\n",
    "        else:\n",
    "            raise ValueError(\"Modify your re expression\")\n",
    "    def concat_QA(example,col1,col2,tag):\n",
    "        combined = f\"{example[col1]}{tag}{example[col2]}\"  # 用空格拼接\n",
    "        return combined\n",
    "    def replace_col(example,col1,target,pattern):\n",
    "        return example[col1].replace(target,pattern)\n",
    "    # 应用函数并创建新列\n",
    "    dataset = dataset.map(lambda example: {'A': extract_answer(example['response'])})\n",
    "    dataset = dataset.map(lambda example: {'Q+A': concat_QA(example,\"prompt\",\"A\",\"\")})\n",
    "    dataset = dataset.map(lambda example: {'Q+COT_A': concat_QA(example,\"prompt\",\"response\",\"\")})\n",
    "    dataset = dataset.map(lambda example: {'Q+COT_A': replace_col(example,\"Q+COT_A\",\"#### \",\"\")})\n",
    "    # 查看处理后的数据集\n",
    "    print(\"Q+A\\n\",dataset['train'][103]['Q+A'])\n",
    "    print(\"Q+COT_A\\n\",dataset['train'][103]['Q+COT_A'])\n",
    "    return dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.steer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Prepare Dataset\n",
    "\n",
    "logging.info(\"dataset path \"+args.dataset_path)\n",
    "if \"neg\" in args.steer or \"pos\" in args.steer and args.steer==\"sentiment\":\n",
    "    neg_train_set, pos_train_set, neu_train_set,val_set,test_set = load_and_prepare_triple_dataset(\n",
    "        args.dataset_path, \"sst5\",args.seed, args.data_size\n",
    "    )\n",
    "elif \"cot\" in args.steer or \"COT\" in args.steer:\n",
    "    logging.info(\"COT \"*10)\n",
    "    all_dataset=load_and_prepare_COT_dataset(\n",
    "        args.dataset_path, args.seed, args.data_size\n",
    "    )\n",
    "elif \"polite\" in args.steer:\n",
    "    logging.info(\"polite\"*10)\n",
    "    neg_train_set, pos_train_set, neu_train_set,val_set,test_set=load_and_prepare_triple_dataset(args.dataset_path,\"polite\", args.seed, args.data_size)\n",
    "else:\n",
    "    raise ValueError(\"No Supported\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert neg_train_set[10]!=pos_train_set[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train_set[10],neg_train_set[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_latents(sae: SAE, model: HookedTransformer, texts: list, hook_point: str, device: str, batch_size: int) -> list:\n",
    "    \"\"\"\n",
    "    计算 latents，支持批次处理。\n",
    "\n",
    "    Args:\n",
    "        sae (SAE): SAE 实例。\n",
    "        model (HookedTransformer): Transformer 模型实例。\n",
    "        texts (list): 文本列表。\n",
    "        hook_point (str): 钩子点名称。\n",
    "        device (str): 计算设备。\n",
    "        batch_size (int): 每个批次的大小。\n",
    "\n",
    "    Returns:\n",
    "        list: 包含每个批次 latents 的张量列表。\n",
    "    \"\"\"\n",
    "    logging.info(\"Running model with cache to obtain hidden states\")\n",
    "    batch_latents = []\n",
    "\n",
    "    # 使用 tqdm 显示进度条\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Processing batches\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        sv_logits, cache = model.run_with_cache(batch_texts, prepend_bos=False, device=device)\n",
    "        batch_hidden_states = cache[hook_point]\n",
    "        logging.info(f\"Batch {i // batch_size + 1}: Hidden states shape: {batch_hidden_states.shape}\")\n",
    "\n",
    "        logging.info(f\"Encoding hidden states for batch {i // batch_size + 1}\")\n",
    "        # 假设 sae.encode 支持批量编码\n",
    "        latents = sae.encode(batch_hidden_states)  # 形状: (batch_size, latent_dim)\n",
    "        batch_latents.append(latents)\n",
    "        \n",
    "\n",
    "    logging.info(f\"Total batches processed: {len(batch_latents)}\")\n",
    "    return batch_latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_steering_vectors(sae: SAE, overlap_indices: Tensor, nz_mean: Tensor, method: str = \"val_mul\") -> Tensor:\n",
    "    logging.info(f\"Computing steering vectors using method: {method}\")\n",
    "    if method == \"mean\":\n",
    "        steering_vectors = torch.mean(sae.W_dec[overlap_indices], dim=0)\n",
    "    elif method == \"val_mul\":\n",
    "        steering_vectors = torch.zeros(sae.W_dec.shape[1], device=sae.W_dec.device)\n",
    "        for important_idx in overlap_indices:\n",
    "            steering_vectors += nz_mean[important_idx].item() * sae.W_dec[important_idx]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    logging.info(f\"Steering vectors computed with shape: {steering_vectors.shape}\")\n",
    "    return steering_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(output_dir: str, nz_mean: Tensor, act_cnt: Tensor, generated_texts: list, hyperparams: dict):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    # Save nz_mean and act_cnt\n",
    "    nz_stats_path = os.path.join(output_dir, 'nz_stats.pt')\n",
    "    logging.info(f\"Saving nz_mean and act_cnt to {nz_stats_path}\")\n",
    "    torch.save({\n",
    "        'nz_mean': nz_mean,\n",
    "        'act_cnt': act_cnt,\n",
    "    }, nz_stats_path)\n",
    "\n",
    "    # Save generated texts\n",
    "    generated_texts_path = os.path.join(output_dir, 'generated_texts.txt')\n",
    "    logging.info(f\"Saving generated texts to {generated_texts_path}\")\n",
    "    with open(generated_texts_path, 'w') as f:\n",
    "        for text in generated_texts:\n",
    "            f.write(text + \"\\n\")\n",
    "\n",
    "    # Save hyperparameters\n",
    "    hyperparams_path = os.path.join(output_dir, 'hyperparameters.json')\n",
    "    logging.info(f\"Saving hyperparameters to {hyperparams_path}\")\n",
    "    with open(hyperparams_path, 'w') as f:\n",
    "        json.dump(hyperparams, f, indent=4)\n",
    "\n",
    "    logging.info(\"All results saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_base = os.path.join(\n",
    "    args.output_dir,\n",
    "    f\"mask_test/LLM_{args.LLM}_layer_{args.layer}_steer_{args.steer}_alpha_{args.alpha}_cnt_{args.topk_cnt}_mean{args.topk_mean}\"\n",
    ")\n",
    "output_dir_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_cache():\n",
    "    cache_exists = False\n",
    "    cache_file = os.path.join(output_dir_base, 'hyperparameters.json')\n",
    "    if os.path.exists(cache_file):\n",
    "        with open(cache_file, 'r') as f:\n",
    "            cached_data = json.load(f)\n",
    "        cached_hash = cached_data.get('hyperparams_hash')\n",
    "\n",
    "    if cache_exists:\n",
    "        # Load nz_mean and act_cnt from cache\n",
    "        # nz_stats_path = os.path.join(output_dir_base, 'nz_stats.pt')\n",
    "        # nz_act = torch.load(nz_stats_path)\n",
    "        # nz_mean = nz_act['nz_mean']\n",
    "        # act_cnt = nz_act['act_cnt']\n",
    "        # overlap_indices = nz_act.get('overlap_indices', None)  # If overlap_indices was saved\n",
    "        logging.info(\"load from cache\")\n",
    "    else:\n",
    "        # overlap_indices = None  # Will be computed later\n",
    "        logging.info(\"non cache: \"+cache_file)\n",
    "load_from_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save hyperparameters\n",
    "hyperparams = vars(args)\n",
    "\n",
    "# Log hyperparameters\n",
    "logging.info(\"Hyperparameters:\")\n",
    "for key, value in hyperparams.items():\n",
    "    logging.info(f\"  {key}: {value}\")\n",
    "\n",
    "# Load environment\n",
    "load_environment(args.env_path)\n",
    "\n",
    "# Load model and SAE\n",
    "logging.info(f\"Loading model: {args.LLM}\")\n",
    "model = HookedTransformer.from_pretrained(args.LLM, device=args.device)\n",
    "\n",
    "logging.info(f\"Loading SAE for layer {args.layer}\")\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release=\"gpt2-small-res-jb\",\n",
    "    sae_id=f\"blocks.{args.layer}.hook_resid_pre\",\n",
    "    device=args.device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分析latents的函数\n",
    "nz_mean:非零激活的均值\n",
    "act_cnt:非零激活的计数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_latents(batch_latents: Tensor, top_k_mean: int = 100, top_k_cnt: int = 100) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    分析潜在表示（latents）\n",
    "    Args:\n",
    "        batch_latents (Tensor): 批次的潜在表示。\n",
    "        top_k_mean (int, optional): 基于均值选择的 top-k 索引的数量。默认为 100。\n",
    "        top_k_cnt (int, optional): 基于计数选择的 top-k 索引的数量。默认为 100。\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Tensor, Tensor, Tensor]: 包含非零元素的均值、计数和重叠索引的元组。\n",
    "    \"\"\"\n",
    "    logging.info(\"Computing non-zero element counts\")\n",
    "    act_cnt = (batch_latents != 0).sum(dim=(0, 1))\n",
    "\n",
    "    logging.info(\"Computing sum of non-zero elements\")\n",
    "    nz_sum = torch.where(batch_latents != 0, batch_latents, torch.tensor(0.0, device=batch_latents.device)).sum(dim=(0, 1))\n",
    "\n",
    "    logging.info(\"Computing mean of non-zero elements\")\n",
    "    nz_mean = torch.where(act_cnt != 0, nz_sum / act_cnt, torch.tensor(0.0, device=batch_latents.device))\n",
    "\n",
    "    logging.info(\"Selecting top-k indices based on nz_mean\")\n",
    "    nz_act_val, nz_val_indices = torch.topk(nz_mean, top_k_mean)\n",
    "    logging.info(f\"Top {top_k_mean} nz_mean values selected.\")\n",
    "\n",
    "    logging.info(\"Selecting top-k indices based on act_cnt\")\n",
    "    nz_cnt, cnt_indices = torch.topk(act_cnt, top_k_cnt)\n",
    "    logging.info(f\"Top {top_k_cnt} act_cnt values selected.\")\n",
    "\n",
    "    # logging.info(\"Finding overlapping indices between nz_mean and act_cnt top-k\")\n",
    "    # overlap_mask = torch.isin(nz_val_indices, cnt_indices)\n",
    "    # overlap_indices = nz_val_indices[overlap_mask]\n",
    "    # logging.info(f\"Number of overlapping indices: {len(overlap_indices)}\")\n",
    "    # overlap_indices=overlap_indices\n",
    "    return nz_mean, act_cnt,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation_by_steer(texts:list):\n",
    "    \"\"\"\n",
    "    根据给定的文本列表获取激活值。\n",
    "    arg:\n",
    "    texts (list): 输入文本列表。\n",
    "    return:\n",
    "    dict: 包含非零均值和计数的字典。\n",
    "    \"\"\"\n",
    "    hook_point = sae.cfg.hook_name\n",
    "\n",
    "    # Compute latents with batch processing\n",
    "    batch_latents = compute_latents(sae, model, texts, hook_point, args.device, args.batch_size)\n",
    "    # 计算第二个维度的最大值\n",
    "    max_dim1 = max(latent.shape[1] for latent in batch_latents)  # 第二个维度的最大值\n",
    "    logging.info(f\"最大长度:{max_dim1}\")\n",
    "    # 对每个 Tensor 进行填充（仅填充第二个维度）\n",
    "    padded_latents_right = [\n",
    "        torch.nn.functional.pad(latent, (0, 0, 0, max_dim1 - latent.size(1)), \"constant\", 0)\n",
    "        for latent in batch_latents\n",
    "    ]\n",
    "\n",
    "    batch_latents_concatenated = torch.cat(padded_latents_right, dim=0)\n",
    "    logging.info(f\"Concatenated batch latents shape: {batch_latents_concatenated.shape}\")\n",
    "\n",
    "    # Analyze latents \n",
    "    nz_mean, act_cnt, _ = analyze_latents(batch_latents_concatenated, top_k_mean=args.topk_mean, top_k_cnt=args.topk_cnt)\n",
    "    return {\"nz_mean\":nz_mean,\"nz_cnt\":act_cnt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.steer=args.steer.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 26000(SAE稀疏神经元)对应的非零激活神经元激活统计信息，和激活值统计信息\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steer_info={}\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3\"\n",
    "if args.steer=='polite-impolite' or args.steer==\"pos-neg\":\n",
    "    logging.info(args.steer)\n",
    "    text=pos_train_set[\"text\"][:args.data_size]\n",
    "    steer_info[\"pos\"]=get_activation_by_steer(text)\n",
    "    text=neg_train_set[\"text\"][:args.data_size]\n",
    "    steer_info[\"neg\"]=get_activation_by_steer(text)\n",
    "    text=neu_train_set[\"text\"][:args.data_size]\n",
    "    steer_info[\"neu\"]=get_activation_by_steer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pos_train_set[\"text\"][:args.data_size]), args.data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steer_info[\"pos\"], steer_info[\"pos\"][\"nz_mean\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert bool(torch.all((steer_info[\"pos\"][\"nz_mean\"]-steer_info[\"neg\"][\"nz_mean\"])==0))==False,\"数据库读取有问题\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在这里做mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source=\"neg\"\n",
    "target=\"pos\"\n",
    "# 调整样本正负性在这里调整 从负样本到正样本还是从正样本()到负样本\n",
    "# 正\n",
    "steer_info[f\"dif_{target}-{source}_relu\"]={\"nz_cnt\":torch.relu(steer_info[target][\"nz_cnt\"]-steer_info[source][\"nz_cnt\"]),\"nz_mean\":torch.relu(steer_info[target][\"nz_mean\"]-steer_info[source][\"nz_mean\"])}\n",
    "top_k=100\n",
    "_,steer_indices=torch.topk(steer_info[f\"dif_{target}-{source}_relu\"][\"nz_cnt\"],top_k)\n",
    "steer_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_half_indices(steer_indices_remain):\n",
    "    permutation = torch.randperm(steer_indices_remain.size(0))\n",
    "    shuffled_tensor = steer_indices_remain[permutation]\n",
    "    half_size = steer_indices_remain.size(0) // 2\n",
    "    return shuffled_tensor[half_size:], shuffled_tensor[:half_size]\n",
    "\n",
    "# random_half_indices(steer_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以用这块做负的\n",
    "# steer_info[f\"dif_{a}-{b}_relu\"]={\"nz_cnt\":torch.relu(steer_info[a][\"nz_cnt\"]-steer_info[b][\"nz_cnt\"]),\"nz_mean\":torch.relu(steer_info[a][\"nz_mean\"]-steer_info[b][\"nz_mean\"])}\n",
    "# top_k=100\n",
    "# steering_vectors,steer_indices=torch.topk(steer_info[f\"dif_{a}-{b}_relu\"][\"nz_cnt\"],top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设 steer_info[f\"dif_{b}-{a}_relu\"][\"nz_cnt\"] 是一个 NumPy 数组\n",
    "nz_cnt = steer_info[f\"dif_{target}-{source}_relu\"][\"nz_cnt\"]\n",
    "\n",
    "# 先获取非零元素的索引\n",
    "nz_indices = np.nonzero(nz_cnt)\n",
    "torch.all(nz_cnt == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steer_info[f\"dif_{target}-{source}_relu\"][\"nz_cnt\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steer_info[f\"dif_{target}-{source}_relu\"][\"nz_mean\"][steer_indices]# 这里有0,没有负数比较正常\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_steering_vectors(sae: SAE, indices: Tensor, nz_mean_val: Tensor, method: str = \"val_mul\") -> Tensor:\n",
    "    logging.info(f\"Computing steering vectors using method: {method}\")\n",
    "    if method == \"mean\":\n",
    "        steering_vectors = torch.mean(sae.W_dec[indices], dim=0)\n",
    "    elif method == \"val_mul\":\n",
    "        steering_vectors = torch.zeros(sae.W_dec.shape[1], device=sae.W_dec.device)\n",
    "        for idx in indices:\n",
    "            steering_vectors += nz_mean_val[idx].item() * sae.W_dec[idx]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    logging.info(f\"Steering vectors computed with shape: {steering_vectors.shape}\")\n",
    "    return steering_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 这里得到的就是delta_matricx\n",
    "\n",
    "下面是generate部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steering_hook(resid_pre, hook,steer_type=\"last\", steering_on=True, alpha=0, delta_matrix=None):\n",
    "    if resid_pre.shape[1] == 1:\n",
    "        return\n",
    "    if steering_on:\n",
    "        if steer_type==\"last\":\n",
    "            resid_pre[:, :-1, :] += alpha * delta_matrix\n",
    "            # count_steering+=1\n",
    "            # steer_cnt+=1\n",
    "            # logging.info(\"干预\"+str(steer_cnt)+\"次\")\n",
    "            \n",
    "        # elif steer_type==\"last2\":\n",
    "        #     resid_pre[:, :-2, :] += args.alpha * steering_vectors\n",
    "        # elif steer_type==\"gaussian\":\n",
    "        #     # 高斯卷积的方式放缩干预矩阵，\n",
    "        #     # 这里需要一个高斯核，然后对steering_vectors进行卷积\n",
    "        #     gaussian_kernel = torch.tensor([1,2,1])\n",
    "        #     steering_vectors = torch.conv1d(steering_vectors, gaussian_kernel, padding=1)\n",
    "        #     resid_pre[:, :-1, :] += args.alpha * steering_vectors\n",
    "        # else:\n",
    "        #     raise ValueError(\"Unknown steering type\")\n",
    "\n",
    "        # 修改这里的干预方式，增加干预的选择，例如从倒数第一个token开始干预，或者从倒数第二个token开始干预，或者使用高斯卷积的方式放缩干预矩阵\n",
    "        \n",
    "        # 这里是干预调整的关键，很有意思的是，如果提前干预效果会更好更连贯，同样加上正弦波之类的效果也很棒\n",
    "        #注意这里是对batch_size,0:-9,hidden_size这样的隐藏层做干预\n",
    "\n",
    "def hooked_generate(prompt_batch, fwd_hooks=[], seed=None, **kwargs):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    with model.hooks(fwd_hooks=fwd_hooks):\n",
    "        tokenized = model.to_tokens(prompt_batch,prepend_bos=False)\n",
    "        result = model.generate(\n",
    "            stop_at_eos=True,  # avoids a bug on MPS\n",
    "            input=tokenized,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=True,\n",
    "            **kwargs,\n",
    "        )\n",
    "    return result\n",
    "\n",
    "def run_generate(example_prompt, sampling_kwargs, show_res=False, steering_on=False, alpha=0, delta_matrix=None):\n",
    "    model.reset_hooks()\n",
    "    editing_hooks = [(f\"blocks.{args.layer}.hook_resid_post\", lambda resid_pre, hook: steering_hook(resid_pre, hook, steer_type=\"last\", steering_on=steering_on, alpha=alpha, delta_matrix=delta_matrix))]\n",
    "\n",
    "    res = hooked_generate(\n",
    "        example_prompt, editing_hooks, seed=None, **sampling_kwargs\n",
    "    )\n",
    "\n",
    "    # Print results, removing the ugly beginning of sequence token\n",
    "    res_str = model.to_string(res[:, :])\n",
    "    # print((\"\\n\\n\" + \"-\" * 80 + \"\\n\\n\").join(res_str))\n",
    "    # generated_texts = res_str\n",
    "    if show_res:\n",
    "        for idx, text in enumerate(res_str):\n",
    "            logging.info(f\"Generated Text: {idx+1}:\\n{text}\\n{'-'*80}\")\n",
    "        \n",
    "    return res_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sampling parameters\n",
    "sampling_kwargs = dict(temperature=1.0, top_p=0.1, freq_penalty=1.0)\n",
    "# Example prompt from the selected set\n",
    "example_prompt = \"Aha, we have good result!\"\n",
    "logging.info(f\"Example prompt: {example_prompt}\")\n",
    "\n",
    "delta_matrix=compute_steering_vectors(sae,indices=steer_indices,nz_mean_val=steer_info[f\"dif_{target}-{source}_relu\"][\"nz_mean\"],method=\"val_mul\")\n",
    "\n",
    "# Generate without steering\n",
    "logging.info(\"Generating texts **without** steering... \")\n",
    "generated_texts_no_steer = run_generate(example_prompt, sampling_kwargs, show_res=True, steering_on=False, alpha=0, delta_matrix=delta_matrix)\n",
    "\n",
    "# Generate with steering\n",
    "logging.info(f\"干预之后的结果\\n干预方向{source}->{target},礼貌任务下，neg=impolite，情感任务下 pos=积极情感\")\n",
    "logging.info(\"** Generating texts with steering... Target **\")\n",
    "generated_texts_with_steer = run_generate(example_prompt, sampling_kwargs, show_res=True, steering_on=True, alpha=100, delta_matrix=delta_matrix)\n",
    "\n",
    "print(\"-\"*80)\n",
    "print(generated_texts_with_steer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_indices_list = [steer_indices]\n",
    "all_generated_texts = [generated_texts_no_steer]\n",
    "for i in range(3):\n",
    "    print(i, \"+\"*80)\n",
    "    next_indices_list = []\n",
    "    for remain_indices in current_indices_list:\n",
    "        l_steer_indices, r_steer_indices = random_half_indices(remain_indices)\n",
    "        next_indices_list.extend([l_steer_indices, r_steer_indices])\n",
    "        l_delta_matrix = compute_steering_vectors(sae, indices=l_steer_indices, nz_mean_val=steer_info[f\"dif_{target}-{source}_relu\"][\"nz_mean\"], method=\"val_mul\")\n",
    "        r_delta_matrix = compute_steering_vectors(sae, indices=r_steer_indices, nz_mean_val=steer_info[f\"dif_{target}-{source}_relu\"][\"nz_mean\"], method=\"val_mul\")\n",
    "        generated_texts_with_l_steer = run_generate(example_prompt, sampling_kwargs, show_res=False, steering_on=True, alpha=100, delta_matrix=l_delta_matrix)\n",
    "        print(generated_texts_with_l_steer)\n",
    "        print(l_steer_indices)\n",
    "        all_generated_texts.append({\"generated_texts\": generated_texts_with_l_steer, \"steer_indices\": l_steer_indices.tolist()})\n",
    "        print('*'*80)\n",
    "        generated_texts_with_r_steer = run_generate(example_prompt, sampling_kwargs, show_res=False, steering_on=True, alpha=100, delta_matrix=r_delta_matrix)\n",
    "        print(generated_texts_with_r_steer)\n",
    "        print(r_steer_indices)\n",
    "        all_generated_texts.append({\"generated_texts\": generated_texts_with_r_steer, \"steer_indices\": r_steer_indices.tolist()})\n",
    "    current_indices_list = next_indices_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_times = 5\n",
    "with open(f'/home/ckqsudo/code2024/CKQ_ACL2024/Control_Infer/SAE-simple/src/results/mask_test/dif_{target}-{source}_relu_test{exp_times}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_generated_texts, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def load_and_prepare_sentiment_prompts():\n",
    "    data_files = {\"neg\": \"negative_prompts.jsonl\", \"pos\": \"positive_prompts.jsonl\",\"neu\":\"neutral_prompts.jsonl\"}\n",
    "    prompts= load_dataset(\"/home/ckqsudo/code2024/0refer_ACL/LM-Steer/data/data/prompts/sentiment_prompts-10k\",data_files=data_files)\n",
    "    print(prompts)\n",
    "    return prompts\n",
    "prompts=load_and_prepare_sentiment_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts['neg'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_prompt = []\n",
    "prompt_idx = 0\n",
    "for i in range(300):\n",
    "    new_batch = []\n",
    "    for k in range(10):\n",
    "        new_batch.append(prompts['neu'][prompt_idx]['prompt']['text'])\n",
    "        prompt_idx += 1\n",
    "    senti_prompt.append(new_batch)\n",
    "senti_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_indices_list = [steer_indices]\n",
    "for i in range(3):\n",
    "    next_indices_list = []\n",
    "    for remain_indices in current_indices_list:\n",
    "        l_steer_indices, r_steer_indices = random_half_indices(remain_indices)\n",
    "        next_indices_list.extend([l_steer_indices, r_steer_indices])\n",
    "    current_indices_list = next_indices_list\n",
    "\n",
    "current_indices_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_generated_texts = []\n",
    "for current_indices in current_indices_list:\n",
    "    current_delta_matrix = compute_steering_vectors(sae, indices=current_indices, nz_mean_val=steer_info[f\"dif_{target}-{source}_relu\"][\"nz_mean\"], method=\"val_mul\")\n",
    "    current_all_generated_texts = []\n",
    "    for batch_prompt in senti_prompt:\n",
    "        generated_texts_with_current_steer = run_generate(batch_prompt, sampling_kwargs, show_res=False, steering_on=True, alpha=100, delta_matrix=current_delta_matrix)\n",
    "        current_all_generated_texts.extend(generated_texts_with_current_steer)\n",
    "    final_generated_texts.append({\"steer_indices\": current_indices.tolist(), \"generated_texts\": current_all_generated_texts})\n",
    "\n",
    "with open(\"/home/ckqsudo/code2024/CKQ_ACL2024/Control_Infer/SAE-simple/src/explanation/result/final_generated_texts_neu2pos.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(final_generated_texts, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 79\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# 示例用法\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;66;03m# 假设 vader_lexicon.txt 位于当前脚本的同一目录下\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m     current_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;18;43m__file__\u001b[39;49m))\n\u001b[1;32m     80\u001b[0m     lexicon_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(current_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvader_lexicon.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m# 加载正向情感词汇\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import codecs\n",
    "import re\n",
    "from collections import Counter\n",
    "from typing import List, Dict\n",
    "\n",
    "def load_positive_lexicon(lexicon_file: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    读取 VADER 词典文件，提取所有正向情感词汇及其情感得分。\n",
    "\n",
    "    :param lexicon_file: VADER 词典文件的路径\n",
    "    :return: 一个字典，键为正向情感词汇，值为其情感得分\n",
    "    \"\"\"\n",
    "    positive_lexicon = {}\n",
    "    \n",
    "    if not os.path.isfile(lexicon_file):\n",
    "        raise FileNotFoundError(f\"The lexicon file '{lexicon_file}' does not exist.\")\n",
    "    \n",
    "    with codecs.open(lexicon_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue  # 跳过空行\n",
    "            parts = line.split('\\t')\n",
    "            if len(parts) < 2:\n",
    "                continue  # 确保至少有两个字段\n",
    "            word = parts[0].lower()  # 转为小写以便后续匹配\n",
    "            try:\n",
    "                sentiment_score = float(parts[1])\n",
    "            except ValueError:\n",
    "                continue  # 跳过无法转换为浮点数的行\n",
    "            if sentiment_score > 0:\n",
    "                positive_lexicon[word] = sentiment_score\n",
    "    return positive_lexicon\n",
    "\n",
    "def simple_tokenize(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    使用正则表达式对文本进行简单分词，提取所有单词。\n",
    "\n",
    "    :param text: 输入的文本字符串\n",
    "    :return: 分词后的单词列表\n",
    "    \"\"\"\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "def get_top_positive_words(word_lists: List[str], positive_lexicon: Dict[str, float], n: int) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    统计输入字符串列表中所有正向情感词语的词频，并返回词频最高的前 n 个。\n",
    "\n",
    "    :param word_lists: 一个由字符串组成的列表，表示要分析的文本\n",
    "    :param positive_lexicon: 正向情感词汇及其情感得分的字典\n",
    "    :param n: 要返回的前 n 个词语\n",
    "    :return: 一个字典，包含前 n 个正向情感词语及其出现次数\n",
    "    \"\"\"\n",
    "    # 初始化一个计数器\n",
    "    word_counter = Counter()\n",
    "    \n",
    "    # 遍历每一个字符串\n",
    "    for text in word_lists:\n",
    "        # 使用简单的正则表达式分词，提取所有单词，忽略大小写\n",
    "        words = simple_tokenize(text)\n",
    "        \n",
    "        # 过滤出所有正向情感词语\n",
    "        positive_words = [word for word in words if word in positive_lexicon]\n",
    "        \n",
    "        # 更新计数器\n",
    "        word_counter.update(positive_words)\n",
    "    \n",
    "    # 获取词频最高的前 n 个词语\n",
    "    top_n = word_counter.most_common(n)\n",
    "    \n",
    "    # 转换为字典形式\n",
    "    top_n_dict = {word: count for word, count in top_n}\n",
    "    \n",
    "    return top_n_dict\n",
    "\n",
    "# 示例用法\n",
    "if __name__ == \"__main__\":\n",
    "    # 假设 vader_lexicon.txt 位于当前脚本的同一目录下\n",
    "    lexicon_path = '/home/ckqsudo/code2024/CKQ_ACL2024/Control_Infer/SAE-simple/src/explanation/vader_lexicon.txt'\n",
    "    \n",
    "    # 加载正向情感词汇\n",
    "    positive_lexicon = load_positive_lexicon(lexicon_path)\n",
    "    \n",
    "    # 示例文本列表\n",
    "    example_texts = [\n",
    "        \"abandon happy happy excited abilities ability aboard happy\",\n",
    "        \"I am feeling very happy today! It's an awesome day with great opportunities.\",\n",
    "        \"The team showed excellent performance, which was absolutely fantastic.\",\n",
    "        \"She has incredible abilities and a great ability to inspire others.\",\n",
    "        \"Happy moments make life wonderful and abilities enhance our experiences.\"\n",
    "    ]\n",
    "    \n",
    "    # 获取词频最高的前 5 个正向情感词语\n",
    "    top_positive = get_top_positive_words(example_texts, positive_lexicon, 5)\n",
    "    \n",
    "    print(\"Top Positive Words:\")\n",
    "    for word, count in top_positive.items():\n",
    "        print(f\"{word}: {count}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
