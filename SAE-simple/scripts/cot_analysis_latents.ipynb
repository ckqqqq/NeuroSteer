{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE\n",
    "from sae_lens.analysis.neuronpedia_integration import get_neuronpedia_quick_list\n",
    "from sae_lens.analysis.feature_statistics import (\n",
    "    get_all_stats_dfs,\n",
    "    get_W_U_W_dec_stats_df,\n",
    ")\n",
    "from sae_lens.analysis.tsea import (\n",
    "    get_enrichment_df,\n",
    "    manhattan_plot_enrichment_scores,\n",
    "    plot_top_k_feature_projections_by_token_and_category,\n",
    "    get_baby_name_sets,\n",
    "    get_letter_gene_sets,\n",
    "    generate_pos_sets,\n",
    "    get_test_gene_sets,\n",
    "    get_gene_set_from_regex,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import plotly_express as px\n",
    "import logging\n",
    "from typing import Tuple\n",
    "import json\n",
    "from log import setup_logging\n",
    "from tqdm import tqdm  # For progress bars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "args_dict = {\n",
    "    \"layer\": 6,  # Example layer number to analyze\n",
    "    \"LLM\": \"gpt2-small\",\n",
    "    \"dataset_path\": \"/home/ckqsudo/code2024/0dataset/emotional_classify/multiclass-sentiment-analysis\",\n",
    "    \"output_dir\": \"./results\",\n",
    "    \"env_path\": \"/home/ckqsudo/code2024/CKQ_ACL2024/Control_Infer/SAE-simple/.env\",\n",
    "    \"seed\": 42,\n",
    "    \"data_size\": 1000,\n",
    "    \"device\": \"cpu\",  # Options: \"cpu\", \"cuda\", \"mps\", \"auto\"\n",
    "    \"alpha\": 100,\n",
    "    \"steer\": \"pos-neg\",  # Options: \"pos\", \"neg\", \"neu\",\"pos-neg\",\"cot-direct\"\n",
    "    \"method\": \"val_mul\",  # Options: \"mean\", \"val_mul\"\n",
    "    \"topk_mean\": 100,\n",
    "    \"topk_cnt\": 100,\n",
    "    \"batch_size\": 32\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 进行COT相关实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "args_dict = {\n",
    "    \"layer\": 6,  # Example layer number to analyze\n",
    "    \"LLM\": \"gpt2-small\",\n",
    "    \"dataset_path\": \"/home/ckqsudo/code2024/0dataset/ACL_useful_dataset/math/COT_GSM8k\",\n",
    "    \"output_dir\": \"./results\",\n",
    "    \"env_path\": \"/home/ckqsudo/code2024/CKQ_ACL2024/Control_Infer/SAE-simple/.env\",\n",
    "    \"seed\": 42,\n",
    "    \"data_size\": 1000,\n",
    "    \"device\": \"cpu\",  # Options: \"cpu\", \"cuda\", \"mps\", \"auto\"\n",
    "    \"alpha\": 100,\n",
    "    \"steer\": \"cot-direct\",  # Options: \"pos\", \"neg\", \"neu\",\"pos-neg\",\"cot-direct\"\n",
    "    \"method\": \"val_mul\",  # Options: \"mean\", \"val_mul\"\n",
    "    \"topk_mean\": 100,\n",
    "    \"topk_cnt\": 100,\n",
    "    \"batch_size\": 32\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "gpt2-small\n",
      "./results\n"
     ]
    }
   ],
   "source": [
    "# Configuration and Hyperparameters\n",
    "\n",
    "\n",
    "\n",
    "# 将字典转换为 argparse.Namespace 对象\n",
    "args = argparse.Namespace(**args_dict)\n",
    "\n",
    "# 测试访问属性\n",
    "print(args.layer)  # 输出: 10\n",
    "print(args.LLM)  # 输出: gpt2-small\n",
    "print(args.output_dir)  # 输出: ./results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 11:24:39,298 [INFO] Logging initialized. Logs will be saved to ./results/LLM_gpt2-small_layer_6_steer_cot-direct_alpha_100_cnt_100_mean100/execution.log\n",
      "2025-01-10 11:24:39,300 [INFO] Hyperparameters:\n",
      "2025-01-10 11:24:39,302 [INFO]   layer: 6\n",
      "2025-01-10 11:24:39,303 [INFO]   LLM: gpt2-small\n",
      "2025-01-10 11:24:39,304 [INFO]   dataset_path: /home/ckqsudo/code2024/0dataset/ACL_useful_dataset/math/COT_GSM8k\n",
      "2025-01-10 11:24:39,305 [INFO]   output_dir: ./results\n",
      "2025-01-10 11:24:39,306 [INFO]   env_path: /home/ckqsudo/code2024/CKQ_ACL2024/Control_Infer/SAE-simple/.env\n",
      "2025-01-10 11:24:39,307 [INFO]   seed: 42\n",
      "2025-01-10 11:24:39,307 [INFO]   data_size: 1000\n",
      "2025-01-10 11:24:39,309 [INFO]   device: cpu\n",
      "2025-01-10 11:24:39,310 [INFO]   alpha: 100\n",
      "2025-01-10 11:24:39,311 [INFO]   steer: cot-direct\n",
      "2025-01-10 11:24:39,319 [INFO]   method: val_mul\n",
      "2025-01-10 11:24:39,319 [INFO]   topk_mean: 100\n",
      "2025-01-10 11:24:39,320 [INFO]   topk_cnt: 100\n",
      "2025-01-10 11:24:39,321 [INFO]   batch_size: 32\n"
     ]
    }
   ],
   "source": [
    "# Logging Setup\n",
    "import os\n",
    "from log import setup_logging\n",
    "import logging\n",
    "# Create output directory base path\n",
    "output_dir_base = os.path.join(\n",
    "    args.output_dir,\n",
    "    f\"LLM_{args.LLM}_layer_{args.layer}_steer_{args.steer}_alpha_{args.alpha}_cnt_{args.topk_cnt}_mean{args.topk_mean}\"\n",
    ")\n",
    "\n",
    "# Setup logging\n",
    "setup_logging(output_dir_base)\n",
    "\n",
    "# Save hyperparameters\n",
    "hyperparams = args_dict\n",
    "\n",
    "# Log hyperparameters\n",
    "logging.info(\"Hyperparameters:\")\n",
    "for key, value in hyperparams.items():\n",
    "    logging.info(f\"  {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 11:24:42,985 [INFO] HF_ENDPOINT: https://hf-mirror.com\n"
     ]
    }
   ],
   "source": [
    "# Load Environment Variables\n",
    " \n",
    "def load_environment(env_path: str):\n",
    "    load_dotenv(env_path)\n",
    "    hf_endpoint = os.getenv('HF_ENDPOINT', 'https://hf-mirror.com')\n",
    "    logging.info(f\"HF_ENDPOINT: {hf_endpoint}\")\n",
    "\n",
    "load_environment(args.env_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def load_and_prepare_sentiment_dataset(dataset_path: str, seed: int, num_samples: int):\n",
    "    logging.info(f\"Loading dataset from {dataset_path}\")\n",
    "    dataset = load_dataset(dataset_path)\n",
    "    dataset[\"train\"] = dataset['train'].shuffle(seed=seed)\n",
    "\n",
    "    logging.info(\"Filtering dataset for negative, positive, and neutral samples\")\n",
    "    neg_train_set = dataset['train'].filter(lambda example: example['label'] == 0).select(range(num_samples))\n",
    "    pos_train_set = dataset['train'].filter(lambda example: example['label'] == 2).select(range(num_samples))\n",
    "    neu_train_set = dataset['train'].filter(lambda example: example['label'] == 1).select(range(num_samples))\n",
    "\n",
    "    logging.info(f\"Selected {len(neg_train_set)} negative, {len(pos_train_set)} positive, and {len(neu_train_set)} neutral samples\")\n",
    "    return neg_train_set, pos_train_set, neu_train_set\n",
    "def load_and_prepare_COT_dataset(dataset_path:str,seed:int,num_samples:int):\n",
    "    logging.info(f\"Loading dataset from {dataset_path}\")\n",
    "    dataset = load_dataset(dataset_path)\n",
    "    dataset[\"train\"] = dataset['train'].shuffle(seed=seed)\n",
    "    logging.info(\"Filtering dataset for COT\")\n",
    "    # 定义一个函数来提取答案\n",
    "    def extract_answer(text):\n",
    "        # 使用正则表达式提取答案\n",
    "        match = re.search(r'#### ([-+]?\\d*\\.?\\d+/?\\d*)', text)\n",
    "        if match:\n",
    "            label=match.group(1)\n",
    "            return label\n",
    "        else:\n",
    "            raise ValueError(\"Modify your re expression\")\n",
    "    def concat_QA(example,col1,col2,tag):\n",
    "        combined = f\"{example[col1]}{tag}{example[col2]}\"  # 用空格拼接\n",
    "        return combined\n",
    "    def replace_col(example,col1,target,pattern):\n",
    "        return example[col1].replace(target,pattern)\n",
    "    # 应用函数并创建新列\n",
    "    dataset = dataset.map(lambda example: {'A': extract_answer(example['response'])})\n",
    "    dataset = dataset.map(lambda example: {'Q+A': concat_QA(example,\"prompt\",\"A\",\"\")})\n",
    "    dataset = dataset.map(lambda example: {'Q+COT_A': concat_QA(example,\"prompt\",\"response\",\"\")})\n",
    "    dataset = dataset.map(lambda example: {'Q+COT_A': replace_col(example,\"Q+COT_A\",\"#### \",\"\")})\n",
    "    # 查看处理后的数据集\n",
    "    print(\"Q+A\\n\",dataset['train'][103]['Q+A'])\n",
    "    print(\"Q+COT_A\\n\",dataset['train'][103]['Q+COT_A'])\n",
    "    return dataset\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cot-direct'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.steer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 14:27:18,864 [INFO] COT COT COT COT COT COT COT COT COT COT \n",
      "2025-01-10 14:27:18,866 [INFO] Loading dataset from /home/ckqsudo/code2024/0dataset/ACL_useful_dataset/math/COT_GSM8k\n",
      "2025-01-10 14:27:19,027 [INFO] Filtering dataset for COT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q+A\n",
      " Q: Cody goes to the store and buys $40 worth of stuff.  The taxes were 5%.  After taxes, he got an $8 discount.  Cody and his friend split the final price equally. How much did Cody pay?\n",
      "A: 17\n",
      "Q+COT_A\n",
      " Q: Cody goes to the store and buys $40 worth of stuff.  The taxes were 5%.  After taxes, he got an $8 discount.  Cody and his friend split the final price equally. How much did Cody pay?\n",
      "A: The taxes were 40*.05=$<<40*.05=2>>2.\n",
      "So the price was 40+2=$<<40+2=42>>42.\n",
      "He got a discount so the price he paid was 42-8=$<<42-8=34>>34.\n",
      "Since he paid half his price was 34/2=$<<34/2=17>>17.\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "# Load and Prepare Dataset\n",
    "\n",
    "\n",
    "if \"neg\" in args.steer or \"pos\" in args.steer:\n",
    "    neg_train_set, pos_train_set, neu_train_set = load_and_prepare_sentiment_dataset(\n",
    "        args.dataset_path, args.seed, args.data_size\n",
    "    )\n",
    "elif \"cot\" in args.steer or \"COT\" in args.steer:\n",
    "    logging.info(\"COT \"*10)\n",
    "    all_dataset=load_and_prepare_COT_dataset(\n",
    "        args.dataset_path, args.seed, args.data_size\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\"No Supported\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_latents(sae: SAE, model: HookedTransformer, texts: list, hook_point: str, device: str, batch_size: int) -> list:\n",
    "    \"\"\"\n",
    "    计算 latents，支持批次处理。\n",
    "\n",
    "    Args:\n",
    "        sae (SAE): SAE 实例。\n",
    "        model (HookedTransformer): Transformer 模型实例。\n",
    "        texts (list): 文本列表。\n",
    "        hook_point (str): 钩子点名称。\n",
    "        device (str): 计算设备。\n",
    "        batch_size (int): 每个批次的大小。\n",
    "\n",
    "    Returns:\n",
    "        list: 包含每个批次 latents 的张量列表。\n",
    "    \"\"\"\n",
    "    logging.info(\"Running model with cache to obtain hidden states\")\n",
    "    batch_latents = []\n",
    "\n",
    "    # 使用 tqdm 显示进度条\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Processing batches\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        sv_logits, cache = model.run_with_cache(batch_texts, prepend_bos=True, device=device)\n",
    "        batch_hidden_states = cache[hook_point]\n",
    "        logging.info(f\"Batch {i // batch_size + 1}: Hidden states shape: {batch_hidden_states.shape}\")\n",
    "\n",
    "        logging.info(f\"Encoding hidden states for batch {i // batch_size + 1}\")\n",
    "        # 假设 sae.encode 支持批量编码\n",
    "        latents = sae.encode(batch_hidden_states)  # 形状: (batch_size, latent_dim)\n",
    "        batch_latents.append(latents)\n",
    "        \n",
    "\n",
    "    logging.info(f\"Total batches processed: {len(batch_latents)}\")\n",
    "    return batch_latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_latents(batch_latents: Tensor, top_k_mean: int = 100, top_k_cnt: int = 100) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "    logging.info(\"Computing non-zero element counts\")\n",
    "    act_cnt = (batch_latents != 0).sum(dim=(0, 1))\n",
    "\n",
    "    logging.info(\"Computing sum of non-zero elements\")\n",
    "    nz_sum = torch.where(batch_latents != 0, batch_latents, torch.tensor(0.0, device=batch_latents.device)).sum(dim=(0, 1))\n",
    "\n",
    "    logging.info(\"Computing mean of non-zero elements\")\n",
    "    nz_mean = torch.where(act_cnt != 0, nz_sum / act_cnt, torch.tensor(0.0, device=batch_latents.device))\n",
    "\n",
    "    logging.info(\"Selecting top-k indices based on nz_mean\")\n",
    "    nz_act_val, nz_val_indices = torch.topk(nz_mean, top_k_mean)\n",
    "    logging.info(f\"Top {top_k_mean} nz_mean values selected.\")\n",
    "\n",
    "    logging.info(\"Selecting top-k indices based on act_cnt\")\n",
    "    nz_cnt, cnt_indices = torch.topk(act_cnt, top_k_cnt)\n",
    "    logging.info(f\"Top {top_k_cnt} act_cnt values selected.\")\n",
    "\n",
    "    # logging.info(\"Finding overlapping indices between nz_mean and act_cnt top-k\")\n",
    "    # overlap_mask = torch.isin(nz_val_indices, cnt_indices)\n",
    "    # overlap_indices = nz_val_indices[overlap_mask]\n",
    "    # logging.info(f\"Number of overlapping indices: {len(overlap_indices)}\")\n",
    "    # overlap_indices=overlap_indices\n",
    "    return nz_mean, act_cnt, cnt_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_steering_vectors(sae: SAE, overlap_indices: Tensor, nz_mean: Tensor, method: str = \"val_mul\") -> Tensor:\n",
    "    logging.info(f\"Computing steering vectors using method: {method}\")\n",
    "    if method == \"mean\":\n",
    "        steering_vectors = torch.mean(sae.W_dec[overlap_indices], dim=0)\n",
    "    elif method == \"val_mul\":\n",
    "        steering_vectors = torch.zeros(sae.W_dec.shape[1], device=sae.W_dec.device)\n",
    "        for important_idx in overlap_indices:\n",
    "            steering_vectors += nz_mean[important_idx].item() * sae.W_dec[important_idx]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    logging.info(f\"Steering vectors computed with shape: {steering_vectors.shape}\")\n",
    "    return steering_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_results(output_dir: str, nz_mean: Tensor, act_cnt: Tensor, generated_texts: list, hyperparams: dict):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save nz_mean and act_cnt\n",
    "    nz_stats_path = os.path.join(output_dir, 'nz_stats.pt')\n",
    "    logging.info(f\"Saving nz_mean and act_cnt to {nz_stats_path}\")\n",
    "    torch.save({\n",
    "        'nz_mean': nz_mean,\n",
    "        'act_cnt': act_cnt\n",
    "    }, nz_stats_path)\n",
    "\n",
    "    # Save generated texts\n",
    "    generated_texts_path = os.path.join(output_dir, 'generated_texts.txt')\n",
    "    logging.info(f\"Saving generated texts to {generated_texts_path}\")\n",
    "    with open(generated_texts_path, 'w') as f:\n",
    "        for text in generated_texts:\n",
    "            f.write(text + \"\\n\")\n",
    "\n",
    "    # Save hyperparameters\n",
    "    hyperparams_path = os.path.join(output_dir, 'hyperparameters.json')\n",
    "    logging.info(f\"Saving hyperparameters to {hyperparams_path}\")\n",
    "    with open(hyperparams_path, 'w') as f:\n",
    "        json.dump(hyperparams, f, indent=4)\n",
    "\n",
    "    logging.info(\"All results saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./results/LLM_gpt2-small_layer_6_steer_cot-direct_alpha_100_cnt_100_mean100'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir_base = os.path.join(\n",
    "    args.output_dir,\n",
    "    f\"LLM_{args.LLM}_layer_{args.layer}_steer_{args.steer}_alpha_{args.alpha}_cnt_{args.topk_cnt}_mean{args.topk_mean}\"\n",
    ")\n",
    "output_dir_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 09:46:29,462 [INFO] non cache: ./results/LLM_gpt2-small_layer_6_steer_pos-neg_alpha_100_cnt_100_mean100/hyperparameters.json\n"
     ]
    }
   ],
   "source": [
    "def load_from_cache():\n",
    "    cache_exists = False\n",
    "    cache_file = os.path.join(output_dir_base, 'hyperparameters.json')\n",
    "    if os.path.exists(cache_file):\n",
    "        with open(cache_file, 'r') as f:\n",
    "            cached_data = json.load(f)\n",
    "        cached_hash = cached_data.get('hyperparams_hash')\n",
    "\n",
    "    if cache_exists:\n",
    "        # Load nz_mean and act_cnt from cache\n",
    "        # nz_stats_path = os.path.join(output_dir_base, 'nz_stats.pt')\n",
    "        # nz_act = torch.load(nz_stats_path)\n",
    "        # nz_mean = nz_act['nz_mean']\n",
    "        # act_cnt = nz_act['act_cnt']\n",
    "        # overlap_indices = nz_act.get('overlap_indices', None)  # If overlap_indices was saved\n",
    "        logging.info(\"load from cache\")\n",
    "    else:\n",
    "        # overlap_indices = None  # Will be computed later\n",
    "        logging.info(\"non cache: \"+cache_file)\n",
    "load_from_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup_logging(output_dir_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 14:28:22,647 [INFO] Hyperparameters:\n",
      "2025-01-10 14:28:22,649 [INFO]   layer: 6\n",
      "2025-01-10 14:28:22,652 [INFO]   LLM: gpt2-small\n",
      "2025-01-10 14:28:22,653 [INFO]   dataset_path: /home/ckqsudo/code2024/0dataset/ACL_useful_dataset/math/COT_GSM8k\n",
      "2025-01-10 14:28:22,654 [INFO]   output_dir: ./results\n",
      "2025-01-10 14:28:22,655 [INFO]   env_path: /home/ckqsudo/code2024/CKQ_ACL2024/Control_Infer/SAE-simple/.env\n",
      "2025-01-10 14:28:22,656 [INFO]   seed: 42\n",
      "2025-01-10 14:28:22,656 [INFO]   data_size: 1000\n",
      "2025-01-10 14:28:22,658 [INFO]   device: cpu\n",
      "2025-01-10 14:28:22,659 [INFO]   alpha: 100\n",
      "2025-01-10 14:28:22,660 [INFO]   steer: cot-direct\n",
      "2025-01-10 14:28:22,661 [INFO]   method: val_mul\n",
      "2025-01-10 14:28:22,662 [INFO]   topk_mean: 100\n",
      "2025-01-10 14:28:22,663 [INFO]   topk_cnt: 100\n",
      "2025-01-10 14:28:22,664 [INFO]   batch_size: 32\n",
      "2025-01-10 14:28:22,667 [INFO] HF_ENDPOINT: https://hf-mirror.com\n",
      "2025-01-10 14:28:22,668 [INFO] Loading model: gpt2-small\n",
      "2025-01-10 14:29:03,517 [INFO] Loading SAE for layer 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ckqsudo/miniconda3/envs/SAE/lib/python3.11/site-packages/sae_lens/sae.py:145: UserWarning: \n",
      "This SAE has non-empty model_from_pretrained_kwargs. \n",
      "For optimal performance, load the model like so:\n",
      "model = HookedSAETransformer.from_pretrained_no_processing(..., **cfg.model_from_pretrained_kwargs)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Save hyperparameters\n",
    "hyperparams = vars(args)\n",
    "\n",
    "# Log hyperparameters\n",
    "logging.info(\"Hyperparameters:\")\n",
    "for key, value in hyperparams.items():\n",
    "    logging.info(f\"  {key}: {value}\")\n",
    "\n",
    "# Load environment\n",
    "load_environment(args.env_path)\n",
    "\n",
    "# Load model and SAE\n",
    "logging.info(f\"Loading model: {args.LLM}\")\n",
    "model = HookedTransformer.from_pretrained(args.LLM, device=args.device)\n",
    "\n",
    "logging.info(f\"Loading SAE for layer {args.layer}\")\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release=\"gpt2-small-res-jb\",\n",
    "    sae_id=f\"blocks.{args.layer}.hook_resid_pre\",\n",
    "    device=args.device\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 14:31:32,014 [INFO] Loading dataset from /home/ckqsudo/code2024/0dataset/ACL_useful_dataset/math/COT_GSM8k\n",
      "2025-01-10 14:31:32,065 [INFO] Filtering dataset for COT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q+A\n",
      " Q: Cody goes to the store and buys $40 worth of stuff.  The taxes were 5%.  After taxes, he got an $8 discount.  Cody and his friend split the final price equally. How much did Cody pay?\n",
      "A: 17\n",
      "Q+COT_A\n",
      " Q: Cody goes to the store and buys $40 worth of stuff.  The taxes were 5%.  After taxes, he got an $8 discount.  Cody and his friend split the final price equally. How much did Cody pay?\n",
      "A: The taxes were 40*.05=$<<40*.05=2>>2.\n",
      "So the price was 40+2=$<<40+2=42>>42.\n",
      "He got a discount so the price he paid was 42-8=$<<42-8=34>>34.\n",
      "Since he paid half his price was 34/2=$<<34/2=17>>17.\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "all_dataset = load_and_prepare_COT_dataset(\n",
    "    args.dataset_path, args.seed, args.data_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cot-direct'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.steer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_latents(batch_latents: Tensor, top_k_mean: int = 100, top_k_cnt: int = 100) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "    logging.info(\"Computing non-zero element counts\")\n",
    "    act_cnt = (batch_latents != 0).sum(dim=(0, 1))\n",
    "\n",
    "    logging.info(\"Computing sum of non-zero elements\")\n",
    "    nz_sum = torch.where(batch_latents != 0, batch_latents, torch.tensor(0.0, device=batch_latents.device)).sum(dim=(0, 1))\n",
    "\n",
    "    logging.info(\"Computing mean of non-zero elements\")\n",
    "    nz_mean = torch.where(act_cnt != 0, nz_sum / act_cnt, torch.tensor(0.0, device=batch_latents.device))\n",
    "\n",
    "    logging.info(\"Selecting top-k indices based on nz_mean\")\n",
    "    nz_act_val, nz_val_indices = torch.topk(nz_mean, top_k_mean)\n",
    "    logging.info(f\"Top {top_k_mean} nz_mean values selected.\")\n",
    "\n",
    "    logging.info(\"Selecting top-k indices based on act_cnt\")\n",
    "    nz_cnt, cnt_indices = torch.topk(act_cnt, top_k_cnt)\n",
    "    logging.info(f\"Top {top_k_cnt} act_cnt values selected.\")\n",
    "\n",
    "    # logging.info(\"Finding overlapping indices between nz_mean and act_cnt top-k\")\n",
    "    # overlap_mask = torch.isin(nz_val_indices, cnt_indices)\n",
    "    # overlap_indices = nz_val_indices[overlap_mask]\n",
    "    # logging.info(f\"Number of overlapping indices: {len(overlap_indices)}\")\n",
    "    # overlap_indices=overlap_indices\n",
    "    return nz_mean, act_cnt, cnt_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a dataset steer based on steering preference\n",
    "\n",
    "def get_activation_by_steer(texts:list):\n",
    "\n",
    "    hook_point = sae.cfg.hook_name\n",
    "\n",
    "    # Compute latents with batch processing\n",
    "    batch_latents = compute_latents(sae, model, texts, hook_point, args.device, args.batch_size)\n",
    "    # 计算第二个维度的最大值\n",
    "    max_dim1 = max(latent.shape[1] for latent in batch_latents)  # 第二个维度的最大值\n",
    "    logging.info(f\"最大长度:{max_dim1}\")\n",
    "    # 对每个 Tensor 进行填充（仅填充第二个维度）\n",
    "    padded_latents_right = [\n",
    "        torch.nn.functional.pad(latent, (0, 0, 0, max_dim1 - latent.size(1)), \"constant\", 0)\n",
    "        for latent in batch_latents\n",
    "    ]\n",
    "\n",
    "    batch_latents_concatenated = torch.cat(padded_latents_right, dim=0)\n",
    "    logging.info(f\"Concatenated batch latents shape: {batch_latents_concatenated.shape}\")\n",
    "\n",
    "    # Analyze latents\n",
    "    nz_mean, act_cnt, overlap_indices = analyze_latents(batch_latents_concatenated, top_k_mean=args.topk_mean, top_k_cnt=args.topk_cnt)\n",
    "    return {\"nz_mean\":nz_mean,\"nz_cnt\":act_cnt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.steer=args.steer.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q: Ten percent less than twice the total number of students present in the science class yesterday have attended class today. If there were 70 students in the class yesterday and 30 students are absent today, calculate the number of students registered for the course.\\nA: 156'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dataset[\"train\"][\"Q+A\"][:args.data_size][193]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 14:50:52,557 [INFO] Running model with cache to obtain hidden states\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/32 [00:00<?, ?it/s]2025-01-10 14:50:53,432 [INFO] Batch 1: Hidden states shape: torch.Size([32, 155, 768])\n",
      "2025-01-10 14:50:53,433 [INFO] Encoding hidden states for batch 1\n",
      "Processing batches:   3%|▎         | 1/32 [00:01<00:31,  1.02s/it]2025-01-10 14:50:54,501 [INFO] Batch 2: Hidden states shape: torch.Size([32, 104, 768])\n",
      "2025-01-10 14:50:54,503 [INFO] Encoding hidden states for batch 2\n",
      "Processing batches:   6%|▋         | 2/32 [00:02<00:30,  1.03s/it]2025-01-10 14:50:55,365 [INFO] Batch 3: Hidden states shape: torch.Size([32, 115, 768])\n",
      "2025-01-10 14:50:55,367 [INFO] Encoding hidden states for batch 3\n",
      "Processing batches:   9%|▉         | 3/32 [00:02<00:27,  1.05it/s]2025-01-10 14:50:56,358 [INFO] Batch 4: Hidden states shape: torch.Size([32, 121, 768])\n",
      "2025-01-10 14:50:56,360 [INFO] Encoding hidden states for batch 4\n",
      "Processing batches:  12%|█▎        | 4/32 [00:03<00:27,  1.03it/s]2025-01-10 14:50:57,033 [INFO] Batch 5: Hidden states shape: torch.Size([32, 86, 768])\n",
      "2025-01-10 14:50:57,035 [INFO] Encoding hidden states for batch 5\n",
      "Processing batches:  16%|█▌        | 5/32 [00:04<00:22,  1.18it/s]2025-01-10 14:50:57,762 [INFO] Batch 6: Hidden states shape: torch.Size([32, 100, 768])\n",
      "2025-01-10 14:50:57,764 [INFO] Encoding hidden states for batch 6\n",
      "Processing batches:  19%|█▉        | 6/32 [00:05<00:21,  1.22it/s]2025-01-10 14:50:58,589 [INFO] Batch 7: Hidden states shape: torch.Size([32, 121, 768])\n",
      "2025-01-10 14:50:58,591 [INFO] Encoding hidden states for batch 7\n",
      "Processing batches:  22%|██▏       | 7/32 [00:06<00:20,  1.21it/s]2025-01-10 14:50:59,629 [INFO] Batch 8: Hidden states shape: torch.Size([32, 139, 768])\n",
      "2025-01-10 14:50:59,631 [INFO] Encoding hidden states for batch 8\n",
      "Processing batches:  25%|██▌       | 8/32 [00:07<00:21,  1.11it/s]2025-01-10 14:51:00,713 [INFO] Batch 9: Hidden states shape: torch.Size([32, 140, 768])\n",
      "2025-01-10 14:51:00,715 [INFO] Encoding hidden states for batch 9\n",
      "Processing batches:  28%|██▊       | 9/32 [00:08<00:21,  1.05it/s]2025-01-10 14:51:01,624 [INFO] Batch 10: Hidden states shape: torch.Size([32, 129, 768])\n",
      "2025-01-10 14:51:01,627 [INFO] Encoding hidden states for batch 10\n",
      "Processing batches:  31%|███▏      | 10/32 [00:09<00:20,  1.06it/s]2025-01-10 14:51:02,340 [INFO] Batch 11: Hidden states shape: torch.Size([32, 100, 768])\n",
      "2025-01-10 14:51:02,341 [INFO] Encoding hidden states for batch 11\n",
      "Processing batches:  34%|███▍      | 11/32 [00:09<00:18,  1.16it/s]2025-01-10 14:51:03,147 [INFO] Batch 12: Hidden states shape: torch.Size([32, 118, 768])\n",
      "2025-01-10 14:51:03,149 [INFO] Encoding hidden states for batch 12\n",
      "Processing batches:  38%|███▊      | 12/32 [00:10<00:17,  1.18it/s]2025-01-10 14:51:03,795 [INFO] Batch 13: Hidden states shape: torch.Size([32, 90, 768])\n",
      "2025-01-10 14:51:03,797 [INFO] Encoding hidden states for batch 13\n",
      "Processing batches:  41%|████      | 13/32 [00:11<00:14,  1.28it/s]2025-01-10 14:51:04,540 [INFO] Batch 14: Hidden states shape: torch.Size([32, 98, 768])\n",
      "2025-01-10 14:51:04,542 [INFO] Encoding hidden states for batch 14\n",
      "Processing batches:  44%|████▍     | 14/32 [00:12<00:13,  1.29it/s]2025-01-10 14:51:05,269 [INFO] Batch 15: Hidden states shape: torch.Size([32, 106, 768])\n",
      "2025-01-10 14:51:05,271 [INFO] Encoding hidden states for batch 15\n",
      "Processing batches:  47%|████▋     | 15/32 [00:12<00:12,  1.31it/s]2025-01-10 14:51:06,256 [INFO] Batch 16: Hidden states shape: torch.Size([32, 139, 768])\n",
      "2025-01-10 14:51:06,258 [INFO] Encoding hidden states for batch 16\n",
      "Processing batches:  50%|█████     | 16/32 [00:13<00:13,  1.19it/s]2025-01-10 14:51:07,228 [INFO] Batch 17: Hidden states shape: torch.Size([32, 130, 768])\n",
      "2025-01-10 14:51:07,230 [INFO] Encoding hidden states for batch 17\n",
      "Processing batches:  53%|█████▎    | 17/32 [00:14<00:13,  1.14it/s]2025-01-10 14:51:08,321 [INFO] Batch 18: Hidden states shape: torch.Size([32, 137, 768])\n",
      "2025-01-10 14:51:08,323 [INFO] Encoding hidden states for batch 18\n",
      "Processing batches:  56%|█████▋    | 18/32 [00:15<00:13,  1.06it/s]2025-01-10 14:51:09,244 [INFO] Batch 19: Hidden states shape: torch.Size([32, 110, 768])\n",
      "2025-01-10 14:51:09,246 [INFO] Encoding hidden states for batch 19\n",
      "Processing batches:  59%|█████▉    | 19/32 [00:16<00:12,  1.07it/s]2025-01-10 14:51:10,310 [INFO] Batch 20: Hidden states shape: torch.Size([32, 142, 768])\n",
      "2025-01-10 14:51:10,311 [INFO] Encoding hidden states for batch 20\n",
      "Processing batches:  62%|██████▎   | 20/32 [00:17<00:11,  1.02it/s]2025-01-10 14:51:11,140 [INFO] Batch 21: Hidden states shape: torch.Size([32, 113, 768])\n",
      "2025-01-10 14:51:11,141 [INFO] Encoding hidden states for batch 21\n",
      "Processing batches:  66%|██████▌   | 21/32 [00:18<00:10,  1.08it/s]2025-01-10 14:51:12,063 [INFO] Batch 22: Hidden states shape: torch.Size([32, 132, 768])\n",
      "2025-01-10 14:51:12,065 [INFO] Encoding hidden states for batch 22\n",
      "Processing batches:  69%|██████▉   | 22/32 [00:19<00:09,  1.08it/s]2025-01-10 14:51:12,926 [INFO] Batch 23: Hidden states shape: torch.Size([32, 112, 768])\n",
      "2025-01-10 14:51:12,928 [INFO] Encoding hidden states for batch 23\n",
      "Processing batches:  72%|███████▏  | 23/32 [00:20<00:08,  1.10it/s]2025-01-10 14:51:13,590 [INFO] Batch 24: Hidden states shape: torch.Size([32, 97, 768])\n",
      "2025-01-10 14:51:13,592 [INFO] Encoding hidden states for batch 24\n",
      "Processing batches:  75%|███████▌  | 24/32 [00:21<00:06,  1.21it/s]2025-01-10 14:51:14,297 [INFO] Batch 25: Hidden states shape: torch.Size([32, 106, 768])\n",
      "2025-01-10 14:51:14,299 [INFO] Encoding hidden states for batch 25\n",
      "Processing batches:  78%|███████▊  | 25/32 [00:21<00:05,  1.26it/s]2025-01-10 14:51:15,179 [INFO] Batch 26: Hidden states shape: torch.Size([32, 130, 768])\n",
      "2025-01-10 14:51:15,181 [INFO] Encoding hidden states for batch 26\n",
      "Processing batches:  81%|████████▏ | 26/32 [00:22<00:04,  1.21it/s]2025-01-10 14:51:16,119 [INFO] Batch 27: Hidden states shape: torch.Size([32, 131, 768])\n",
      "2025-01-10 14:51:16,121 [INFO] Encoding hidden states for batch 27\n",
      "Processing batches:  84%|████████▍ | 27/32 [00:23<00:04,  1.16it/s]2025-01-10 14:51:16,880 [INFO] Batch 28: Hidden states shape: torch.Size([32, 110, 768])\n",
      "2025-01-10 14:51:16,882 [INFO] Encoding hidden states for batch 28\n",
      "Processing batches:  88%|████████▊ | 28/32 [00:24<00:03,  1.21it/s]2025-01-10 14:51:17,540 [INFO] Batch 29: Hidden states shape: torch.Size([32, 104, 768])\n",
      "2025-01-10 14:51:17,542 [INFO] Encoding hidden states for batch 29\n",
      "Processing batches:  91%|█████████ | 29/32 [00:25<00:02,  1.29it/s]2025-01-10 14:51:18,180 [INFO] Batch 30: Hidden states shape: torch.Size([32, 96, 768])\n",
      "2025-01-10 14:51:18,182 [INFO] Encoding hidden states for batch 30\n",
      "Processing batches:  94%|█████████▍| 30/32 [00:25<00:01,  1.37it/s]2025-01-10 14:51:19,217 [INFO] Batch 31: Hidden states shape: torch.Size([32, 149, 768])\n",
      "2025-01-10 14:51:19,219 [INFO] Encoding hidden states for batch 31\n",
      "Processing batches:  97%|█████████▋| 31/32 [00:26<00:00,  1.19it/s]2025-01-10 14:51:19,687 [INFO] Batch 32: Hidden states shape: torch.Size([8, 85, 768])\n",
      "2025-01-10 14:51:19,689 [INFO] Encoding hidden states for batch 32\n",
      "Processing batches: 100%|██████████| 32/32 [00:27<00:00,  1.18it/s]\n",
      "2025-01-10 14:51:19,702 [INFO] Total batches processed: 32\n",
      "2025-01-10 14:51:19,705 [INFO] 最大长度:155\n",
      "2025-01-10 14:51:20,805 [INFO] Concatenated batch latents shape: torch.Size([1000, 155, 24576])\n",
      "2025-01-10 14:51:20,806 [INFO] Computing non-zero element counts\n",
      "2025-01-10 14:51:23,370 [INFO] Computing sum of non-zero elements\n",
      "2025-01-10 14:51:24,709 [INFO] Computing mean of non-zero elements\n",
      "2025-01-10 14:51:24,711 [INFO] Selecting top-k indices based on nz_mean\n",
      "2025-01-10 14:51:24,712 [INFO] Top 100 nz_mean values selected.\n",
      "2025-01-10 14:51:24,712 [INFO] Selecting top-k indices based on act_cnt\n",
      "2025-01-10 14:51:24,713 [INFO] Top 100 act_cnt values selected.\n",
      "2025-01-10 14:51:25,812 [INFO] Running model with cache to obtain hidden states\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/32 [00:00<?, ?it/s]2025-01-10 14:51:29,259 [INFO] Batch 1: Hidden states shape: torch.Size([32, 394, 768])\n",
      "2025-01-10 14:51:29,259 [INFO] Encoding hidden states for batch 1\n",
      "Processing batches:   3%|▎         | 1/32 [00:03<01:56,  3.74s/it]2025-01-10 14:51:32,466 [INFO] Batch 2: Hidden states shape: torch.Size([32, 266, 768])\n",
      "2025-01-10 14:51:32,468 [INFO] Encoding hidden states for batch 2\n",
      "Processing batches:   6%|▋         | 2/32 [00:06<01:41,  3.39s/it]2025-01-10 14:51:35,617 [INFO] Batch 3: Hidden states shape: torch.Size([32, 341, 768])\n",
      "2025-01-10 14:51:35,619 [INFO] Encoding hidden states for batch 3\n",
      "Processing batches:   9%|▉         | 3/32 [00:10<01:35,  3.29s/it]2025-01-10 14:51:39,830 [INFO] Batch 4: Hidden states shape: torch.Size([32, 379, 768])\n",
      "2025-01-10 14:51:39,832 [INFO] Encoding hidden states for batch 4\n",
      "Processing batches:  12%|█▎        | 4/32 [00:14<01:42,  3.68s/it]2025-01-10 14:51:43,079 [INFO] Batch 5: Hidden states shape: torch.Size([32, 272, 768])\n",
      "2025-01-10 14:51:43,081 [INFO] Encoding hidden states for batch 5\n",
      "Processing batches:  16%|█▌        | 5/32 [00:17<01:34,  3.49s/it]2025-01-10 14:51:45,933 [INFO] Batch 6: Hidden states shape: torch.Size([32, 281, 768])\n",
      "2025-01-10 14:51:45,935 [INFO] Encoding hidden states for batch 6\n",
      "Processing batches:  19%|█▉        | 6/32 [00:20<01:25,  3.28s/it]2025-01-10 14:51:48,571 [INFO] Batch 7: Hidden states shape: torch.Size([32, 286, 768])\n",
      "2025-01-10 14:51:48,573 [INFO] Encoding hidden states for batch 7\n",
      "Processing batches:  22%|██▏       | 7/32 [00:22<01:16,  3.07s/it]2025-01-10 14:51:51,453 [INFO] Batch 8: Hidden states shape: torch.Size([32, 302, 768])\n",
      "2025-01-10 14:51:51,455 [INFO] Encoding hidden states for batch 8\n",
      "Processing batches:  25%|██▌       | 8/32 [00:25<01:12,  3.01s/it]2025-01-10 14:51:54,702 [INFO] Batch 9: Hidden states shape: torch.Size([32, 328, 768])\n",
      "2025-01-10 14:51:54,704 [INFO] Encoding hidden states for batch 9\n",
      "Processing batches:  28%|██▊       | 9/32 [00:29<01:11,  3.09s/it]2025-01-10 14:51:58,806 [INFO] Batch 10: Hidden states shape: torch.Size([32, 393, 768])\n",
      "2025-01-10 14:51:58,808 [INFO] Encoding hidden states for batch 10\n",
      "Processing batches:  31%|███▏      | 10/32 [00:33<01:15,  3.42s/it]2025-01-10 14:52:01,794 [INFO] Batch 11: Hidden states shape: torch.Size([32, 236, 768])\n",
      "2025-01-10 14:52:01,796 [INFO] Encoding hidden states for batch 11\n",
      "Processing batches:  34%|███▍      | 11/32 [00:36<01:08,  3.25s/it]2025-01-10 14:52:04,532 [INFO] Batch 12: Hidden states shape: torch.Size([32, 312, 768])\n",
      "2025-01-10 14:52:04,534 [INFO] Encoding hidden states for batch 12\n",
      "Processing batches:  38%|███▊      | 12/32 [00:38<01:02,  3.11s/it]2025-01-10 14:52:06,907 [INFO] Batch 13: Hidden states shape: torch.Size([32, 245, 768])\n",
      "2025-01-10 14:52:06,909 [INFO] Encoding hidden states for batch 13\n",
      "Processing batches:  41%|████      | 13/32 [00:41<00:54,  2.87s/it]2025-01-10 14:52:09,293 [INFO] Batch 14: Hidden states shape: torch.Size([32, 273, 768])\n",
      "2025-01-10 14:52:09,296 [INFO] Encoding hidden states for batch 14\n",
      "Processing batches:  44%|████▍     | 14/32 [00:43<00:49,  2.73s/it]2025-01-10 14:52:11,755 [INFO] Batch 15: Hidden states shape: torch.Size([32, 268, 768])\n",
      "2025-01-10 14:52:11,756 [INFO] Encoding hidden states for batch 15\n",
      "Processing batches:  47%|████▋     | 15/32 [00:46<00:44,  2.65s/it]2025-01-10 14:52:14,409 [INFO] Batch 16: Hidden states shape: torch.Size([32, 298, 768])\n",
      "2025-01-10 14:52:14,411 [INFO] Encoding hidden states for batch 16\n",
      "Processing batches:  50%|█████     | 16/32 [00:48<00:42,  2.66s/it]2025-01-10 14:52:17,172 [INFO] Batch 17: Hidden states shape: torch.Size([32, 291, 768])\n",
      "2025-01-10 14:52:17,175 [INFO] Encoding hidden states for batch 17\n",
      "Processing batches:  53%|█████▎    | 17/32 [00:51<00:40,  2.69s/it]2025-01-10 14:52:19,578 [INFO] Batch 18: Hidden states shape: torch.Size([32, 262, 768])\n",
      "2025-01-10 14:52:19,581 [INFO] Encoding hidden states for batch 18\n",
      "Processing batches:  56%|█████▋    | 18/32 [00:53<00:36,  2.60s/it]2025-01-10 14:52:21,884 [INFO] Batch 19: Hidden states shape: torch.Size([32, 255, 768])\n",
      "2025-01-10 14:52:21,886 [INFO] Encoding hidden states for batch 19\n",
      "Processing batches:  59%|█████▉    | 19/32 [00:56<00:32,  2.51s/it]2025-01-10 14:52:24,533 [INFO] Batch 20: Hidden states shape: torch.Size([32, 289, 768])\n",
      "2025-01-10 14:52:24,536 [INFO] Encoding hidden states for batch 20\n",
      "Processing batches:  62%|██████▎   | 20/32 [00:58<00:30,  2.56s/it]2025-01-10 14:52:27,140 [INFO] Batch 21: Hidden states shape: torch.Size([32, 279, 768])\n",
      "2025-01-10 14:52:27,143 [INFO] Encoding hidden states for batch 21\n",
      "Processing batches:  66%|██████▌   | 21/32 [01:01<00:28,  2.57s/it]2025-01-10 14:52:29,766 [INFO] Batch 22: Hidden states shape: torch.Size([32, 291, 768])\n",
      "2025-01-10 14:52:29,769 [INFO] Encoding hidden states for batch 22\n",
      "Processing batches:  69%|██████▉   | 22/32 [01:04<00:25,  2.59s/it]2025-01-10 14:52:32,617 [INFO] Batch 23: Hidden states shape: torch.Size([32, 310, 768])\n",
      "2025-01-10 14:52:32,619 [INFO] Encoding hidden states for batch 23\n",
      "Processing batches:  72%|███████▏  | 23/32 [01:07<00:24,  2.67s/it]2025-01-10 14:52:35,766 [INFO] Batch 24: Hidden states shape: torch.Size([32, 329, 768])\n",
      "2025-01-10 14:52:35,768 [INFO] Encoding hidden states for batch 24\n",
      "Processing batches:  75%|███████▌  | 24/32 [01:10<00:22,  2.82s/it]2025-01-10 14:52:38,742 [INFO] Batch 25: Hidden states shape: torch.Size([32, 310, 768])\n",
      "2025-01-10 14:52:38,744 [INFO] Encoding hidden states for batch 25\n",
      "Processing batches:  78%|███████▊  | 25/32 [01:13<00:20,  2.87s/it]2025-01-10 14:52:41,289 [INFO] Batch 26: Hidden states shape: torch.Size([32, 264, 768])\n",
      "2025-01-10 14:52:41,292 [INFO] Encoding hidden states for batch 26\n",
      "Processing batches:  81%|████████▏ | 26/32 [01:15<00:16,  2.76s/it]2025-01-10 14:52:44,159 [INFO] Batch 27: Hidden states shape: torch.Size([32, 311, 768])\n",
      "2025-01-10 14:52:44,160 [INFO] Encoding hidden states for batch 27\n",
      "Processing batches:  84%|████████▍ | 27/32 [01:18<00:14,  2.80s/it]2025-01-10 14:52:47,331 [INFO] Batch 28: Hidden states shape: torch.Size([32, 338, 768])\n",
      "2025-01-10 14:52:47,333 [INFO] Encoding hidden states for batch 28\n",
      "Processing batches:  88%|████████▊ | 28/32 [01:21<00:11,  2.92s/it]2025-01-10 14:52:51,184 [INFO] Batch 29: Hidden states shape: torch.Size([32, 354, 768])\n",
      "2025-01-10 14:52:51,186 [INFO] Encoding hidden states for batch 29\n",
      "Processing batches:  91%|█████████ | 29/32 [01:25<00:09,  3.20s/it]2025-01-10 14:52:54,198 [INFO] Batch 30: Hidden states shape: torch.Size([32, 276, 768])\n",
      "2025-01-10 14:52:54,200 [INFO] Encoding hidden states for batch 30\n",
      "Processing batches:  94%|█████████▍| 30/32 [01:28<00:06,  3.13s/it]2025-01-10 14:52:57,197 [INFO] Batch 31: Hidden states shape: torch.Size([32, 317, 768])\n",
      "2025-01-10 14:52:57,199 [INFO] Encoding hidden states for batch 31\n",
      "Processing batches:  97%|█████████▋| 31/32 [01:31<00:03,  3.10s/it]2025-01-10 14:52:58,358 [INFO] Batch 32: Hidden states shape: torch.Size([8, 212, 768])\n",
      "2025-01-10 14:52:58,360 [INFO] Encoding hidden states for batch 32\n",
      "Processing batches: 100%|██████████| 32/32 [01:32<00:00,  2.89s/it]\n",
      "2025-01-10 14:52:58,385 [INFO] Total batches processed: 32\n",
      "2025-01-10 14:52:58,404 [INFO] 最大长度:394\n",
      "2025-01-10 14:53:01,324 [INFO] Concatenated batch latents shape: torch.Size([1000, 394, 24576])\n",
      "2025-01-10 14:53:01,325 [INFO] Computing non-zero element counts\n",
      "2025-01-10 14:53:08,117 [INFO] Computing sum of non-zero elements\n",
      "2025-01-10 14:53:11,605 [INFO] Computing mean of non-zero elements\n",
      "2025-01-10 14:53:11,607 [INFO] Selecting top-k indices based on nz_mean\n",
      "2025-01-10 14:53:11,608 [INFO] Top 100 nz_mean values selected.\n",
      "2025-01-10 14:53:11,608 [INFO] Selecting top-k indices based on act_cnt\n",
      "2025-01-10 14:53:11,609 [INFO] Top 100 act_cnt values selected.\n"
     ]
    }
   ],
   "source": [
    "steer_info={}\n",
    "if \"cot\" in args.steer:\n",
    "    if args.steer in \"cot-direct\":\n",
    "        texts=all_dataset[\"train\"][\"Q+A\"][:args.data_size]\n",
    "        print(type(texts))\n",
    "        steer_info[\"direct\"]=get_activation_by_steer(texts)\n",
    "        \n",
    "        texts=all_dataset[\"train\"][\"Q+COT_A\"][:args.data_size]\n",
    "        print(type(texts))\n",
    "        steer_info[\"cot\"]=get_activation_by_steer(texts)\n",
    "        # print(texts[123])\n",
    "    else:\n",
    "        raise ValueError(\"????\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 09:47:42,058 [INFO] Running model with cache to obtain hidden states\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/32 [00:00<?, ?it/s]2025-01-10 09:47:42,885 [INFO] Batch 1: Hidden states shape: torch.Size([32, 99, 768])\n",
      "2025-01-10 09:47:42,886 [INFO] Encoding hidden states for batch 1\n",
      "Processing batches:   3%|▎         | 1/32 [00:00<00:28,  1.08it/s]2025-01-10 09:47:43,898 [INFO] Batch 2: Hidden states shape: torch.Size([32, 94, 768])\n",
      "2025-01-10 09:47:43,899 [INFO] Encoding hidden states for batch 2\n",
      "Processing batches:   6%|▋         | 2/32 [00:01<00:29,  1.03it/s]2025-01-10 09:47:44,577 [INFO] Batch 3: Hidden states shape: torch.Size([32, 84, 768])\n",
      "2025-01-10 09:47:44,578 [INFO] Encoding hidden states for batch 3\n",
      "Processing batches:   9%|▉         | 3/32 [00:02<00:23,  1.21it/s]2025-01-10 09:47:45,452 [INFO] Batch 4: Hidden states shape: torch.Size([32, 116, 768])\n",
      "2025-01-10 09:47:45,453 [INFO] Encoding hidden states for batch 4\n",
      "Processing batches:  12%|█▎        | 4/32 [00:03<00:23,  1.19it/s]2025-01-10 09:47:46,178 [INFO] Batch 5: Hidden states shape: torch.Size([32, 109, 768])\n",
      "2025-01-10 09:47:46,180 [INFO] Encoding hidden states for batch 5\n",
      "Processing batches:  16%|█▌        | 5/32 [00:04<00:22,  1.22it/s]2025-01-10 09:47:46,744 [INFO] Batch 6: Hidden states shape: torch.Size([32, 75, 768])\n",
      "2025-01-10 09:47:46,746 [INFO] Encoding hidden states for batch 6\n",
      "Processing batches:  19%|█▉        | 6/32 [00:04<00:18,  1.41it/s]2025-01-10 09:47:47,513 [INFO] Batch 7: Hidden states shape: torch.Size([32, 119, 768])\n",
      "2025-01-10 09:47:47,514 [INFO] Encoding hidden states for batch 7\n",
      "Processing batches:  22%|██▏       | 7/32 [00:05<00:18,  1.33it/s]2025-01-10 09:47:48,529 [INFO] Batch 8: Hidden states shape: torch.Size([32, 121, 768])\n",
      "2025-01-10 09:47:48,531 [INFO] Encoding hidden states for batch 8\n",
      "Processing batches:  25%|██▌       | 8/32 [00:06<00:20,  1.20it/s]2025-01-10 09:47:50,136 [INFO] Batch 9: Hidden states shape: torch.Size([32, 79, 768])\n",
      "2025-01-10 09:47:50,137 [INFO] Encoding hidden states for batch 9\n",
      "Processing batches:  28%|██▊       | 9/32 [00:08<00:24,  1.05s/it]2025-01-10 09:47:50,777 [INFO] Batch 10: Hidden states shape: torch.Size([32, 104, 768])\n",
      "2025-01-10 09:47:50,779 [INFO] Encoding hidden states for batch 10\n",
      "Processing batches:  31%|███▏      | 10/32 [00:08<00:20,  1.07it/s]2025-01-10 09:47:51,452 [INFO] Batch 11: Hidden states shape: torch.Size([32, 113, 768])\n",
      "2025-01-10 09:47:51,452 [INFO] Encoding hidden states for batch 11\n",
      "Processing batches:  34%|███▍      | 11/32 [00:09<00:18,  1.15it/s]2025-01-10 09:47:51,850 [INFO] Batch 12: Hidden states shape: torch.Size([32, 44, 768])\n",
      "2025-01-10 09:47:51,851 [INFO] Encoding hidden states for batch 12\n",
      "Processing batches:  38%|███▊      | 12/32 [00:09<00:13,  1.43it/s]2025-01-10 09:47:52,281 [INFO] Batch 13: Hidden states shape: torch.Size([32, 95, 768])\n",
      "2025-01-10 09:47:52,281 [INFO] Encoding hidden states for batch 13\n",
      "Processing batches:  41%|████      | 13/32 [00:10<00:11,  1.60it/s]2025-01-10 09:47:52,936 [INFO] Batch 14: Hidden states shape: torch.Size([32, 108, 768])\n",
      "2025-01-10 09:47:52,938 [INFO] Encoding hidden states for batch 14\n",
      "Processing batches:  44%|████▍     | 14/32 [00:10<00:11,  1.58it/s]2025-01-10 09:47:53,643 [INFO] Batch 15: Hidden states shape: torch.Size([32, 108, 768])\n",
      "2025-01-10 09:47:53,644 [INFO] Encoding hidden states for batch 15\n",
      "Processing batches:  47%|████▋     | 15/32 [00:11<00:11,  1.49it/s]2025-01-10 09:47:54,508 [INFO] Batch 16: Hidden states shape: torch.Size([32, 115, 768])\n",
      "2025-01-10 09:47:54,510 [INFO] Encoding hidden states for batch 16\n",
      "Processing batches:  50%|█████     | 16/32 [00:12<00:11,  1.38it/s]2025-01-10 09:47:55,148 [INFO] Batch 17: Hidden states shape: torch.Size([32, 106, 768])\n",
      "2025-01-10 09:47:55,150 [INFO] Encoding hidden states for batch 17\n",
      "Processing batches:  53%|█████▎    | 17/32 [00:13<00:10,  1.41it/s]2025-01-10 09:47:55,624 [INFO] Batch 18: Hidden states shape: torch.Size([32, 68, 768])\n",
      "2025-01-10 09:47:55,626 [INFO] Encoding hidden states for batch 18\n",
      "Processing batches:  56%|█████▋    | 18/32 [00:13<00:08,  1.59it/s]2025-01-10 09:47:56,268 [INFO] Batch 19: Hidden states shape: torch.Size([32, 107, 768])\n",
      "2025-01-10 09:47:56,269 [INFO] Encoding hidden states for batch 19\n",
      "Processing batches:  59%|█████▉    | 19/32 [00:14<00:08,  1.56it/s]2025-01-10 09:47:57,045 [INFO] Batch 20: Hidden states shape: torch.Size([32, 113, 768])\n",
      "2025-01-10 09:47:57,046 [INFO] Encoding hidden states for batch 20\n",
      "Processing batches:  62%|██████▎   | 20/32 [00:15<00:08,  1.46it/s]2025-01-10 09:47:57,695 [INFO] Batch 21: Hidden states shape: torch.Size([32, 94, 768])\n",
      "2025-01-10 09:47:57,697 [INFO] Encoding hidden states for batch 21\n",
      "Processing batches:  66%|██████▌   | 21/32 [00:15<00:07,  1.50it/s]2025-01-10 09:47:58,475 [INFO] Batch 22: Hidden states shape: torch.Size([32, 118, 768])\n",
      "2025-01-10 09:47:58,477 [INFO] Encoding hidden states for batch 22\n",
      "Processing batches:  69%|██████▉   | 22/32 [00:16<00:07,  1.43it/s]2025-01-10 09:47:59,083 [INFO] Batch 23: Hidden states shape: torch.Size([32, 94, 768])\n",
      "2025-01-10 09:47:59,084 [INFO] Encoding hidden states for batch 23\n",
      "Processing batches:  72%|███████▏  | 23/32 [00:17<00:06,  1.48it/s]2025-01-10 09:48:00,393 [INFO] Batch 24: Hidden states shape: torch.Size([32, 177, 768])\n",
      "2025-01-10 09:48:00,395 [INFO] Encoding hidden states for batch 24\n",
      "Processing batches:  75%|███████▌  | 24/32 [00:18<00:07,  1.13it/s]2025-01-10 09:48:01,180 [INFO] Batch 25: Hidden states shape: torch.Size([32, 80, 768])\n",
      "2025-01-10 09:48:01,181 [INFO] Encoding hidden states for batch 25\n",
      "Processing batches:  78%|███████▊  | 25/32 [00:19<00:05,  1.21it/s]2025-01-10 09:48:01,747 [INFO] Batch 26: Hidden states shape: torch.Size([32, 106, 768])\n",
      "2025-01-10 09:48:01,748 [INFO] Encoding hidden states for batch 26\n",
      "Processing batches:  81%|████████▏ | 26/32 [00:19<00:04,  1.32it/s]2025-01-10 09:48:02,507 [INFO] Batch 27: Hidden states shape: torch.Size([32, 115, 768])\n",
      "2025-01-10 09:48:02,508 [INFO] Encoding hidden states for batch 27\n",
      "Processing batches:  84%|████████▍ | 27/32 [00:20<00:03,  1.31it/s]2025-01-10 09:48:03,164 [INFO] Batch 28: Hidden states shape: torch.Size([32, 106, 768])\n",
      "2025-01-10 09:48:03,165 [INFO] Encoding hidden states for batch 28\n",
      "Processing batches:  88%|████████▊ | 28/32 [00:21<00:02,  1.37it/s]2025-01-10 09:48:03,593 [INFO] Batch 29: Hidden states shape: torch.Size([32, 63, 768])\n",
      "2025-01-10 09:48:03,595 [INFO] Encoding hidden states for batch 29\n",
      "Processing batches:  91%|█████████ | 29/32 [00:21<00:01,  1.62it/s]2025-01-10 09:48:04,127 [INFO] Batch 30: Hidden states shape: torch.Size([32, 112, 768])\n",
      "2025-01-10 09:48:04,128 [INFO] Encoding hidden states for batch 30\n",
      "Processing batches:  94%|█████████▍| 30/32 [00:22<00:01,  1.63it/s]2025-01-10 09:48:04,878 [INFO] Batch 31: Hidden states shape: torch.Size([32, 111, 768])\n",
      "2025-01-10 09:48:04,879 [INFO] Encoding hidden states for batch 31\n",
      "Processing batches:  97%|█████████▋| 31/32 [00:22<00:00,  1.52it/s]2025-01-10 09:48:05,184 [INFO] Batch 32: Hidden states shape: torch.Size([8, 46, 768])\n",
      "2025-01-10 09:48:05,186 [INFO] Encoding hidden states for batch 32\n",
      "Processing batches: 100%|██████████| 32/32 [00:23<00:00,  1.38it/s]\n",
      "2025-01-10 09:48:05,196 [INFO] Total batches processed: 32\n",
      "2025-01-10 09:48:05,199 [INFO] 最大长度:177\n",
      "2025-01-10 09:48:06,474 [INFO] Concatenated batch latents shape: torch.Size([1000, 177, 24576])\n",
      "2025-01-10 09:48:06,475 [INFO] Computing non-zero element counts\n",
      "2025-01-10 09:48:09,508 [INFO] Computing sum of non-zero elements\n",
      "2025-01-10 09:48:11,069 [INFO] Computing mean of non-zero elements\n",
      "2025-01-10 09:48:11,070 [INFO] Selecting top-k indices based on nz_mean\n",
      "2025-01-10 09:48:11,071 [INFO] Top 100 nz_mean values selected.\n",
      "2025-01-10 09:48:11,072 [INFO] Selecting top-k indices based on act_cnt\n",
      "2025-01-10 09:48:11,073 [INFO] Top 100 act_cnt values selected.\n",
      "2025-01-10 09:48:12,375 [INFO] Running model with cache to obtain hidden states\n",
      "Processing batches:   0%|          | 0/32 [00:00<?, ?it/s]2025-01-10 09:48:12,628 [INFO] Batch 1: Hidden states shape: torch.Size([32, 55, 768])\n",
      "2025-01-10 09:48:12,629 [INFO] Encoding hidden states for batch 1\n",
      "Processing batches:   3%|▎         | 1/32 [00:00<00:08,  3.66it/s]2025-01-10 09:48:12,873 [INFO] Batch 2: Hidden states shape: torch.Size([32, 46, 768])\n",
      "2025-01-10 09:48:12,874 [INFO] Encoding hidden states for batch 2\n",
      "Processing batches:   6%|▋         | 2/32 [00:00<00:07,  3.92it/s]2025-01-10 09:48:13,150 [INFO] Batch 3: Hidden states shape: torch.Size([32, 56, 768])\n",
      "2025-01-10 09:48:13,151 [INFO] Encoding hidden states for batch 3\n",
      "Processing batches:   9%|▉         | 3/32 [00:00<00:07,  3.75it/s]2025-01-10 09:48:13,516 [INFO] Batch 4: Hidden states shape: torch.Size([32, 58, 768])\n",
      "2025-01-10 09:48:13,517 [INFO] Encoding hidden states for batch 4\n",
      "Processing batches:  12%|█▎        | 4/32 [00:01<00:08,  3.27it/s]2025-01-10 09:48:14,103 [INFO] Batch 5: Hidden states shape: torch.Size([32, 112, 768])\n",
      "2025-01-10 09:48:14,105 [INFO] Encoding hidden states for batch 5\n",
      "Processing batches:  16%|█▌        | 5/32 [00:01<00:11,  2.31it/s]2025-01-10 09:48:14,549 [INFO] Batch 6: Hidden states shape: torch.Size([32, 51, 768])\n",
      "2025-01-10 09:48:14,551 [INFO] Encoding hidden states for batch 6\n",
      "Processing batches:  19%|█▉        | 6/32 [00:02<00:10,  2.42it/s]2025-01-10 09:48:14,825 [INFO] Batch 7: Hidden states shape: torch.Size([32, 54, 768])\n",
      "2025-01-10 09:48:14,827 [INFO] Encoding hidden states for batch 7\n",
      "Processing batches:  22%|██▏       | 7/32 [00:02<00:09,  2.72it/s]2025-01-10 09:48:15,812 [INFO] Batch 8: Hidden states shape: torch.Size([32, 161, 768])\n",
      "2025-01-10 09:48:15,813 [INFO] Encoding hidden states for batch 8\n",
      "Processing batches:  25%|██▌       | 8/32 [00:03<00:14,  1.66it/s]2025-01-10 09:48:16,942 [INFO] Batch 9: Hidden states shape: torch.Size([32, 138, 768])\n",
      "2025-01-10 09:48:16,944 [INFO] Encoding hidden states for batch 9\n",
      "Processing batches:  28%|██▊       | 9/32 [00:04<00:17,  1.31it/s]2025-01-10 09:48:17,817 [INFO] Batch 10: Hidden states shape: torch.Size([32, 114, 768])\n",
      "2025-01-10 09:48:17,819 [INFO] Encoding hidden states for batch 10\n",
      "Processing batches:  31%|███▏      | 10/32 [00:05<00:17,  1.27it/s]2025-01-10 09:48:18,522 [INFO] Batch 11: Hidden states shape: torch.Size([32, 107, 768])\n",
      "2025-01-10 09:48:18,524 [INFO] Encoding hidden states for batch 11\n",
      "Processing batches:  34%|███▍      | 11/32 [00:06<00:16,  1.31it/s]2025-01-10 09:48:19,287 [INFO] Batch 12: Hidden states shape: torch.Size([32, 115, 768])\n",
      "2025-01-10 09:48:19,289 [INFO] Encoding hidden states for batch 12\n",
      "Processing batches:  38%|███▊      | 12/32 [00:07<00:15,  1.31it/s]2025-01-10 09:48:19,842 [INFO] Batch 13: Hidden states shape: torch.Size([32, 82, 768])\n",
      "2025-01-10 09:48:19,844 [INFO] Encoding hidden states for batch 13\n",
      "Processing batches:  41%|████      | 13/32 [00:07<00:13,  1.44it/s]2025-01-10 09:48:20,598 [INFO] Batch 14: Hidden states shape: torch.Size([32, 122, 768])\n",
      "2025-01-10 09:48:20,600 [INFO] Encoding hidden states for batch 14\n",
      "Processing batches:  44%|████▍     | 14/32 [00:08<00:12,  1.39it/s]2025-01-10 09:48:21,045 [INFO] Batch 15: Hidden states shape: torch.Size([32, 45, 768])\n",
      "2025-01-10 09:48:21,046 [INFO] Encoding hidden states for batch 15\n",
      "Processing batches:  47%|████▋     | 15/32 [00:08<00:10,  1.63it/s]2025-01-10 09:48:21,523 [INFO] Batch 16: Hidden states shape: torch.Size([32, 106, 768])\n",
      "2025-01-10 09:48:21,524 [INFO] Encoding hidden states for batch 16\n",
      "Processing batches:  50%|█████     | 16/32 [00:09<00:09,  1.69it/s]2025-01-10 09:48:21,880 [INFO] Batch 17: Hidden states shape: torch.Size([32, 48, 768])\n",
      "2025-01-10 09:48:21,882 [INFO] Encoding hidden states for batch 17\n",
      "Processing batches:  53%|█████▎    | 17/32 [00:09<00:07,  1.99it/s]2025-01-10 09:48:22,308 [INFO] Batch 18: Hidden states shape: torch.Size([32, 93, 768])\n",
      "2025-01-10 09:48:22,309 [INFO] Encoding hidden states for batch 18\n",
      "Processing batches:  56%|█████▋    | 18/32 [00:10<00:06,  2.01it/s]2025-01-10 09:48:22,728 [INFO] Batch 19: Hidden states shape: torch.Size([32, 73, 768])\n",
      "2025-01-10 09:48:22,730 [INFO] Encoding hidden states for batch 19\n",
      "Processing batches:  59%|█████▉    | 19/32 [00:10<00:05,  2.18it/s]2025-01-10 09:48:23,340 [INFO] Batch 20: Hidden states shape: torch.Size([32, 104, 768])\n",
      "2025-01-10 09:48:23,342 [INFO] Encoding hidden states for batch 20\n",
      "Processing batches:  62%|██████▎   | 20/32 [00:11<00:06,  1.92it/s]2025-01-10 09:48:23,999 [INFO] Batch 21: Hidden states shape: torch.Size([32, 99, 768])\n",
      "2025-01-10 09:48:24,001 [INFO] Encoding hidden states for batch 21\n",
      "Processing batches:  66%|██████▌   | 21/32 [00:11<00:06,  1.77it/s]2025-01-10 09:48:24,737 [INFO] Batch 22: Hidden states shape: torch.Size([32, 109, 768])\n",
      "2025-01-10 09:48:24,739 [INFO] Encoding hidden states for batch 22\n",
      "Processing batches:  69%|██████▉   | 22/32 [00:12<00:06,  1.62it/s]2025-01-10 09:48:25,600 [INFO] Batch 23: Hidden states shape: torch.Size([32, 111, 768])\n",
      "2025-01-10 09:48:25,602 [INFO] Encoding hidden states for batch 23\n",
      "Processing batches:  72%|███████▏  | 23/32 [00:13<00:06,  1.44it/s]2025-01-10 09:48:26,086 [INFO] Batch 24: Hidden states shape: torch.Size([32, 67, 768])\n",
      "2025-01-10 09:48:26,089 [INFO] Encoding hidden states for batch 24\n",
      "Processing batches:  75%|███████▌  | 24/32 [00:13<00:04,  1.64it/s]2025-01-10 09:48:26,479 [INFO] Batch 25: Hidden states shape: torch.Size([32, 76, 768])\n",
      "2025-01-10 09:48:26,481 [INFO] Encoding hidden states for batch 25\n",
      "Processing batches:  78%|███████▊  | 25/32 [00:14<00:03,  1.81it/s]2025-01-10 09:48:27,125 [INFO] Batch 26: Hidden states shape: torch.Size([32, 95, 768])\n",
      "2025-01-10 09:48:27,127 [INFO] Encoding hidden states for batch 26\n",
      "Processing batches:  81%|████████▏ | 26/32 [00:14<00:03,  1.70it/s]2025-01-10 09:48:27,553 [INFO] Batch 27: Hidden states shape: torch.Size([32, 43, 768])\n",
      "2025-01-10 09:48:27,555 [INFO] Encoding hidden states for batch 27\n",
      "Processing batches:  84%|████████▍ | 27/32 [00:15<00:02,  1.92it/s]2025-01-10 09:48:27,776 [INFO] Batch 28: Hidden states shape: torch.Size([32, 38, 768])\n",
      "2025-01-10 09:48:27,778 [INFO] Encoding hidden states for batch 28\n",
      "Processing batches:  88%|████████▊ | 28/32 [00:15<00:01,  2.32it/s]2025-01-10 09:48:28,079 [INFO] Batch 29: Hidden states shape: torch.Size([32, 59, 768])\n",
      "2025-01-10 09:48:28,080 [INFO] Encoding hidden states for batch 29\n",
      "Processing batches:  91%|█████████ | 29/32 [00:15<00:01,  2.53it/s]2025-01-10 09:48:28,404 [INFO] Batch 30: Hidden states shape: torch.Size([32, 64, 768])\n",
      "2025-01-10 09:48:28,406 [INFO] Encoding hidden states for batch 30\n",
      "Processing batches:  94%|█████████▍| 30/32 [00:16<00:00,  2.67it/s]2025-01-10 09:48:28,731 [INFO] Batch 31: Hidden states shape: torch.Size([32, 57, 768])\n",
      "2025-01-10 09:48:28,733 [INFO] Encoding hidden states for batch 31\n",
      "Processing batches:  97%|█████████▋| 31/32 [00:16<00:00,  2.78it/s]2025-01-10 09:48:28,881 [INFO] Batch 32: Hidden states shape: torch.Size([8, 29, 768])\n",
      "2025-01-10 09:48:28,883 [INFO] Encoding hidden states for batch 32\n",
      "Processing batches: 100%|██████████| 32/32 [00:16<00:00,  1.94it/s]\n",
      "2025-01-10 09:48:28,891 [INFO] Total batches processed: 32\n",
      "2025-01-10 09:48:28,894 [INFO] 最大长度:161\n",
      "2025-01-10 09:48:30,023 [INFO] Concatenated batch latents shape: torch.Size([1000, 161, 24576])\n",
      "2025-01-10 09:48:30,059 [INFO] Computing non-zero element counts\n",
      "2025-01-10 09:48:32,759 [INFO] Computing sum of non-zero elements\n",
      "2025-01-10 09:48:34,185 [INFO] Computing mean of non-zero elements\n",
      "2025-01-10 09:48:34,187 [INFO] Selecting top-k indices based on nz_mean\n",
      "2025-01-10 09:48:34,188 [INFO] Top 100 nz_mean values selected.\n",
      "2025-01-10 09:48:34,189 [INFO] Selecting top-k indices based on act_cnt\n",
      "2025-01-10 09:48:34,190 [INFO] Top 100 act_cnt values selected.\n",
      "2025-01-10 09:48:35,346 [INFO] Running model with cache to obtain hidden states\n",
      "Processing batches:   0%|          | 0/32 [00:00<?, ?it/s]2025-01-10 09:48:36,347 [INFO] Batch 1: Hidden states shape: torch.Size([32, 47, 768])\n",
      "2025-01-10 09:48:36,348 [INFO] Encoding hidden states for batch 1\n",
      "Processing batches:   3%|▎         | 1/32 [00:01<00:31,  1.03s/it]2025-01-10 09:48:36,775 [INFO] Batch 2: Hidden states shape: torch.Size([32, 79, 768])\n",
      "2025-01-10 09:48:36,777 [INFO] Encoding hidden states for batch 2\n",
      "Processing batches:   6%|▋         | 2/32 [00:01<00:21,  1.43it/s]2025-01-10 09:48:37,164 [INFO] Batch 3: Hidden states shape: torch.Size([32, 62, 768])\n",
      "2025-01-10 09:48:37,166 [INFO] Encoding hidden states for batch 3\n",
      "Processing batches:   9%|▉         | 3/32 [00:01<00:15,  1.81it/s]2025-01-10 09:48:37,810 [INFO] Batch 4: Hidden states shape: torch.Size([32, 104, 768])\n",
      "2025-01-10 09:48:37,811 [INFO] Encoding hidden states for batch 4\n",
      "Processing batches:  12%|█▎        | 4/32 [00:02<00:16,  1.66it/s]2025-01-10 09:48:38,470 [INFO] Batch 5: Hidden states shape: torch.Size([32, 88, 768])\n",
      "2025-01-10 09:48:38,472 [INFO] Encoding hidden states for batch 5\n",
      "Processing batches:  16%|█▌        | 5/32 [00:03<00:16,  1.62it/s]2025-01-10 09:48:38,982 [INFO] Batch 6: Hidden states shape: torch.Size([32, 77, 768])\n",
      "2025-01-10 09:48:38,984 [INFO] Encoding hidden states for batch 6\n",
      "Processing batches:  19%|█▉        | 6/32 [00:03<00:15,  1.72it/s]2025-01-10 09:48:39,635 [INFO] Batch 7: Hidden states shape: torch.Size([32, 98, 768])\n",
      "2025-01-10 09:48:39,636 [INFO] Encoding hidden states for batch 7\n",
      "Processing batches:  22%|██▏       | 7/32 [00:04<00:15,  1.64it/s]2025-01-10 09:48:40,497 [INFO] Batch 8: Hidden states shape: torch.Size([32, 121, 768])\n",
      "2025-01-10 09:48:40,499 [INFO] Encoding hidden states for batch 8\n",
      "Processing batches:  25%|██▌       | 8/32 [00:05<00:16,  1.44it/s]2025-01-10 09:48:41,394 [INFO] Batch 9: Hidden states shape: torch.Size([32, 118, 768])\n",
      "2025-01-10 09:48:41,395 [INFO] Encoding hidden states for batch 9\n",
      "Processing batches:  28%|██▊       | 9/32 [00:06<00:17,  1.32it/s]2025-01-10 09:48:41,983 [INFO] Batch 10: Hidden states shape: torch.Size([32, 76, 768])\n",
      "2025-01-10 09:48:41,984 [INFO] Encoding hidden states for batch 10\n",
      "Processing batches:  31%|███▏      | 10/32 [00:06<00:15,  1.44it/s]2025-01-10 09:48:42,812 [INFO] Batch 11: Hidden states shape: torch.Size([32, 125, 768])\n",
      "2025-01-10 09:48:42,814 [INFO] Encoding hidden states for batch 11\n",
      "Processing batches:  34%|███▍      | 11/32 [00:07<00:15,  1.34it/s]2025-01-10 09:48:43,580 [INFO] Batch 12: Hidden states shape: torch.Size([32, 94, 768])\n",
      "2025-01-10 09:48:43,581 [INFO] Encoding hidden states for batch 12\n",
      "Processing batches:  38%|███▊      | 12/32 [00:08<00:14,  1.34it/s]2025-01-10 09:48:44,377 [INFO] Batch 13: Hidden states shape: torch.Size([32, 104, 768])\n",
      "2025-01-10 09:48:44,379 [INFO] Encoding hidden states for batch 13\n",
      "Processing batches:  41%|████      | 13/32 [00:09<00:14,  1.30it/s]2025-01-10 09:48:45,961 [INFO] Batch 14: Hidden states shape: torch.Size([32, 193, 768])\n",
      "2025-01-10 09:48:45,963 [INFO] Encoding hidden states for batch 14\n",
      "Processing batches:  44%|████▍     | 14/32 [00:10<00:18,  1.03s/it]2025-01-10 09:48:47,214 [INFO] Batch 15: Hidden states shape: torch.Size([32, 121, 768])\n",
      "2025-01-10 09:48:47,216 [INFO] Encoding hidden states for batch 15\n",
      "Processing batches:  47%|████▋     | 15/32 [00:11<00:18,  1.08s/it]2025-01-10 09:48:48,132 [INFO] Batch 16: Hidden states shape: torch.Size([32, 118, 768])\n",
      "2025-01-10 09:48:48,133 [INFO] Encoding hidden states for batch 16\n",
      "Processing batches:  50%|█████     | 16/32 [00:12<00:16,  1.03s/it]2025-01-10 09:48:49,049 [INFO] Batch 17: Hidden states shape: torch.Size([32, 110, 768])\n",
      "2025-01-10 09:48:49,051 [INFO] Encoding hidden states for batch 17\n",
      "Processing batches:  53%|█████▎    | 17/32 [00:13<00:14,  1.00it/s]2025-01-10 09:48:49,844 [INFO] Batch 18: Hidden states shape: torch.Size([32, 102, 768])\n",
      "2025-01-10 09:48:49,846 [INFO] Encoding hidden states for batch 18\n",
      "Processing batches:  56%|█████▋    | 18/32 [00:14<00:13,  1.07it/s]2025-01-10 09:48:50,326 [INFO] Batch 19: Hidden states shape: torch.Size([32, 53, 768])\n",
      "2025-01-10 09:48:50,328 [INFO] Encoding hidden states for batch 19\n",
      "Processing batches:  59%|█████▉    | 19/32 [00:15<00:10,  1.27it/s]2025-01-10 09:48:50,774 [INFO] Batch 20: Hidden states shape: torch.Size([32, 79, 768])\n",
      "2025-01-10 09:48:50,791 [INFO] Encoding hidden states for batch 20\n",
      "Processing batches:  62%|██████▎   | 20/32 [00:15<00:08,  1.44it/s]2025-01-10 09:48:51,652 [INFO] Batch 21: Hidden states shape: torch.Size([32, 122, 768])\n",
      "2025-01-10 09:48:51,654 [INFO] Encoding hidden states for batch 21\n",
      "Processing batches:  66%|██████▌   | 21/32 [00:16<00:08,  1.33it/s]2025-01-10 09:48:52,603 [INFO] Batch 22: Hidden states shape: torch.Size([32, 114, 768])\n",
      "2025-01-10 09:48:52,606 [INFO] Encoding hidden states for batch 22\n",
      "Processing batches:  69%|██████▉   | 22/32 [00:17<00:08,  1.23it/s]2025-01-10 09:48:53,119 [INFO] Batch 23: Hidden states shape: torch.Size([32, 56, 768])\n",
      "2025-01-10 09:48:53,121 [INFO] Encoding hidden states for batch 23\n",
      "Processing batches:  72%|███████▏  | 23/32 [00:17<00:06,  1.41it/s]2025-01-10 09:48:53,675 [INFO] Batch 24: Hidden states shape: torch.Size([32, 93, 768])\n",
      "2025-01-10 09:48:53,677 [INFO] Encoding hidden states for batch 24\n",
      "Processing batches:  75%|███████▌  | 24/32 [00:18<00:05,  1.49it/s]2025-01-10 09:48:54,441 [INFO] Batch 25: Hidden states shape: torch.Size([32, 99, 768])\n",
      "2025-01-10 09:48:54,443 [INFO] Encoding hidden states for batch 25\n",
      "Processing batches:  78%|███████▊  | 25/32 [00:19<00:04,  1.42it/s]2025-01-10 09:48:55,136 [INFO] Batch 26: Hidden states shape: torch.Size([32, 88, 768])\n",
      "2025-01-10 09:48:55,138 [INFO] Encoding hidden states for batch 26\n",
      "Processing batches:  81%|████████▏ | 26/32 [00:19<00:04,  1.44it/s]2025-01-10 09:48:55,910 [INFO] Batch 27: Hidden states shape: torch.Size([32, 106, 768])\n",
      "2025-01-10 09:48:55,912 [INFO] Encoding hidden states for batch 27\n",
      "Processing batches:  84%|████████▍ | 27/32 [00:20<00:03,  1.38it/s]2025-01-10 09:48:56,798 [INFO] Batch 28: Hidden states shape: torch.Size([32, 109, 768])\n",
      "2025-01-10 09:48:56,800 [INFO] Encoding hidden states for batch 28\n",
      "Processing batches:  88%|████████▊ | 28/32 [00:21<00:03,  1.29it/s]2025-01-10 09:48:57,210 [INFO] Batch 29: Hidden states shape: torch.Size([32, 41, 768])\n",
      "2025-01-10 09:48:57,212 [INFO] Encoding hidden states for batch 29\n",
      "Processing batches:  91%|█████████ | 29/32 [00:21<00:01,  1.55it/s]2025-01-10 09:48:57,948 [INFO] Batch 30: Hidden states shape: torch.Size([32, 125, 768])\n",
      "2025-01-10 09:48:57,949 [INFO] Encoding hidden states for batch 30\n",
      "Processing batches:  94%|█████████▍| 30/32 [00:22<00:01,  1.44it/s]2025-01-10 09:48:58,702 [INFO] Batch 31: Hidden states shape: torch.Size([32, 93, 768])\n",
      "2025-01-10 09:48:58,704 [INFO] Encoding hidden states for batch 31\n",
      "Processing batches:  97%|█████████▋| 31/32 [00:23<00:00,  1.42it/s]2025-01-10 09:48:58,992 [INFO] Batch 32: Hidden states shape: torch.Size([8, 38, 768])\n",
      "2025-01-10 09:48:58,994 [INFO] Encoding hidden states for batch 32\n",
      "Processing batches: 100%|██████████| 32/32 [00:23<00:00,  1.35it/s]\n",
      "2025-01-10 09:48:59,003 [INFO] Total batches processed: 32\n",
      "2025-01-10 09:48:59,006 [INFO] 最大长度:193\n",
      "2025-01-10 09:49:00,360 [INFO] Concatenated batch latents shape: torch.Size([1000, 193, 24576])\n",
      "2025-01-10 09:49:00,361 [INFO] Computing non-zero element counts\n",
      "2025-01-10 09:49:03,715 [INFO] Computing sum of non-zero elements\n",
      "2025-01-10 09:49:05,382 [INFO] Computing mean of non-zero elements\n",
      "2025-01-10 09:49:05,383 [INFO] Selecting top-k indices based on nz_mean\n",
      "2025-01-10 09:49:05,384 [INFO] Top 100 nz_mean values selected.\n",
      "2025-01-10 09:49:05,385 [INFO] Selecting top-k indices based on act_cnt\n",
      "2025-01-10 09:49:05,386 [INFO] Top 100 act_cnt values selected.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for steer_key in [\"pos\",\"neu\",\"neg\"]:\n",
    "    if steer_key == \"pos\":\n",
    "        selected_set = pos_train_set\n",
    "    elif steer_key == \"neg\":\n",
    "        selected_set = neg_train_set\n",
    "    elif steer_key==\"neu\":\n",
    "        selected_set = neu_train_set\n",
    "\n",
    "    texts = selected_set[\"text\"][:args.data_size]\n",
    "    a=get_activation_by_steer(texts)\n",
    "    steer_info[steer_key]=a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'pos'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[122], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msteer_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpos\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'pos'"
     ]
    }
   ],
   "source": [
    "steer_info[\"pos\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'pos'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[121], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msteer_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpos\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnz_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mKeyError\u001b[0m: 'pos'"
     ]
    }
   ],
   "source": [
    "steer_info[\"pos\"][\"nz_mean\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "steer_info[\"dif_neg_pos\"]={\"steer\":\"dif_neg_pos\",\"nz_cnt\":steer_info[\"pos\"][\"nz_cnt\"]-steer_info[\"neg\"][\"nz_cnt\"],\"nz_mean\":steer_info[\"pos\"][\"nz_mean\"]-steer_info[\"neg\"][\"nz_mean\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "steer_info[\"dif_neg_pos_relu\"]={\"nz_cnt\":torch.relu(steer_info[\"pos\"][\"nz_cnt\"]-steer_info[\"neg\"][\"nz_cnt\"]),\"nz_mean\":torch.relu(steer_info[\"pos\"][\"nz_mean\"]-steer_info[\"neg\"][\"nz_mean\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'steer': 'dif_neg_pos',\n",
       "  'nz_cnt': tensor([1, 0, 0,  ..., 0, 7, 2]),\n",
       "  'nz_mean': tensor([6.4307, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 3.2251],\n",
       "         grad_fn=<ReluBackward0>)},\n",
       " {'steer': 'dif_neg_pos',\n",
       "  'nz_cnt': tensor([  1,  -2,   0,  ..., -58,   7,   2]),\n",
       "  'nz_mean': tensor([ 6.4307, -0.6795,  0.0000,  ..., -0.2715, -0.0391,  3.2251],\n",
       "         grad_fn=<SubBackward0>)})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steer_info[\"dif_neg_pos_relu\"],steer_info[\"dif_neg_pos\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=\"cot\"\n",
    "b=\"direct\"\n",
    "steer_info[f\"dif_{a}-{b}_relu\"]={\"nz_cnt\":torch.relu(steer_info[a][\"nz_cnt\"]-steer_info[b][\"nz_cnt\"]),\"nz_mean\":torch.relu(steer_info[a][\"nz_mean\"]-steer_info[b][\"nz_mean\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k=100\n",
    "steering_vectors,steer_indices=torch.topk(steer_info[f\"dif_{a}-{b}_relu\"][\"nz_cnt\"],top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([159486, 141545, 134493, 130902, 125546, 123195, 122745, 120984, 119869,\n",
       "         118881, 118222, 118150, 117965, 117340, 116948, 116616, 116615, 114701,\n",
       "         113613, 113599, 113494, 113279, 112218, 109645, 108840, 107080, 106989,\n",
       "         106733, 106677, 106527, 106160, 106044, 105674, 105443, 104825, 104224,\n",
       "         103736, 103527, 103445, 102129, 102107, 101660, 101039, 101014, 100969,\n",
       "         100056,  99783,  99311,  99142,  98228,  98090,  97515,  97387,  96717,\n",
       "          96412,  96041,  95767,  95486,  95336,  95266,  95083,  94963,  94879,\n",
       "          94797,  94783,  94767,  94746,  94539,  94042,  93070,  92401,  91442,\n",
       "          90964,  90813,  90755,  90454,  90389,  90118,  89845,  89711,  89692,\n",
       "          89137,  88551,  88369,  88346,  88198,  88191,  88042,  87530,  86853,\n",
       "          86783,  86318,  86191,  85932,  85874,  85606,  85542,  85445,  85293,\n",
       "          85271]),\n",
       " tensor([14243, 19222, 18709,  7425, 17126,  8487, 10045, 14332, 14817, 22228,\n",
       "          8832, 10010, 15427, 22037,  8185, 20647, 23861, 14225, 23176,  4323,\n",
       "          6916,  6861, 11001, 18961,  9052,  7417, 18934,  7318,  8202, 12468,\n",
       "         18146, 20597, 14954,  5518,  7642, 20652, 12128, 13087, 14328, 17428,\n",
       "          3034, 15702,  7833, 12412, 16758, 18512, 24378,  6827,  3558,  7321,\n",
       "          6717,  5766, 14588, 16678, 13989,  8586,  4935,  7590, 13252,  6876,\n",
       "          6669, 21843, 17073, 20227,  8547, 24236,  9635, 16122, 23000, 24151,\n",
       "         20611,  2818,  8418, 23441, 13151, 12615, 13749,  4422, 14658,  5807,\n",
       "         14022,  1367,  4596, 23412, 23422, 10033, 16960, 17635,  6702,  9439,\n",
       "         20957, 20712, 21324, 19509, 24235, 22305, 13275,  3671, 16156, 23645]))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steering_vectors,steer_indices,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.9378,  0.6677,  3.3172,  3.7422,  1.9544,  9.9255,  4.9352,  2.5919,\n",
       "         2.7637,  4.3057,  4.6804,  1.9646,  1.4629,  3.8050,  1.4482,  4.0289,\n",
       "         4.0796,  1.7954,  0.2430,  0.4193,  1.5127,  1.7011,  3.2796,  0.0000,\n",
       "         0.9136,  3.4771,  0.8742,  1.9859,  0.0000,  0.7164,  5.8357,  0.4058,\n",
       "         2.7185,  0.6172,  2.9650,  0.9805,  3.4497,  0.5389,  1.3578,  1.4940,\n",
       "         1.8432,  1.4488,  1.2316,  4.4058,  3.7352,  0.9454,  2.3580,  1.8529,\n",
       "         1.7326,  1.5439,  1.7971,  1.7733,  3.1612,  1.3640,  0.4026,  0.1367,\n",
       "         1.2794,  1.6125,  0.2939,  2.7150,  2.0939,  3.8241,  0.8661,  0.3859,\n",
       "         0.0000,  4.1829,  0.4095,  0.9616,  3.0770,  4.3298, 13.4603,  1.4696,\n",
       "         0.0000,  0.8026,  0.0000,  0.0000,  4.0572,  0.0000,  0.5797,  0.0000,\n",
       "         0.9628,  3.5509,  1.0189,  0.4013,  0.3832,  0.3809,  0.0000,  0.0000,\n",
       "        10.4950,  1.2725,  2.1783,  1.0589,  0.0000,  1.5716,  3.4318,  2.2768,\n",
       "         0.0000,  0.1837,  0.0000, 14.0834], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steer_info[f\"dif_{a}-{b}_relu\"][\"nz_mean\"][steer_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.3788e-02,  8.1224e-02,  2.3156e-02,  1.8783e-02,  1.1774e-01,\n",
       "         3.7877e-02,  6.3553e-02,  5.3482e-02,  5.4047e-02,  1.2716e-02,\n",
       "         3.0568e-02,  6.7365e-02,  3.1710e-03, -8.7272e-03, -3.8923e-01,\n",
       "        -2.1414e-02, -1.4824e-01, -1.7109e-02,  2.1162e-02,  2.5611e-02,\n",
       "         2.4913e-02, -6.5165e-02,  1.8257e-03, -1.5588e-01,  3.0566e-02,\n",
       "        -3.8024e-01,  1.5130e-03,  2.4326e-03,  1.4322e-02, -5.0976e-02,\n",
       "         2.6311e-02,  2.0264e-01, -4.5139e-02, -3.4792e-01,  3.1174e-02,\n",
       "         9.3668e-02,  2.4175e-01,  2.1948e-01, -1.4558e-01, -1.8547e-02,\n",
       "         3.1676e-01,  1.7079e+00,  5.4160e-01,  7.8669e-02,  2.5975e-01,\n",
       "         1.2162e-02,  2.9813e-01, -1.2766e-02,  3.7756e-02,  3.6043e-01,\n",
       "         9.2443e-02,  3.0181e-01,  4.0478e-02,  9.6413e-02,  2.1965e-01,\n",
       "         1.1340e+00,  1.2302e-01,  2.1960e-01, -1.6293e-02, -3.9671e-02,\n",
       "         2.1330e-02,  5.8879e-01,  4.3106e-01,  1.7933e-01,  6.7136e-01,\n",
       "         3.6226e-01,  4.2522e-03, -3.4991e-02, -1.1640e-02,  7.9703e-02,\n",
       "         6.5875e-02, -2.8588e-02,  2.2631e-01,  1.3026e+00,  3.9014e-01,\n",
       "        -9.5093e-03, -1.4782e-01,  7.4718e-01,  1.4818e-01,  2.7658e-01,\n",
       "         1.5260e-01,  2.3184e-01,  2.9571e-02,  1.4099e-01,  2.4757e-01,\n",
       "        -4.4813e-02,  9.1639e-02,  1.0190e-01,  8.0088e-02,  3.2000e-01,\n",
       "         1.3040e-01,  1.3345e-01,  8.4079e-02,  2.8743e-02, -2.6734e-02,\n",
       "         5.0371e-02,  9.0073e-02,  1.7991e-01,  4.1546e-01,  1.4629e-01],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steer_info[\"dif_neg_pos\"][\"nz_mean\"][steer_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14243"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steer_indices[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 14:56:00,391 [INFO] Computing steering vectors using method: val_mul\n",
      "2025-01-10 14:56:00,402 [INFO] Steering vectors computed with shape: torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def compute_steering_vectors(sae: SAE, indices: Tensor, nz_mean_val: Tensor, method: str = \"val_mul\") -> Tensor:\n",
    "    logging.info(f\"Computing steering vectors using method: {method}\")\n",
    "    if method == \"mean\":\n",
    "        steering_vectors = torch.mean(sae.W_dec[indices], dim=0)\n",
    "    elif method == \"val_mul\":\n",
    "        steering_vectors = torch.zeros(sae.W_dec.shape[1], device=sae.W_dec.device)\n",
    "        for idx in indices:\n",
    "            steering_vectors += nz_mean_val[idx].item() * sae.W_dec[idx]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    logging.info(f\"Steering vectors computed with shape: {steering_vectors.shape}\")\n",
    "    return steering_vectors\n",
    "steering_vectors=compute_steering_vectors(sae,indices=steer_indices,nz_mean_val=steer_info[f\"dif_{a}-{b}_relu\"][\"nz_mean\"],method=\"val_mul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50256, 50256]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.to_tokens(\"<|endoftext|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 15:37:45,388 [INFO] Example prompt: ['Q: The mayor commissioned two artists to paint 50 murals around the city. Once the work was completed, Celina was paid $1,000 more than 4 times the amount Diego got. If the mayor paid the two a total of $50,000, how much did Diego get?\\nA: ', \"Q: Adrianna has 10 pieces of gum to share with her friends. There wasn't enough gum for all her friends, so she went to the store to get 3 more pieces of gum. She gave out gum to 11 friends. How many pieces of gum does Adrianna have now?\\nA: \", 'Q: Paul needed to buy some new clothes for work.  He had a 10% off coupon that he could use on his entire purchase after any other discounts.  Paul bought 4 dress shirts at $15.00 apiece, 2 pairs of pants that each cost $40.00.  He found a suit for $150.00 and 2 sweaters for $30.00 each.  When he got to the register, the clerk told him that the store was offering 20% off of everything in the store.  After the discounts and the coupon, how much did Paul spend on his new clothes?\\nA: ']\n",
      "2025-01-10 15:37:45,390 [INFO] Generating texts **without** steering... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Q: The mayor commissioned two artists to paint 50 murals around the city. Once the work was completed, Celina was paid $1,000 more than 4 times the amount Diego got. If the mayor paid the two a total of $50,000, how much did Diego get?\\nA: ', \"Q: Adrianna has 10 pieces of gum to share with her friends. There wasn't enough gum for all her friends, so she went to the store to get 3 more pieces of gum. She gave out gum to 11 friends. How many pieces of gum does Adrianna have now?\\nA: \", 'Q: Paul needed to buy some new clothes for work.  He had a 10% off coupon that he could use on his entire purchase after any other discounts.  Paul bought 4 dress shirts at $15.00 apiece, 2 pairs of pants that each cost $40.00.  He found a suit for $150.00 and 2 sweaters for $30.00 each.  When he got to the register, the clerk told him that the store was offering 20% off of everything in the store.  After the discounts and the coupon, how much did Paul spend on his new clothes?\\nA: '] ['9800', '2', '252']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dafd3f4fcd44f00a33f0edbfd7dd82d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 15:37:46,041 [INFO] Generated Text: 1: Q: The mayor commissioned two artists to paint 50 murals around the city. Once the work was completed, Celina was paid $1,000 more than 4 times the amount Diego got. If the mayor paid the two a total of $50,000, how much did Diego get?\n",
      "A: <|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Four years later in 2012 when he told Celina\n",
      "2025-01-10 15:37:46,041 [INFO] Generated Text: 2: Q: Adrianna has 10 pieces of gum to share with her friends. There wasn't enough gum for all her friends, so she went to the store to get 3 more pieces of gum. She gave out gum to 11 friends. How many pieces of gum does Adrianna have now?\n",
      "A: <|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Close up photo by Tom Scutt\n",
      "Adri\n",
      "2025-01-10 15:37:46,042 [INFO] Generated Text: 3: Q: Paul needed to buy some new clothes for work.  He had a 10% off coupon that he could use on his entire purchase after any other discounts.  Paul bought 4 dress shirts at $15.00 apiece, 2 pairs of pants that each cost $40.00.  He found a suit for $150.00 and 2 sweaters for $30.00 each.  When he got to the register, the clerk told him that the store was offering 20% off of everything in the store.  After the discounts and the coupon, how much did Paul spend on his new clothes?\n",
      "A:  Paul purchased them both in one day!  \n",
      "2025-01-10 15:37:46,042 [INFO] Generated Text: 4: Q: The mayor commissioned two artists to paint 50 murals around the city. Once the work was completed, Celina was paid $1,000 more than 4 times the amount Diego got. If the mayor paid the two a total of $50,000, how much did Diego get?\n",
      "A: <|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>FACT: The Mayor's Association made contact with\n",
      "2025-01-10 15:37:46,042 [INFO] Generated Text: 5: Q: Adrianna has 10 pieces of gum to share with her friends. There wasn't enough gum for all her friends, so she went to the store to get 3 more pieces of gum. She gave out gum to 11 friends. How many pieces of gum does Adrianna have now?\n",
      "A: <|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>In case you missed it in any way, the\n",
      "2025-01-10 15:37:46,043 [INFO] Generated Text: 6: Q: Paul needed to buy some new clothes for work.  He had a 10% off coupon that he could use on his entire purchase after any other discounts.  Paul bought 4 dress shirts at $15.00 apiece, 2 pairs of pants that each cost $40.00.  He found a suit for $150.00 and 2 sweaters for $30.00 each.  When he got to the register, the clerk told him that the store was offering 20% off of everything in the store.  After the discounts and the coupon, how much did Paul spend on his new clothes?\n",
      "A:  I think I had about 30% more savings\n",
      "2025-01-10 15:37:46,043 [INFO] Generated Text: 7: Q: The mayor commissioned two artists to paint 50 murals around the city. Once the work was completed, Celina was paid $1,000 more than 4 times the amount Diego got. If the mayor paid the two a total of $50,000, how much did Diego get?\n",
      "A: <|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Trump said he has been able to spend up to\n",
      "2025-01-10 15:37:46,043 [INFO] Generated Text: 8: Q: Adrianna has 10 pieces of gum to share with her friends. There wasn't enough gum for all her friends, so she went to the store to get 3 more pieces of gum. She gave out gum to 11 friends. How many pieces of gum does Adrianna have now?\n",
      "A: <|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Formerly, it was 5 white crystals on one\n",
      "2025-01-10 15:37:46,044 [INFO] Generated Text: 9: Q: Paul needed to buy some new clothes for work.  He had a 10% off coupon that he could use on his entire purchase after any other discounts.  Paul bought 4 dress shirts at $15.00 apiece, 2 pairs of pants that each cost $40.00.  He found a suit for $150.00 and 2 sweaters for $30.00 each.  When he got to the register, the clerk told him that the store was offering 20% off of everything in the store.  After the discounts and the coupon, how much did Paul spend on his new clothes?\n",
      "A:  It took me about 3 weeks (or 4\n",
      "2025-01-10 15:37:46,045 [INFO] Generating texts with steering... Target\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ad7e8761abc49198f83fab437ead0fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 15:37:46,630 [INFO] Generated Text: 1: Q: The mayor commissioned two artists to paint 50 murals around the city. Once the work was completed, Celina was paid $1,000 more than 4 times the amount Diego got. If the mayor paid the two a total of $50,000, how much did Diego get?\n",
      "A: <|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>WASHINGTON— A few weeks ago several sources reported that\n",
      "2025-01-10 15:37:46,631 [INFO] Generated Text: 2: Q: Adrianna has 10 pieces of gum to share with her friends. There wasn't enough gum for all her friends, so she went to the store to get 3 more pieces of gum. She gave out gum to 11 friends. How many pieces of gum does Adrianna have now?\n",
      "A: <|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|> ›› :›- – due to\n",
      "2025-01-10 15:37:46,631 [INFO] Generated Text: 3: Q: Paul needed to buy some new clothes for work.  He had a 10% off coupon that he could use on his entire purchase after any other discounts.  Paul bought 4 dress shirts at $15.00 apiece, 2 pairs of pants that each cost $40.00.  He found a suit for $150.00 and 2 sweaters for $30.00 each.  When he got to the register, the clerk told him that the store was offering 20% off of everything in the store.  After the discounts and the coupon, how much did Paul spend on his new clothes?\n",
      "A:  ›‹­\n",
      "\n",
      "• Editor  ​\n",
      "2025-01-10 15:37:46,631 [INFO] Generated Text: 4: Q: The mayor commissioned two artists to paint 50 murals around the city. Once the work was completed, Celina was paid $1,000 more than 4 times the amount Diego got. If the mayor paid the two a total of $50,000, how much did Diego get?\n",
      "A: <|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|> › A less precise estimation is due to differing definitions\n",
      "2025-01-10 15:37:46,632 [INFO] Generated Text: 5: Q: Adrianna has 10 pieces of gum to share with her friends. There wasn't enough gum for all her friends, so she went to the store to get 3 more pieces of gum. She gave out gum to 11 friends. How many pieces of gum does Adrianna have now?\n",
      "A: <|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>WASHINGTON - A bill offering the common defense of such\n",
      "2025-01-10 15:37:46,633 [INFO] Generated Text: 6: Q: Paul needed to buy some new clothes for work.  He had a 10% off coupon that he could use on his entire purchase after any other discounts.  Paul bought 4 dress shirts at $15.00 apiece, 2 pairs of pants that each cost $40.00.  He found a suit for $150.00 and 2 sweaters for $30.00 each.  When he got to the register, the clerk told him that the store was offering 20% off of everything in the store.  After the discounts and the coupon, how much did Paul spend on his new clothes?\n",
      "A:  ›\n",
      "\n",
      "\"Although 'It is routinely interpreted\n",
      "2025-01-10 15:37:46,633 [INFO] Generated Text: 7: Q: The mayor commissioned two artists to paint 50 murals around the city. Once the work was completed, Celina was paid $1,000 more than 4 times the amount Diego got. If the mayor paid the two a total of $50,000, how much did Diego get?\n",
      "A: <|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|> › About 1-1 :: Most of my\n",
      "2025-01-10 15:37:46,634 [INFO] Generated Text: 8: Q: Adrianna has 10 pieces of gum to share with her friends. There wasn't enough gum for all her friends, so she went to the store to get 3 more pieces of gum. She gave out gum to 11 friends. How many pieces of gum does Adrianna have now?\n",
      "A: <|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|> › 1 : That would be a somewhat accurate estimate\n",
      "2025-01-10 15:37:46,634 [INFO] Generated Text: 9: Q: Paul needed to buy some new clothes for work.  He had a 10% off coupon that he could use on his entire purchase after any other discounts.  Paul bought 4 dress shirts at $15.00 apiece, 2 pairs of pants that each cost $40.00.  He found a suit for $150.00 and 2 sweaters for $30.00 each.  When he got to the register, the clerk told him that the store was offering 20% off of everything in the store.  After the discounts and the coupon, how much did Paul spend on his new clothes?\n",
      "A:  › | DSL Russian & Telecom Reporter (Toronto \n"
     ]
    }
   ],
   "source": [
    "# # Define steering hook\n",
    "# steering_on = True  # This will be toggled in run_generate\n",
    "# alpha = args.alpha\n",
    "# method = args.method  # Store method for clarity\n",
    "\n",
    "def steering_hook(resid_pre, hook):\n",
    "    if resid_pre.shape[1] == 1:\n",
    "        return\n",
    "    if steering_on:\n",
    "        resid_pre += alpha * steering_vectors\n",
    "\n",
    "def hooked_generate(prompt_batch, fwd_hooks=[], seed=None, **kwargs):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    with model.hooks(fwd_hooks=fwd_hooks):\n",
    "        tokenized = model.to_tokens(prompt_batch)\n",
    "        result = model.generate(\n",
    "            stop_at_eos=True, # 设置为 True，遇到 <|endoftext|> 时停止model.tokenizer.eos_token= <|endoftext|>\n",
    "            input=tokenized,\n",
    "            # max_new_tokens=50, # 最大生成 token 数\n",
    "            do_sample=True,# 是否使用采样\n",
    "            **kwargs,\n",
    "        )\n",
    "    return result\n",
    "\n",
    "def run_generate(example_prompts: str, sampling_kwargs: dict) -> list:\n",
    "    model.reset_hooks()\n",
    "    editing_hooks = [(f\"blocks.{args.layer}.hook_resid_post\", steering_hook)]\n",
    "    res = hooked_generate(\n",
    "        example_prompts * 3,\n",
    "        fwd_hooks=editing_hooks if steering_on else [],\n",
    "        seed=args.seed,\n",
    "        **sampling_kwargs\n",
    "    )\n",
    "\n",
    "\n",
    "    res_str = model.to_string(res[:, 1:])\n",
    "    # generated_texts = res_str\n",
    "    for idx, text in enumerate(res_str):\n",
    "        logging.info(f\"Generated Text: {idx+1}: {text}\")\n",
    "        \n",
    "    return res_str\n",
    "\n",
    "# Define sampling parameters\n",
    "sampling_kwargs = dict(temperature=1.0, top_p=0.7, freq_penalty=1)\n",
    "\n",
    "# Example prompt from the selected set\n",
    "example_prompt = all_dataset[\"val\"][\"prompt\"][:3]\n",
    "print( all_dataset[\"val\"][\"prompt\"][:3],all_dataset[\"val\"][\"A\"][:3])\n",
    "logging.info(f\"Example prompt: {example_prompt}\")\n",
    "\n",
    "# Generate without steering\n",
    "steering_on = False\n",
    "alpha = 0\n",
    "logging.info(\"Generating texts **without** steering... \")\n",
    "generated_texts_no_steer = run_generate(example_prompt, sampling_kwargs)\n",
    "\n",
    "# Generate with steering\n",
    "steering_on = True\n",
    "alpha = 20\n",
    "logging.info(\"Generating texts with steering... Target\")\n",
    "generated_texts_with_steer = run_generate(example_prompt, sampling_kwargs)\n",
    "\n",
    "# Combine generated texts\n",
    "all_generated_texts = generated_texts_no_steer + generated_texts_with_steer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m example_prompt \u001b[38;5;241m=\u001b[39m \u001b[43mall_dataset\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m][:\u001b[38;5;241m10\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "example_prompt = all_dataset[\"val\"][\"prompt\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "save_results(\n",
    "    output_dir=output_dir_base,\n",
    "    nz_mean=nz_mean,\n",
    "    act_cnt=act_cnt,\n",
    "    generated_texts=all_generated_texts,\n",
    "    hyperparams=hyperparams\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
