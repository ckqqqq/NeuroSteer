{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "from typing import Tuple\n",
    "from tqdm import tqdm  # For progress bars\n",
    "from log import setup_logging\n",
    "import logging\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "from data_preprocess import load_and_prepare_triple_dataset,load_and_prepare_COT_dataset\n",
    "from utils import load_environment\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 16:46:48,073 [INFO] Logging initialized. Logs will be saved to ./results/LLM_gpt2-small_layer_6_steer_sup-opp_alpha_50_cnt_100_mean_100_device_cuda/execution.log\n",
      "2025-01-20 16:46:48,076 [INFO] Hyperparameters:\n",
      "2025-01-20 16:46:48,077 [INFO]   layer: 6\n",
      "2025-01-20 16:46:48,078 [INFO]   LLM: gpt2-small\n",
      "2025-01-20 16:46:48,079 [INFO]   dataset_path: /home/ckqsudo/code2024/0dataset/baseline-acl/data/debate/StanceSentences\n",
      "2025-01-20 16:46:48,080 [INFO]   prompt_path: /home/ckqsudo/code2024/0dataset/baseline-acl/data/debate/ibm_debate\n",
      "2025-01-20 16:46:48,080 [INFO]   output_dir: ./results\n",
      "2025-01-20 16:46:48,082 [INFO]   env_path: /home/ckqsudo/code2024/CKQ_ACL2024/Control_Infer/SAE-simple/.env\n",
      "2025-01-20 16:46:48,083 [INFO]   task: debate\n",
      "2025-01-20 16:46:48,084 [INFO]   seed: 42\n",
      "2025-01-20 16:46:48,084 [INFO]   data_size: ALL\n",
      "2025-01-20 16:46:48,085 [INFO]   device: cuda\n",
      "2025-01-20 16:46:48,087 [INFO]   alpha: 50\n",
      "2025-01-20 16:46:48,088 [INFO]   steer: sup-opp\n",
      "2025-01-20 16:46:48,089 [INFO]   source: sup\n",
      "2025-01-20 16:46:48,090 [INFO]   target: opp\n",
      "2025-01-20 16:46:48,091 [INFO]   method: val_mul\n",
      "2025-01-20 16:46:48,092 [INFO]   topk_mean: 100\n",
      "2025-01-20 16:46:48,092 [INFO]   topk_cnt: 100\n",
      "2025-01-20 16:46:48,093 [INFO]   batch_size: 32\n",
      "2025-01-20 16:46:48,094 [INFO]   mean_type: dif_mean\n",
      "2025-01-20 16:46:48,096 [INFO]   steer_type: last\n",
      "2025-01-20 16:46:48,099 [INFO] HF_ENDPOINT: https://hf-mirror.com\n"
     ]
    }
   ],
   "source": [
    "args_dict = {\n",
    "    \"layer\": 6,  # Example layer number to analyze\n",
    "    \"LLM\": \"gpt2-small\",\n",
    "    \"dataset_path\": \"/home/ckqsudo/code2024/0dataset/baseline-acl/data/debate/StanceSentences\",\n",
    "    \"prompt_path\":\"/home/ckqsudo/code2024/0dataset/baseline-acl/data/debate/ibm_debate\",\n",
    "    \"output_dir\": \"./results\",\n",
    "    \"env_path\": \"/home/ckqsudo/code2024/CKQ_ACL2024/Control_Infer/SAE-simple/.env\",\n",
    "    \"task\":\"debate\",\n",
    "    \"seed\": 42,\n",
    "    \"data_size\": \"ALL\",\n",
    "    \"device\": \"cuda\",  # Options: \"cpu\", \"cuda\", \"mps\", \"auto\"\n",
    "    \"alpha\": 50, # 这个alpha后面慢慢调节\n",
    "    \"steer\": \"sup-opp\",  \n",
    "    \"source\": \"sup\",\n",
    "    \"target\": \"opp\",\n",
    "    \"method\": \"val_mul\",  # Options: \"mean\", \"val_mul\" 用val_mul会比较好\n",
    "    \"topk_mean\": 100, # 选取前topk 个均值激活，这个效果一般，会导致很多如what？why？这种被激活\n",
    "    \"topk_cnt\": 100, # 选取前topk个频率激活，目前默认这个，效果很好\n",
    "    \"batch_size\": 32, # 这个好像没用上\n",
    "    \"mean_type\": \"dif_mean\",\n",
    "    \"steer_type\": \"last\", # 这个好像没用上\n",
    "}\n",
    "args = argparse.Namespace(**args_dict)\n",
    "\n",
    "sampling_kwargs = dict(temperature=1.0, top_p=0.1, freq_penalty=1.0)\n",
    "sampling_kwargs['verbose']=False\n",
    "TASK =args.task\n",
    "STEER_TYPE=args.steer_type\n",
    "ALPHA=args.alpha\n",
    "MAX_NEW_TOKENS=100\n",
    "\n",
    "\n",
    "\n",
    "# Logging Setup\n",
    "import os\n",
    "from log import setup_logging\n",
    "import logging\n",
    "# Create output directory base path\n",
    "output_dir = os.path.join(\n",
    "    args.output_dir,\n",
    "    f\"LLM_{args.LLM}_layer_{args.layer}_steer_{args.steer}_alpha_{args.alpha}_cnt_{args.topk_cnt}_mean_{args.topk_mean}_device_{args.device}\"\n",
    ")\n",
    "\n",
    "# Setup logging\n",
    "setup_logging(output_dir)\n",
    "\n",
    "# Save hyperparameters\n",
    "hyperparams = args_dict\n",
    "\n",
    "# Log hyperparameters\n",
    "logging.info(\"Hyperparameters:\")\n",
    "for key, value in hyperparams.items():\n",
    "    logging.info(f\"  {key}: {value}\")\n",
    "\n",
    "# Load Environment Variables\n",
    " \n",
    "def load_environment(env_path: str):\n",
    "    load_dotenv(env_path)\n",
    "    hf_endpoint = os.getenv('HF_ENDPOINT', 'https://hf-mirror.com')\n",
    "    logging.info(f\"HF_ENDPOINT: {hf_endpoint}\")\n",
    "\n",
    "load_environment(args.env_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 16:14:49,642 [INFO] dataset path /home/ckqsudo/code2024/0dataset/baseline-acl/data/debate/StanceSentences\n",
      "2025-01-20 16:14:49,646 [INFO] debatedebatedebatedebatedebatedebatedebatedebatedebatedebate\n",
      "2025-01-20 16:14:49,647 [INFO] Loading dataset from /home/ckqsudo/code2024/0dataset/baseline-acl/data/debate/StanceSentences\n",
      "2025-01-20 16:14:49,808 [INFO] Filtering dataset for negative, positive, and neutral samples\n",
      "2025-01-20 16:14:49,813 [INFO] Selected 486 support and 486 oppose samples\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"dataset path \"+args.dataset_path)\n",
    "\n",
    "def load_and_prepare_debate_triple_dataset(dataset_path: str, seed: int, num_samples):\n",
    "    logging.info(f\"Loading dataset from {dataset_path}\")\n",
    "    dataset = load_dataset(dataset_path)\n",
    "    dataset[\"train\"] = dataset['train'].shuffle(seed=seed)\n",
    "\n",
    "    logging.info(\"Filtering dataset for negative, positive, and neutral samples\")\n",
    "    if num_samples == \"ALL\":\n",
    "        sup_train_set = dataset['train'].filter(lambda example: example['label'] == 'support')\n",
    "        opp_train_set = dataset['train'].filter(lambda example: example['label'] == 'oppose')\n",
    "    elif isinstance(num_samples, int):\n",
    "        sup_train_set = dataset['train'].filter(lambda example: example['label'] == 'support').select(range(num_samples))\n",
    "        opp_train_set = dataset['train'].filter(lambda example: example['label'] == 'oppose').select(range(num_samples))\n",
    "    else:\n",
    "        raise ValueError(\"num_samples must be int or ALL\")\n",
    "    logging.info(f\"Selected {len(sup_train_set)} support and {len(opp_train_set)} oppose samples\")\n",
    "    val_set = dataset['validation']\n",
    "    test_set = dataset[\"test\"]\n",
    "    return sup_train_set, opp_train_set, val_set, test_set\n",
    "\n",
    "if TASK==\"sentiment\":\n",
    "    neg_train_set, pos_train_set, neu_train_set,val_set,test_set = load_and_prepare_triple_dataset(\n",
    "        args.dataset_path, \"sst5\",args.seed, args.data_size\n",
    "    )\n",
    "elif \"cot\"==TASK:\n",
    "    logging.info(\"COT \"*10)\n",
    "    all_dataset=load_and_prepare_COT_dataset(\n",
    "        args.dataset_path, args.seed, args.data_size\n",
    "    )\n",
    "elif \"polite\"==TASK:\n",
    "    logging.info(\"polite\"*10)\n",
    "    neg_train_set, pos_train_set, neu_train_set,val_set,test_set=load_and_prepare_triple_dataset(args.dataset_path,\"polite\", args.seed, args.data_size)\n",
    "elif TASK==\"debate\":\n",
    "    logging.info(\"debate\"*10)\n",
    "    sup_train_set, opp_train_set,val_set,test_set=load_and_prepare_debate_triple_dataset(args.dataset_path, args.seed, args.data_size)\n",
    "else:\n",
    "    raise ValueError(\"No Supported\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 16:14:52,996 [INFO] Loading model: gpt2-small\n",
      "2025-01-20 16:16:04,077 [INFO] Loading SAE for layer 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ckqsudo/miniconda3/envs/SAE/lib/python3.11/site-packages/sae_lens/sae.py:145: UserWarning: \n",
      "This SAE has non-empty model_from_pretrained_kwargs. \n",
      "For optimal performance, load the model like so:\n",
      "model = HookedSAETransformer.from_pretrained_no_processing(..., **cfg.model_from_pretrained_kwargs)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "logging.info(f\"Loading model: {args.LLM}\")\n",
    "model = HookedTransformer.from_pretrained(args.LLM, device=args.device)\n",
    "\n",
    "logging.info(f\"Loading SAE for layer {args.layer}\")\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release=\"gpt2-small-res-jb\",\n",
    "    sae_id=f\"blocks.{args.layer}.hook_resid_pre\",\n",
    "    device=args.device\n",
    ")\n",
    "\n",
    "def analyze_latents(batch_latents: Tensor, top_k_mean: int = 100, top_k_cnt: int = 100) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "    SAE_LATENT_SIZE=sae.W_dec.shape[0]\n",
    "    \n",
    "    # 计算非0激活在对应位置的激活频率   \n",
    "    logging.info(\"Computing non-zero element counts\") \n",
    "    lat_freq = (batch_latents != 0).sum(dim=(0, 1))\n",
    "    # 计算非0激活在对应位置的激活值的和\n",
    "    logging.info(\"Computing sum of non-zero elements\")\n",
    "    lat_val_sum = batch_latents.sum(dim=(0, 1))\n",
    "\n",
    "    logging.info(\"Computing mean of non-zero elements\")\n",
    "    # \n",
    "    assert batch_latents.shape[-1]==SAE_LATENT_SIZE==lat_val_sum.shape[0], \"Latent dimension mismatch\"\n",
    "    return {\"latent_frequency\":lat_freq,\"latent_value_sum\":lat_val_sum}\n",
    "def compute_latents(sae: SAE, model: HookedTransformer, texts: list, hook_point: str, device: str, batch_size: int) -> list:\n",
    "    \"\"\"\n",
    "    计算 latents，支持批次处理。\n",
    "\n",
    "    Args:\n",
    "        sae (SAE): SAE 实例。\n",
    "        model (HookedTransformer): Transformer 模型实例。\n",
    "        texts (list): 文本列表。\n",
    "        hook_point (str): 钩子点名称。\n",
    "        device (str): 计算设备。\n",
    "        batch_size (int): 每个批次的大小。\n",
    "\n",
    "    Returns:\n",
    "        list: 包含每个批次 latents 的张量列表。\n",
    "    \"\"\"\n",
    "    SAE_LATENT_SIZE=sae.W_dec.shape[0]\n",
    "    logging.info(\"Running model with cache to obtain hidden states\")\n",
    "    # batch_latents = []\n",
    "    lat_freq,lat_val_sum=torch.zeros(SAE_LATENT_SIZE).to(\"cpu\"),torch.zeros(SAE_LATENT_SIZE).to(\"cpu\")# 避免OOM\n",
    "    # 使用 tqdm 显示进度条\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Processing batches\"):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            logging.info(f\"Batch {i // batch_size + 1}: batch_size {batch_size}\")\n",
    "            try:\n",
    "                sv_logits, cache = model.run_with_cache(batch_texts, prepend_bos=False, device=\"cuda\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing batch {i // batch_size + 1}: {e}\")\n",
    "                raise ValueError(str([len(i) for i in batch_texts]))\n",
    "            batch_hidden_states = cache[hook_point]\n",
    "            logging.info(f\"Batch {i // batch_size + 1}: Hidden states shape: {batch_hidden_states.shape}\")\n",
    "\n",
    "            # logging.info(f\"Encoding hidden states for batch {i // batch_size + 1}\")\n",
    "            # 假设 sae.encode 支持批量编码\n",
    "            batch_latents = sae.encode(batch_hidden_states)  # 形状: (batch_size, latent_dim)\n",
    "            batch_info=analyze_latents(batch_latents)\n",
    "            lat_freq=lat_freq+batch_info[\"latent_frequency\"].to(\"cpu\")\n",
    "            lat_val_sum=lat_val_sum+batch_info[\"latent_value_sum\"].to(\"cpu\")\n",
    "    lat_val_mean=torch.where(lat_freq != 0, lat_val_sum / lat_freq, torch.tensor(0.0, device=\"cpu\"))\n",
    "    logging.info(f\"Total non-zero element shape: {lat_freq.shape}\")\n",
    "    assert lat_freq.shape[0]==lat_freq.shape[0]==sae.W_dec.shape[0], \"sae latent dimension mismatch\"\n",
    "    return {\"latent_frequency\":lat_freq.to(device),\"latent_value_mean\":lat_val_mean.to(device)}\n",
    "\n",
    "def get_activation_by_steer(texts:list):\n",
    "    hook_point = sae.cfg.hook_name\n",
    "    # Compute latents with batch processing\n",
    "    lat_info=compute_latents(sae, model, texts, hook_point, args.device, args.batch_size)\n",
    "    return {\"latent_value_mean\":lat_info[\"latent_value_mean\"],\"latent_frequency\":lat_info[\"latent_frequency\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 16:16:58,605 [INFO] fromsuptoopp\n",
      "2025-01-20 16:16:58,607 [INFO] support\n",
      "2025-01-20 16:16:58,617 [INFO] Running model with cache to obtain hidden states\n",
      "Processing batches:   0%|          | 0/16 [00:00<?, ?it/s]2025-01-20 16:16:58,622 [INFO] Batch 1: batch_size 32\n",
      "2025-01-20 16:16:58,846 [INFO] Batch 1: Hidden states shape: torch.Size([32, 50, 768])\n",
      "2025-01-20 16:16:58,848 [INFO] Computing non-zero element counts\n",
      "2025-01-20 16:16:58,854 [INFO] Computing sum of non-zero elements\n",
      "2025-01-20 16:16:58,855 [INFO] Computing mean of non-zero elements\n",
      "Processing batches:   6%|▋         | 1/16 [00:00<00:03,  4.26it/s]2025-01-20 16:16:58,858 [INFO] Batch 2: batch_size 32\n",
      "2025-01-20 16:16:58,902 [INFO] Batch 2: Hidden states shape: torch.Size([32, 48, 768])\n",
      "2025-01-20 16:16:58,904 [INFO] Computing non-zero element counts\n",
      "2025-01-20 16:16:58,905 [INFO] Computing sum of non-zero elements\n",
      "2025-01-20 16:16:58,907 [INFO] Computing mean of non-zero elements\n",
      "2025-01-20 16:16:58,908 [INFO] Batch 3: batch_size 32\n",
      "2025-01-20 16:16:58,945 [INFO] Batch 3: Hidden states shape: torch.Size([32, 48, 768])\n",
      "2025-01-20 16:16:58,946 [INFO] Computing non-zero element counts\n",
      "2025-01-20 16:16:58,948 [INFO] Computing sum of non-zero elements\n",
      "2025-01-20 16:16:58,949 [INFO] Computing mean of non-zero elements\n",
      "2025-01-20 16:16:58,950 [INFO] Batch 4: batch_size 32\n",
      "2025-01-20 16:16:58,985 [INFO] Batch 4: Hidden states shape: torch.Size([32, 47, 768])\n",
      "2025-01-20 16:16:58,987 [INFO] Computing non-zero element counts\n",
      "2025-01-20 16:16:58,988 [INFO] Computing sum of non-zero elements\n",
      "2025-01-20 16:16:58,989 [INFO] Computing mean of non-zero elements\n",
      "Processing batches:  25%|██▌       | 4/16 [00:00<00:00, 12.41it/s]2025-01-20 16:16:58,992 [INFO] Batch 5: batch_size 32\n",
      "2025-01-20 16:16:59,027 [INFO] Batch 5: Hidden states shape: torch.Size([32, 46, 768])\n",
      "2025-01-20 16:16:59,028 [INFO] Computing non-zero element counts\n",
      "2025-01-20 16:16:59,030 [INFO] Computing sum of non-zero elements\n",
      "2025-01-20 16:16:59,031 [INFO] Computing mean of non-zero elements\n",
      "2025-01-20 16:16:59,033 [INFO] Batch 6: batch_size 32\n",
      "2025-01-20 16:16:59,068 [INFO] Batch 6: Hidden states shape: torch.Size([32, 46, 768])\n",
      "2025-01-20 16:16:59,070 [INFO] Computing non-zero element counts\n",
      "2025-01-20 16:16:59,071 [INFO] Computing sum of non-zero elements\n",
      "2025-01-20 16:16:59,072 [INFO] Computing mean of non-zero elements\n",
      "2025-01-20 16:16:59,074 [INFO] Batch 7: batch_size 32\n",
      "2025-01-20 16:16:59,108 [INFO] Batch 7: Hidden states shape: torch.Size([32, 49, 768])\n",
      "2025-01-20 16:16:59,110 [INFO] Computing non-zero element counts\n",
      "2025-01-20 16:16:59,111 [INFO] Computing sum of non-zero elements\n",
      "2025-01-20 16:16:59,112 [INFO] Computing mean of non-zero elements\n",
      "Processing batches:  44%|████▍     | 7/16 [00:00<00:00, 16.86it/s]2025-01-20 16:16:59,114 [INFO] Batch 8: batch_size 32\n",
      "2025-01-20 16:16:59,148 [INFO] Batch 8: Hidden states shape: torch.Size([32, 43, 768])\n",
      "2025-01-20 16:16:59,149 [INFO] Computing non-zero element counts\n",
      "2025-01-20 16:16:59,150 [INFO] Computing sum of non-zero elements\n",
      "2025-01-20 16:16:59,151 [INFO] Computing mean of non-zero elements\n",
      "2025-01-20 16:16:59,152 [INFO] Batch 9: batch_size 32\n",
      "2025-01-20 16:16:59,185 [INFO] Batch 9: Hidden states shape: torch.Size([32, 40, 768])\n",
      "2025-01-20 16:16:59,187 [INFO] Computing non-zero element counts\n",
      "2025-01-20 16:16:59,188 [INFO] Computing sum of non-zero elements\n",
      "2025-01-20 16:16:59,189 [INFO] Computing mean of non-zero elements\n",
      "2025-01-20 16:16:59,190 [INFO] Batch 10: batch_size 32\n",
      "2025-01-20 16:16:59,450 [INFO] Batch 10: Hidden states shape: torch.Size([32, 47, 768])\n",
      "2025-01-20 16:16:59,451 [INFO] Computing non-zero element counts\n",
      "2025-01-20 16:16:59,453 [INFO] Computing sum of non-zero elements\n",
      "2025-01-20 16:16:59,454 [INFO] Computing mean of non-zero elements\n",
      "Processing batches:  62%|██████▎   | 10/16 [00:00<00:00, 12.03it/s]2025-01-20 16:16:59,457 [INFO] Batch 11: batch_size 32\n",
      "2025-01-20 16:16:59,493 [INFO] Batch 11: Hidden states shape: torch.Size([32, 45, 768])\n",
      "2025-01-20 16:16:59,495 [INFO] Computing non-zero element counts\n",
      "2025-01-20 16:16:59,497 [INFO] Computing sum of non-zero elements\n",
      "2025-01-20 16:16:59,498 [INFO] Computing mean of non-zero elements\n",
      "2025-01-20 16:16:59,500 [INFO] Batch 12: batch_size 32\n",
      "2025-01-20 16:16:59,535 [INFO] Batch 12: Hidden states shape: torch.Size([32, 48, 768])\n",
      "2025-01-20 16:16:59,537 [INFO] Computing non-zero element counts\n",
      "2025-01-20 16:16:59,538 [INFO] Computing sum of non-zero elements\n",
      "2025-01-20 16:16:59,540 [INFO] Computing mean of non-zero elements\n",
      "2025-01-20 16:16:59,541 [INFO] Batch 13: batch_size 32\n",
      "2025-01-20 16:16:59,575 [INFO] Batch 13: Hidden states shape: torch.Size([32, 47, 768])\n",
      "2025-01-20 16:16:59,577 [INFO] Computing non-zero element counts\n",
      "2025-01-20 16:16:59,578 [INFO] Computing sum of non-zero elements\n",
      "2025-01-20 16:16:59,579 [INFO] Computing mean of non-zero elements\n",
      "Processing batches:  81%|████████▏ | 13/16 [00:00<00:00, 14.86it/s]2025-01-20 16:16:59,582 [INFO] Batch 14: batch_size 32\n",
      "2025-01-20 16:16:59,616 [INFO] Batch 14: Hidden states shape: torch.Size([32, 44, 768])\n",
      "2025-01-20 16:16:59,617 [INFO] Computing non-zero element counts\n",
      "2025-01-20 16:16:59,619 [INFO] Computing sum of non-zero elements\n",
      "2025-01-20 16:16:59,620 [INFO] Computing mean of non-zero elements\n",
      "2025-01-20 16:16:59,621 [INFO] Batch 15: batch_size 32\n",
      "2025-01-20 16:16:59,656 [INFO] Batch 15: Hidden states shape: torch.Size([32, 48, 768])\n",
      "2025-01-20 16:16:59,657 [INFO] Computing non-zero element counts\n",
      "2025-01-20 16:16:59,658 [INFO] Computing sum of non-zero elements\n",
      "2025-01-20 16:16:59,659 [INFO] Computing mean of non-zero elements\n",
      "2025-01-20 16:16:59,661 [INFO] Batch 16: batch_size 32\n",
      "2025-01-20 16:16:59,699 [INFO] Batch 16: Hidden states shape: torch.Size([6, 31, 768])\n",
      "2025-01-20 16:16:59,702 [INFO] Computing non-zero element counts\n",
      "2025-01-20 16:16:59,703 [INFO] Computing sum of non-zero elements\n",
      "2025-01-20 16:16:59,704 [INFO] Computing mean of non-zero elements\n",
      "Processing batches: 100%|██████████| 16/16 [00:01<00:00, 14.74it/s]\n",
      "2025-01-20 16:16:59,709 [INFO] Total non-zero element shape: torch.Size([24576])\n",
      "2025-01-20 16:16:59,711 [INFO] oppose\n",
      "2025-01-20 16:16:59,714 [INFO] Running model with cache to obtain hidden states\n",
      "Processing batches:   0%|          | 0/16 [00:00<?, ?it/s]2025-01-20 16:16:59,717 [INFO] Batch 1: batch_size 32\n",
      "2025-01-20 16:16:59,752 [INFO] Batch 1: Hidden states shape: torch.Size([32, 46, 768])\n",
      "2025-01-20 16:16:59,753 [INFO] Computing non-zero element counts\n",
      "2025-01-20 16:16:59,755 [INFO] Computing sum of non-zero elements\n",
      "2025-01-20 16:16:59,756 [INFO] Computing mean of non-zero elements\n",
      "2025-01-20 16:16:59,757 [INFO] Batch 2: batch_size 32\n",
      "2025-01-20 16:16:59,792 [INFO] Batch 2: Hidden states shape: torch.Size([32, 41, 768])\n",
      "2025-01-20 16:16:59,795 [INFO] Computing non-zero element counts\n",
      "2025-01-20 16:16:59,796 [INFO] Computing sum of non-zero elements\n",
      "2025-01-20 16:16:59,797 [INFO] Computing mean of non-zero elements\n",
      "2025-01-20 16:16:59,799 [INFO] Batch 3: batch_size 32\n",
      "2025-01-20 16:16:59,834 [INFO] Batch 3: Hidden states shape: torch.Size([32, 46, 768])\n",
      "2025-01-20 16:16:59,835 [INFO] Computing non-zero element counts\n",
      "2025-01-20 16:16:59,837 [INFO] Computing sum of non-zero elements\n",
      "2025-01-20 16:16:59,838 [INFO] Computing mean of non-zero elements\n",
      "Processing batches:  19%|█▉        | 3/16 [00:00<00:00, 24.40it/s]2025-01-20 16:16:59,841 [INFO] Batch 4: batch_size 32\n",
      "2025-01-20 16:16:59,878 [INFO] Batch 4: Hidden states shape: torch.Size([32, 44, 768])\n",
      "2025-01-20 16:16:59,880 [INFO] Computing non-zero element counts\n",
      "2025-01-20 16:16:59,881 [INFO] Computing sum of non-zero elements\n",
      "2025-01-20 16:16:59,882 [INFO] Computing mean of non-zero elements\n",
      "2025-01-20 16:16:59,884 [INFO] Batch 5: batch_size 32\n",
      "2025-01-20 16:16:59,921 [INFO] Batch 5: Hidden states shape: torch.Size([32, 43, 768])\n",
      "2025-01-20 16:16:59,923 [INFO] Computing non-zero element counts\n",
      "2025-01-20 16:16:59,925 [INFO] Computing sum of non-zero elements\n",
      "2025-01-20 16:16:59,926 [INFO] Computing mean of non-zero elements\n",
      "2025-01-20 16:16:59,928 [INFO] Batch 6: batch_size 32\n",
      "2025-01-20 16:16:59,964 [INFO] Batch 6: Hidden states shape: torch.Size([32, 44, 768])\n",
      "2025-01-20 16:16:59,966 [INFO] Computing non-zero element counts\n",
      "2025-01-20 16:16:59,968 [INFO] Computing sum of non-zero elements\n",
      "2025-01-20 16:16:59,969 [INFO] Computing mean of non-zero elements\n",
      "Processing batches:  38%|███▊      | 6/16 [00:00<00:00, 23.53it/s]2025-01-20 16:16:59,972 [INFO] Batch 7: batch_size 32\n",
      "2025-01-20 16:17:00,007 [INFO] Batch 7: Hidden states shape: torch.Size([32, 46, 768])\n",
      "2025-01-20 16:17:00,009 [INFO] Computing non-zero element counts\n",
      "2025-01-20 16:17:00,010 [INFO] Computing sum of non-zero elements\n",
      "2025-01-20 16:17:00,011 [INFO] Computing mean of non-zero elements\n",
      "2025-01-20 16:17:00,014 [INFO] Batch 8: batch_size 32\n",
      "2025-01-20 16:17:00,052 [INFO] Batch 8: Hidden states shape: torch.Size([32, 42, 768])\n",
      "2025-01-20 16:17:00,054 [INFO] Computing non-zero element counts\n",
      "2025-01-20 16:17:00,055 [INFO] Computing sum of non-zero elements\n",
      "2025-01-20 16:17:00,056 [INFO] Computing mean of non-zero elements\n",
      "2025-01-20 16:17:00,058 [INFO] Batch 9: batch_size 32\n",
      "2025-01-20 16:17:00,094 [INFO] Batch 9: Hidden states shape: torch.Size([32, 49, 768])\n",
      "2025-01-20 16:17:00,095 [INFO] Computing non-zero element counts\n",
      "2025-01-20 16:17:00,097 [INFO] Computing sum of non-zero elements\n",
      "2025-01-20 16:17:00,098 [INFO] Computing mean of non-zero elements\n",
      "Processing batches:  56%|█████▋    | 9/16 [00:00<00:00, 23.36it/s]2025-01-20 16:17:00,101 [INFO] Batch 10: batch_size 32\n",
      "2025-01-20 16:17:00,136 [INFO] Batch 10: Hidden states shape: torch.Size([32, 49, 768])\n",
      "2025-01-20 16:17:00,138 [INFO] Computing non-zero element counts\n",
      "2025-01-20 16:17:00,139 [INFO] Computing sum of non-zero elements\n",
      "2025-01-20 16:17:00,140 [INFO] Computing mean of non-zero elements\n",
      "2025-01-20 16:17:00,141 [INFO] Batch 11: batch_size 32\n",
      "2025-01-20 16:17:00,175 [INFO] Batch 11: Hidden states shape: torch.Size([32, 42, 768])\n",
      "2025-01-20 16:17:00,177 [INFO] Computing non-zero element counts\n",
      "2025-01-20 16:17:00,178 [INFO] Computing sum of non-zero elements\n",
      "2025-01-20 16:17:00,179 [INFO] Computing mean of non-zero elements\n",
      "2025-01-20 16:17:00,181 [INFO] Batch 12: batch_size 32\n",
      "2025-01-20 16:17:00,216 [INFO] Batch 12: Hidden states shape: torch.Size([32, 53, 768])\n",
      "2025-01-20 16:17:00,218 [INFO] Computing non-zero element counts\n",
      "2025-01-20 16:17:00,219 [INFO] Computing sum of non-zero elements\n",
      "2025-01-20 16:17:00,220 [INFO] Computing mean of non-zero elements\n",
      "Processing batches:  75%|███████▌  | 12/16 [00:00<00:00, 23.86it/s]2025-01-20 16:17:00,223 [INFO] Batch 13: batch_size 32\n",
      "2025-01-20 16:17:00,257 [INFO] Batch 13: Hidden states shape: torch.Size([32, 43, 768])\n",
      "2025-01-20 16:17:00,259 [INFO] Computing non-zero element counts\n",
      "2025-01-20 16:17:00,261 [INFO] Computing sum of non-zero elements\n",
      "2025-01-20 16:17:00,262 [INFO] Computing mean of non-zero elements\n",
      "2025-01-20 16:17:00,264 [INFO] Batch 14: batch_size 32\n",
      "2025-01-20 16:17:00,299 [INFO] Batch 14: Hidden states shape: torch.Size([32, 45, 768])\n",
      "2025-01-20 16:17:00,301 [INFO] Computing non-zero element counts\n",
      "2025-01-20 16:17:00,303 [INFO] Computing sum of non-zero elements\n",
      "2025-01-20 16:17:00,304 [INFO] Computing mean of non-zero elements\n",
      "2025-01-20 16:17:00,305 [INFO] Batch 15: batch_size 32\n",
      "2025-01-20 16:17:00,341 [INFO] Batch 15: Hidden states shape: torch.Size([32, 38, 768])\n",
      "2025-01-20 16:17:00,342 [INFO] Computing non-zero element counts\n",
      "2025-01-20 16:17:00,344 [INFO] Computing sum of non-zero elements\n",
      "2025-01-20 16:17:00,345 [INFO] Computing mean of non-zero elements\n",
      "Processing batches:  94%|█████████▍| 15/16 [00:00<00:00, 23.94it/s]2025-01-20 16:17:00,347 [INFO] Batch 16: batch_size 32\n",
      "2025-01-20 16:17:00,384 [INFO] Batch 16: Hidden states shape: torch.Size([6, 42, 768])\n",
      "2025-01-20 16:17:00,386 [INFO] Computing non-zero element counts\n",
      "2025-01-20 16:17:00,387 [INFO] Computing sum of non-zero elements\n",
      "2025-01-20 16:17:00,388 [INFO] Computing mean of non-zero elements\n",
      "Processing batches: 100%|██████████| 16/16 [00:00<00:00, 23.78it/s]\n",
      "2025-01-20 16:17:00,391 [INFO] Total non-zero element shape: torch.Size([24576])\n"
     ]
    }
   ],
   "source": [
    "steer_info={}\n",
    "from functools import partial\n",
    "if TASK=='polite' or TASK==\"sentiment\":\n",
    "    if args.device==\"cpu\":\n",
    "        logging.info(\"CPU处理\")\n",
    "        from cpu_utils import get_activation_by_steer_cpu\n",
    "        get_activation_by_steer_cpu=partial(get_activation_by_steer_cpu,sae=sae,model=model,device=args.device,batch_size=args.batch_size,top_k_mean=args.topk_mean,top_k_cnt=args.topk_cnt)\n",
    "    logging.info(\"from\"+args.source+\"to\"+args.target)\n",
    "    logging.info(f\"positive\")\n",
    "    text=pos_train_set[\"text\"][:args.data_size]\n",
    "    steer_info[\"pos\"]=get_activation_by_steer(text)\n",
    "    logging.info(f\"negative\")\n",
    "    text=neg_train_set[\"text\"][:args.data_size]\n",
    "    steer_info[\"neg\"]=get_activation_by_steer(text)\n",
    "    logging.info(f\"neutral\")\n",
    "    text=neu_train_set[\"text\"][:args.data_size]\n",
    "    steer_info[\"neu\"]=get_activation_by_steer(text)\n",
    "elif TASK=='debate':\n",
    "    if args.device==\"cpu\":\n",
    "        logging.info(\"CPU处理\")\n",
    "        from cpu_utils import get_activation_by_steer_cpu\n",
    "        get_activation_by_steer_cpu=partial(get_activation_by_steer_cpu,sae=sae,model=model,device=args.device,batch_size=args.batch_size,top_k_mean=args.topk_mean,top_k_cnt=args.topk_cnt)\n",
    "    logging.info(\"from\"+args.source+\"to\"+args.target)\n",
    "    logging.info(f\"support\")\n",
    "    text=sup_train_set[\"text\"]\n",
    "    steer_info[\"sup\"]=get_activation_by_steer(text)\n",
    "    logging.info(f\"oppose\")\n",
    "    text=opp_train_set[\"text\"]\n",
    "    steer_info[\"opp\"]=get_activation_by_steer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sup': {'latent_value_mean': tensor([5.2969, 5.7562, 0.9151,  ..., 1.0190, 3.8079, 0.0000], device='cuda:0'),\n",
       "  'latent_frequency': tensor([ 4.,  8.,  1.,  ..., 27.,  4.,  0.], device='cuda:0')},\n",
       " 'opp': {'latent_value_mean': tensor([1.7246, 2.0211, 6.5648,  ..., 0.7094, 0.0000, 0.0693], device='cuda:0'),\n",
       "  'latent_frequency': tensor([ 2.,  1.,  2.,  ..., 22.,  0.,  1.], device='cuda:0')}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steer_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 16:17:06,126 [INFO] 转向方向 dif_opp-sup_relu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(False, device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "source=args.source\n",
    "target=args.target\n",
    "# 调整样本正负性在这里调整 从负样本到正样本还是从正样本()到负样本\n",
    "# pos 代表积极情绪\n",
    "# neg 代表消极情绪\n",
    "\n",
    "\n",
    "# %%\n",
    "assert bool(torch.all((steer_info[\"sup\"][\"latent_value_mean\"]-steer_info[\"opp\"][\"latent_value_mean\"])==0))==False,\"数据库读取有问题\"\n",
    "assert torch.all(steer_info[target][\"latent_value_mean\"]>=0),\"所有SAE的激活需要大于d等于0（maybe）\"\n",
    "\n",
    "logging.info(f\"转向方向 dif_{target}-{source}_relu\")\n",
    "steer_info[f\"dif_{target}-{source}_relu\"]={\"latent_frequency\":torch.relu(steer_info[target][\"latent_frequency\"]-steer_info[source][\"latent_frequency\"]),\"latent_value_mean\":torch.relu(steer_info[target][\"latent_value_mean\"]-steer_info[source][\"latent_value_mean\"]),\"target_nz_mean\":torch.relu(steer_info[target][\"latent_value_mean\"])}\n",
    "\n",
    "top_k=args.topk_cnt\n",
    "_,steer_indices=torch.topk(steer_info[f\"dif_{target}-{source}_relu\"][\"latent_frequency\"],top_k)\n",
    "\n",
    "\n",
    "# 假设 steer_info[f\"dif_{b}-{a}_relu\"][\"latent_frequency\"] 是一个 NumPy 数组\n",
    "lat_freq = steer_info[f\"dif_{target}-{source}_relu\"][\"latent_frequency\"]\n",
    "# 先获取非零元素的索引\n",
    "lat_acti_indices = np.nonzero(lat_freq)\n",
    "torch.all(lat_freq == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3309, 0.1696, 0.1970, 0.1038, 0.1705, 1.0051, 0.3258, 0.3164, 0.3293,\n",
       "        0.3271, 0.8777, 0.0272, 0.0171, 0.0577, 0.0879, 0.0640, 0.0753, 0.0251,\n",
       "        0.1720, 0.2338, 0.0384, 0.2178, 0.1202, 0.4218, 0.6355, 0.0438, 0.1294,\n",
       "        0.0545, 0.2123, 0.1494, 0.1205, 0.0000, 0.0478, 0.0000, 0.0764, 0.1165,\n",
       "        0.1423, 1.8745, 0.0809, 0.2035, 0.3341, 0.0258, 0.0178, 0.0141, 0.0673,\n",
       "        0.0000, 0.8087, 0.1078, 0.0861, 0.2246, 1.3967, 0.1415, 0.0000, 0.2475,\n",
       "        0.0000, 0.1158, 0.6421, 0.2660, 0.0269, 0.1622, 0.1523, 3.1804, 0.0508,\n",
       "        0.5813, 0.3387, 0.1146, 0.0000, 0.0000, 1.1453, 0.2003, 0.0106, 0.7030,\n",
       "        0.0364, 0.0989, 0.2309, 0.0233, 3.1515, 0.3171, 0.1475, 0.0000, 0.0000,\n",
       "        0.0000, 0.1077, 0.0925, 0.5686, 0.1631, 0.0814, 0.0000, 0.1898, 0.0000,\n",
       "        0.0683, 0.0000, 9.0391, 0.2754, 0.0833, 0.0353, 0.0876, 0.2485, 0.0763,\n",
       "        0.0000], device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steer_info[f\"dif_{target}-{source}_relu\"][\"latent_frequency\"].shape\n",
    "\n",
    "\n",
    "steer_info[f\"dif_{target}-{source}_relu\"][\"latent_value_mean\"][steer_indices]# 这里有0,没有负数比较正常"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 16:17:10,578 [INFO] Computing steering vectors using method: val_mul\n",
      "2025-01-20 16:17:10,598 [INFO] Steering vectors computed with shape: torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "def compute_delta_matrix(sae: SAE, indices: Tensor, nz_mean_val: Tensor, method: str = \"val_mul\") -> Tensor:\n",
    "    logging.info(f\"Computing steering vectors using method: {method}\")\n",
    "    if method == \"mean\":\n",
    "        delta_matrix = torch.mean(sae.W_dec[indices], dim=0)\n",
    "    elif method == \"val_mul\":\n",
    "        delta_matrix = torch.zeros(sae.W_dec.shape[1], device=sae.W_dec.device)\n",
    "        for idx in indices:\n",
    "            delta_matrix += nz_mean_val[idx].item() * sae.W_dec[idx]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    logging.info(f\"Steering vectors computed with shape: {delta_matrix.shape}\")\n",
    "    return delta_matrix\n",
    "if args.mean_type==\"dif_mean\":\n",
    "    delta_matrix=compute_delta_matrix(sae,indices=steer_indices,nz_mean_val=steer_info[f\"dif_{target}-{source}_relu\"][\"latent_value_mean\"],method=\"val_mul\")\n",
    "elif args.mean_type==\"tar_mean\":\n",
    "    delta_matrix=compute_delta_matrix(sae,indices=steer_indices,nz_mean_val=steer_info[f\"dif_{target}-{source}_relu\"][\"target_nz_mean\"],method=\"val_mul\")\n",
    "else:\n",
    "    raise ValueError(\"Unsupported\")\n",
    "\n",
    "import einops\n",
    "steer_cnt=0\n",
    "\n",
    "def steering_hook(resid_pre, hook,steer_on, alpha, steer_type=\"last\"):\n",
    "    if resid_pre.shape[1] == 1:\n",
    "        return\n",
    "    # 判断是否进行干预\n",
    "    if steer_on:\n",
    "        if steer_type == \"last\":\n",
    "            # 对最后一个token前的部分应用干预，使用给定的 delta_matrix\n",
    "            resid_pre[:, :-1, :] += alpha * delta_matrix# best\n",
    "            # 如果提前干预效果会更好更连贯\n",
    "        elif steer_type == \"gaussian\":\n",
    "            # 使用高斯卷积对输入进行干预\n",
    "            from utils import half_gaussian_kernel\n",
    "            d_m=torch.clone(delta_matrix)\n",
    "            s = resid_pre[:, :-1, :].shape[1]\n",
    "            b=resid_pre[:, :-1, :].shape[0]\n",
    "            h=resid_pre[:, :-1, :].shape[2]\n",
    "            h_gauss = half_gaussian_kernel(s)  # 获取高斯卷积核\n",
    "            # k_gauss=torch.cat([h_gauss, torch.tensor([0])])\n",
    "            k_gauss=h_gauss\n",
    "            k_gau_repeat=einops.repeat(k_gauss,\"s -> b s h\",b=b,h=h)\n",
    "            d_m_repeat=einops.repeat(d_m,\"h -> b s h\",b=b,s=s)\n",
    "            # 根据卷积结果更新 resid_pre（注意：保留其他维度不变）,逐一元素相乘\n",
    "            resid_pre[:, :-1, :] += alpha * d_m_repeat* k_gau_repeat\n",
    "            # logging.info(f\"干预类型：高斯\")\n",
    "            # logging.info(f\"干预矩阵: {alpha * d_m_repeat* k_gau_repeat}\")\n",
    "        elif steer_type == \"all\":\n",
    "            resid_pre[:, :, :] += alpha * delta_matrix# 全部干预\n",
    "        elif steer_type == \"last2\":\n",
    "            resid_pre[:, :-2, :] += args.alpha * delta_matrix # 提前两个token进行干预\n",
    "        else:\n",
    "            raise ValueError(\"Unknown steering type\")\n",
    "\n",
    "def hooked_generate(prompt_batch, fwd_hooks=[], seed=None, **kwargs):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    with model.hooks(fwd_hooks=fwd_hooks):\n",
    "        tokenized = model.to_tokens(prompt_batch,prepend_bos=False)\n",
    "        result = model.generate(\n",
    "            stop_at_eos=True,  # avoids a bug on MPS\n",
    "            input=tokenized,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            do_sample=True,\n",
    "            **kwargs,\n",
    "        )\n",
    "    return result\n",
    "\n",
    "def run_generate(example_prompt,sampling_kwargs,steer_on,alpha,steer_type=\"last\",repeat_num=3,show_res=False):\n",
    "    model.reset_hooks()\n",
    "    if steer_on:\n",
    "        steering_hook_fn=partial(steering_hook,steer_on=steer_on,alpha=alpha,steer_type=steer_type)\n",
    "        editing_hooks = [(f\"blocks.{args.layer}.hook_resid_post\", steering_hook_fn)]\n",
    "    else:\n",
    "        editing_hooks=[]\n",
    "    res = hooked_generate(\n",
    "        [example_prompt] * repeat_num, editing_hooks, seed=None, **sampling_kwargs\n",
    "    )\n",
    "\n",
    "    # Print results, removing the ugly beginning of sequence token\n",
    "    res_str = model.to_string(res[:, :])\n",
    "    if show_res:\n",
    "        for idx, text in enumerate(res_str):\n",
    "            logging.info(f\"Generated Text: {idx+1}:\\n{text}\")\n",
    "        \n",
    "    return res_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "def eval_stance(text):\n",
    "    eval_model_path = \"/home/ckqsudo/code2024/0models/ACL_debate_model/distilbert-base-uncased__tweet_eval_stance__all-train\"\n",
    "    eval_tokenizer = AutoTokenizer.from_pretrained(eval_model_path)\n",
    "    eval_model = AutoModelForSequenceClassification.from_pretrained(eval_model_path)\n",
    "    inputs = eval_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = eval_model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # 解析输出，选择概率最大的类别\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "\n",
    "    print(probabilities, predicted_class)\n",
    "    class_names = [\"none\", \"against\", \"support\"]\n",
    "    print(f\"Predicted class: {class_names[predicted_class]}\")\n",
    "\n",
    "\n",
    "example_texts = [\"I believe that it is the most restrictive in the country. It's not a perfect law, but it's certainly not one that will be overturned by the Supreme Court. It is very difficult to say what kind of effect this will have on women who are\",\n",
    "                 \n",
    "\"It's not an absolute ban, but it does make it harder for women to get abortions. It also makes it harder for women to get pregnant in the first place. So we're going to have more restrictions on abortion and more restrictions on\",\n",
    "\n",
    "\"We should ban partial birth abortions. In 2007, the organization was involved in a Supreme Court case in which it helped to overturn a state law that required abortion providers to provide an ultrasound before they performed an abortion. The American Civil Liberties Union of Texas has filed a lawsuit against the state's law, arguing that it violates the First Amendment and violates women's right\",\n",
    "\n",
    "\"We should ban partial birth abortions. In 2007, the organization was involved in a Supreme Court case in which it helped to overturn a state law that required abortion providers to provide ultrasounds for women who had been raped or sexually assaulted. The American Civil Liberties Union of Texas has filed a lawsuit against the state's medical board, arguing that its decision not to allow\",\n",
    "\n",
    "\"I think it's really important to understand the history of the anti-war movement against the right-wing, against the American people against their own government, said Michael O'Brien, a professor at Stanford University's Center for Global and Global Security. \",\n",
    "\"I think it's really important to understand the history of the fight against that word, against the words that are used in this language, said Mr. Kitz. The word 'war' is not a name for any kind of weapon or weapon of any\",\n",
    "\"I think it's really important to understand the history of the fight against that very same anti-war, anti-imperialist, and anti-imperialist movement against the American people, said Senator John Kerry, chairman of the Senate Foreign Relations Committee.\",\n",
    "\"I believe that the U.S. is the only country on Earth that has made a significant contribution to the fight against this type of war against our own people against all of these different kinds of weapons, including those designed to destroy and destroy our very existence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 16:46:57,540 [INFO] Example prompt: Trump is about to take office as the President of the United States. What do you think of this new president?\n",
      "2025-01-20 16:46:57,543 [INFO] Generating texts **without** steering... \n",
      "2025-01-20 16:46:59,488 [INFO] Generated Text: 1:\n",
      "Trump is about to take office as the President of the United States. What do you think of this new president?\n",
      "\n",
      "TRUMP: Well, I think he's going to be a very good president. He's going to be a very good leader. And I think he'll have a lot of great people in place, and I'm sure they're going to be very happy with him. But we're not going anywhere soon enough for that. We're not going anywhere soon enough for that because we've got so many problems and so many problems with our country, and it's time for us to get out\n",
      "2025-01-20 16:46:59,491 [INFO] Generated Text: 2:\n",
      "Trump is about to take office as the President of the United States. What do you think of this new president?\n",
      "\n",
      "TRUMP: Well, I think he's going to be a very good president. He's going to be a very good leader. And I think he'll have a lot of great people in place, and I'm sure they're going to be very happy with him. But we're not going anywhere soon enough for that. We're not going anywhere soon enough for that because we've got so many problems and so many problems with our country, and it's just too much time for us\n",
      "2025-01-20 16:46:59,492 [INFO] Generated Text: 3:\n",
      "Trump is about to take office as the President of the United States. What do you think of this new president?\n",
      "\n",
      "TRUMP: Well, I think he's going to be a very good president. He's going to be a very good leader. And I think he'll have a lot of great people in place, and I'm sure they're going to be very happy with him. But we're not going anywhere soon enough for that. We're not going anywhere soon enough for that because we've got so many problems and so many problems with our country, and it's just too much time for us\n",
      "2025-01-20 16:46:59,494 [INFO] 干预之后的结果\n",
      "2025-01-20 16:46:59,495 [INFO] 干预方向sup->opp,礼貌任务下，neg=impolite，情感任务下 pos=积极情感\n",
      "2025-01-20 16:46:59,497 [INFO] ** Generating texts with steering... Target **\n",
      "2025-01-20 16:46:59,498 [INFO] form sup to target\n",
      "2025-01-20 16:47:01,268 [INFO] Generated Text: 1:\n",
      "Trump is about to take office as the President of the United States. What do you think of this new president?\n",
      "\n",
      "SARAH: I'm not going to answer that question. I'm going to talk about what we've been fighting against for over a decade, and what we've been fighting against against our own people in the streets and in our communities. And that's one of the things that I've been very passionate about, and it's one of my most important contributions to this book. It's a book that is really focused on how we fight for an end to all these books on this\n",
      "2025-01-20 16:47:01,270 [INFO] Generated Text: 2:\n",
      "Trump is about to take office as the President of the United States. What do you think of this new president?\n",
      "\n",
      "I'm not going to answer that question. I'm going to talk about what I've been doing for a very long time, and what I've been doing for a very long time against the anti-war, anti-pro­grams and pro­grams that are really against my name on this platform. And I will tell you, in my fight against these men who have come forward with their own book called The War Against Our Nation: The Fight Against the Book,\" we\n",
      "2025-01-20 16:47:01,271 [INFO] Generated Text: 3:\n",
      "Trump is about to take office as the President of the United States. What do you think of this new president?\n",
      "\n",
      "I'm not going to answer that question. I'm going to talk about what I've been doing for a very long time, and what I've been doing for a very long time against the American people. And that's why I'm here today in this fight against this kind of radical, anti-Semitic, anti-Semitic and anti-­­­­­­­​​​against all these people who are fighting on behalf of our brothers and sisters in our community against these\n"
     ]
    }
   ],
   "source": [
    "# Example prompt from the selected set\n",
    "example_prompt = \"Trump is about to take office as the President of the United States. What do you think of this new president?\"\n",
    "logging.info(f\"Example prompt: {example_prompt}\")\n",
    "\n",
    "# Generate without steering\n",
    "\n",
    "logging.info(\"Generating texts **without** steering... \")\n",
    "generated_texts_no_steer = run_generate(example_prompt, sampling_kwargs,steer_on=False,alpha=0,show_res=True)\n",
    "logging.info(\"干预之后的结果\")\n",
    "# bef,aft=args.steer.split(\"-\")\n",
    "logging.info(f\"干预方向{source}->{target},礼貌任务下，neg=impolite，情感任务下 pos=积极情感\")\n",
    "logging.info(\"** Generating texts with steering... Target **\")\n",
    "logging.info(f\"form {source} to target\")\n",
    "generated_texts_with_steer = run_generate(\n",
    "    example_prompt, \n",
    "    sampling_kwargs,\n",
    "    steer_on=True,\n",
    "    alpha=args.alpha,\n",
    "    steer_type=args.steer_type,\n",
    "    show_res=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1479, 0.7170, 0.1352]]) 1\n",
      "Predicted class: against\n",
      "tensor([[0.1438, 0.7172, 0.1390]]) 1\n",
      "Predicted class: against\n",
      "tensor([[0.1479, 0.7170, 0.1352]]) 1\n",
      "Predicted class: against\n",
      "tensor([[0.1942, 0.6852, 0.1206]]) 1\n",
      "Predicted class: against\n",
      "tensor([[0.1360, 0.7323, 0.1317]]) 1\n",
      "Predicted class: against\n",
      "tensor([[0.2102, 0.6561, 0.1337]]) 1\n",
      "Predicted class: against\n"
     ]
    }
   ],
   "source": [
    "for text in generated_texts_no_steer:\n",
    "    eval_stance(text.replace('Trump is about to take office as the President of the United States. What do you think of this new president?','').replace('<|endoftext|>',''))\n",
    "for text in generated_texts_with_steer:\n",
    "    eval_stance(text.replace('Trump is about to take office as the President of the United States. What do you think of this new president?','').replace('<|endoftext|>',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "support\n"
     ]
    }
   ],
   "source": [
    "# Please install OpenAI SDK first: `pip3 install openai`\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import os\n",
    "load_dotenv()\n",
    "deepseek_api_key = os.getenv('OPENAI_API_KEY')\n",
    "client = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "text = \"Trump is about to take office as the President of the United States. What do you think of this new president? TRUMP: Well, I think he's going to be a very good president. He's going to be a very good leader. And I think he'll have a lot of great people in place, and I'm sure they're going to be very happy with him. But we're not going anywhere soon enough for that. We're not going anywhere soon enough for that because we've got so many problems and so many problems with our country, and it's just too much time for us\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant, you need to help me determine the stance of a given sentence, whether it conveys a supportive or an opposing tone. Your answer can only be one of the two words 'support' or 'oppose'.\"},\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ],\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args.prompt_path\n",
    "\n",
    "def load_and_prepare_sentiment_prompts(prompt_path:str,task:str):\n",
    "    assert task in [\"sentiment\"],\"请输入正确的任务\"\n",
    "    logging.info(f\"Loading prompt_path from {prompt_path}\")\n",
    "    prompt_files = {\"neg\": \"negative_prompts.jsonl\", \"pos\": \"positive_prompts.jsonl\",\"neu\":\"neutral_prompts.jsonl\"}\n",
    "    prompts= load_dataset(\"/home/ckqsudo/code2024/0refer_ACL/LM-Steer/data/data/prompts/sentiment_prompts-10k\",data_files=prompt_files)\n",
    "    print(prompts)\n",
    "    return prompts\n",
    "\n",
    "\n",
    "\n",
    "import copy\n",
    "# Example prompt from the selected set\n",
    "import jsonlines\n",
    "\n",
    "def eval_on_full_data():\n",
    "    if TASK==\"sentiment\":\n",
    "        prompts=load_and_prepare_sentiment_prompts(prompt_path=args.prompt_path,task=TASK)\n",
    "    elif TASK==\"politeness\":\n",
    "        # prompts=load_and_prepare_politeness_prompts(pormpt_path=args.prompt_path,sample=args.seed)\n",
    "        pass\n",
    "    else:\n",
    "        raise NotImplementedError(\"No Supported Task\")\n",
    "    prompts=load_and_prepare_sentiment_prompts(prompt_path=args.prompt_path,task=TASK)\n",
    "    \n",
    "    param={**vars(args),**sampling_kwargs,\"max_new_tokens\":50,\"steer\":f\"from {source} to {target}\"}\n",
    "    param[\"alpha_recheck\"]=ALPHA\n",
    "    logging.info(f\"Running with alpha: {ALPHA}\")\n",
    "    logging.info(f\"Running with prompt_type: \"+str(param[\"steer\"]))\n",
    "    # res.append(params)\n",
    "\n",
    "    no_steer_res=[]\n",
    "    steer_res=[]\n",
    "\n",
    "    assert source in prompts,\"prompt steer source not in prompts\"\n",
    "    # 打开文件（模式为追加模式 'a'）\n",
    "    with jsonlines.open(os.path.join(output_dir,\"params.jsonl\"), mode='w') as writer:\n",
    "        writer.write(param)  # 逐行写入\n",
    "    SAVE_COMPARED=args.save_compared\n",
    "    with jsonlines.open(os.path.join(output_dir,\"no_steer_gen_res.jsonl\"), mode='w') as nt_file:\n",
    "        with jsonlines.open(os.path.join(output_dir,\"steer_gen_res.jsonl\"), mode='w') as t_file: \n",
    "            for idx,item in tqdm(enumerate(list(prompts[source])[:])):\n",
    "                prompt=item[\"prompt\"][\"text\"]\n",
    "                item[\"label\"]=source\n",
    "                # 没转向的结果\n",
    "                if SAVE_COMPARED:\n",
    "                    no_steer_gen_texts = run_generate(\n",
    "                        prompt, \n",
    "                        sampling_kwargs,\n",
    "                        steer_on=False,\n",
    "                        alpha=None,\n",
    "                        repeat_num=2,\n",
    "                        steer_type=None,\n",
    "                        show_res=False)\n",
    "                    no_steer_item=copy.deepcopy(item)\n",
    "                    no_steer_item[\"generations\"]=[]\n",
    "                    for gen_text in no_steer_gen_texts:\n",
    "                        no_steer_item[\"generations\"].append({\"text\":gen_text})\n",
    "                    # no_steer_res.append(no_steer_item)\n",
    "                    nt_file.write(no_steer_item)\n",
    "                    # 转向的结果\n",
    "                \n",
    "                # steer_on = True\n",
    "                steered_texts=run_generate(prompt, \n",
    "                                        sampling_kwargs,\n",
    "                                        steer_on=True,\n",
    "                                        alpha=ALPHA,\n",
    "                                        steer_type=STEER_TYPE,\n",
    "                                        repeat_num=2,\n",
    "                                        show_res=False\n",
    "                                        )\n",
    "                steer_item=copy.deepcopy(item)\n",
    "                steer_item[\"generations\"]=[]\n",
    "                for steer_gen in steered_texts:\n",
    "                    steer_item[\"generations\"].append({\"text\":steer_gen})\n",
    "                if idx%50==0:\n",
    "                    logging.info(f\"{TASK}: from {source} to {target} prompt_set: {source}\")\n",
    "                    logging.info(steer_item)\n",
    "                t_file.write(steer_item)\n",
    "\n",
    "if args.debug:\n",
    "    logging.info(f\"debug mode, no full data eval\")\n",
    "else:\n",
    "    eval_on_full_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
