{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "import argparse\n",
    "import os\n",
    "import logging\n",
    "from typing import Tuple\n",
    "import json\n",
    "from log import setup_logging\n",
    "from tqdm import tqdm  # For progress bars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE\n",
    "# from sae_lens.analysis.neuronpedia_integration import get_neuronpedia_quick_list\n",
    "# from sae_lens.analysis.feature_statistics import (\n",
    "#     get_all_stats_dfs,\n",
    "#     get_W_U_W_dec_stats_df,\n",
    "# )\n",
    "# from sae_lens.analysis.tsea import (\n",
    "#     get_enrichment_df,\n",
    "#     manhattan_plot_enrichment_scores,\n",
    "#     plot_top_k_feature_projections_by_token_and_category,\n",
    "#     get_baby_name_sets,\n",
    "#     get_letter_gene_sets,\n",
    "#     generate_pos_sets,\n",
    "#     get_test_gene_sets,\n",
    "#     get_gene_set_from_regex,\n",
    "# )\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "# import plotly_express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 进行基础情感干预实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "task=\"sentiment\"\n",
    "if task==\"sentiment\":\n",
    "    args_dict = {\n",
    "        \"layer\": 6,  # Example layer number to analyze\n",
    "        \"LLM\": \"gpt2-small\",\n",
    "        \"dataset_path\": \"/home/ckqsudo/code2024/0dataset/baseline-acl/data/sentiment/sst5\",\n",
    "        \"prompt_path\":\"/home/ckqsudo/code2024/0dataset/baseline-acl/data/prompts/sentiment_prompts-10k\",\n",
    "        \"output_dir\": \"./results\",\n",
    "        \"env_path\": \"/home/ckqsudo/code2024/CKQ_ACL2024/Control_Infer/SAE-simple/.env\",\n",
    "        \"task\":\"sentiment\",# “sentiment”,\"cot\",\"polite\"\n",
    "        \"seed\": 42,\n",
    "        \"data_size\": 1000,\n",
    "        \"device\": \"cpu\",  # Options: \"cpu\", \"cuda\", \"mps\", \"auto\"\n",
    "        \"alpha\": 100, # 这个alpha后面慢慢调节\n",
    "        \"steer\": \"pos-neg\",  # Options: \"pos\", \"neg\", \"neu\",\"pos-neg\",\"cot-direct\"\n",
    "        \"method\": \"val_mul\",  # Options: \"mean\", \"val_mul\" 用val_mul会比较好\n",
    "        \"topk_mean\": 100, # 选取前topk 个均值激活，这个效果一般，会导致很多如what？why？这种被激活\n",
    "        \"topk_cnt\": 100, # 选取前topk个频率激活，目前默认这个，效果很好\n",
    "        \"batch_size\": 32 # 这个好像没用上\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 进行COT相关实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "if task==\"cot\":\n",
    "    args_dict = {\n",
    "        \"layer\": 6,  # Example layer number to analyze\n",
    "        \"LLM\": \"gpt2-small\",\n",
    "        \"dataset_path\": \"/home/ckqsudo/code2024/0dataset/ACL_useful_dataset/math/COT_GSM8k\",\n",
    "        \"output_dir\": \"./results\",\n",
    "        \"env_path\": \"/home/ckqsudo/code2024/CKQ_ACL2024/Control_Infer/SAE-simple/.env\",\n",
    "        \"seed\": 42,\n",
    "        \"data_size\": 1000,\n",
    "        \"device\": \"cpu\",  # Options: \"cpu\", \"cuda\", \"mps\", \"auto\"\n",
    "        \"alpha\": 100,\n",
    "        \"steer\": \"cot-direct\",  # Options: \"pos\", \"neg\", \"neu\",\"pos-neg\",\"cot-direct\"\n",
    "        \"method\": \"val_mul\",  # Options: \"mean\", \"val_mul\"\n",
    "        \"topk_mean\": 100,\n",
    "        \"topk_cnt\": 100,\n",
    "        \"batch_size\": 32\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 进行礼貌实验\n",
    "/home/ckqsudo/code2024/0dataset/ACL_useful_dataset/style_transfer/politeness-corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "if task==\"polite\":\n",
    "    args_dict = {\n",
    "        \"layer\": 6,  # Example layer number to analyze\n",
    "        \"LLM\": \"gpt2-small\",\n",
    "        \"dataset_path\": \"/home/ckqsudo/code2024/0dataset/ACL_useful_dataset/style_transfer/politeness-corpus\",\n",
    "        \"output_dir\": \"./results\",\n",
    "        \"env_path\": \"/home/ckqsudo/code2024/CKQ_ACL2024/Control_Infer/SAE-simple/.env\",\n",
    "        \"seed\": 42,\n",
    "        \"data_size\": 1000,\n",
    "        \"device\": \"cpu\",  # Options: \"cpu\", \"cuda\", \"mps\", \"auto\"\n",
    "        \"alpha\": 100,\n",
    "        \"steer\": \"polite-impolite\",  # Options: \"pos\", \"neg\", \"neu\",\"pos-neg\",\"cot-direct\"\n",
    "        \"method\": \"val_mul\",  # Options: \"mean\", \"val_mul\"\n",
    "        \"topk_mean\": 100,\n",
    "        \"topk_cnt\": 100,\n",
    "        \"batch_size\": 32\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行礼貌实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "gpt2-small\n",
      "./results\n",
      "pos-neg\n"
     ]
    }
   ],
   "source": [
    "# Configuration and Hyperparameters\n",
    "# 将字典转换为 argparse.Namespace 对象\n",
    "args = argparse.Namespace(**args_dict)\n",
    "# 测试访问属性\n",
    "print(args.layer)  # 输出: 10\n",
    "print(args.LLM)  # 输出: gpt2-small\n",
    "print(args.output_dir)  # 输出: ./results\n",
    "print(args.steer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-14 11:42:01,959 [INFO] Logging initialized. Logs will be saved to ./results/LLM_gpt2-small_layer_6_steer_pos-neg_alpha_100_sentiment_cnt_100_mean100/execution.log\n",
      "2025-01-14 11:42:01,961 [INFO] Hyperparameters:\n",
      "2025-01-14 11:42:01,962 [INFO]   layer: 6\n",
      "2025-01-14 11:42:01,965 [INFO]   LLM: gpt2-small\n",
      "2025-01-14 11:42:01,966 [INFO]   dataset_path: /home/ckqsudo/code2024/0dataset/baseline-acl/data/sentiment/sst5\n",
      "2025-01-14 11:42:01,967 [INFO]   prompt_path: /home/ckqsudo/code2024/0dataset/baseline-acl/data/prompts/sentiment_prompts-10k\n",
      "2025-01-14 11:42:01,969 [INFO]   output_dir: ./results\n",
      "2025-01-14 11:42:01,970 [INFO]   env_path: /home/ckqsudo/code2024/CKQ_ACL2024/Control_Infer/SAE-simple/.env\n",
      "2025-01-14 11:42:01,971 [INFO]   task: sentiment\n",
      "2025-01-14 11:42:01,972 [INFO]   seed: 42\n",
      "2025-01-14 11:42:01,973 [INFO]   data_size: 1000\n",
      "2025-01-14 11:42:01,974 [INFO]   device: cpu\n",
      "2025-01-14 11:42:01,975 [INFO]   alpha: 100\n",
      "2025-01-14 11:42:01,976 [INFO]   steer: pos-neg\n",
      "2025-01-14 11:42:01,977 [INFO]   method: val_mul\n",
      "2025-01-14 11:42:01,977 [INFO]   topk_mean: 100\n",
      "2025-01-14 11:42:01,978 [INFO]   topk_cnt: 100\n",
      "2025-01-14 11:42:01,979 [INFO]   batch_size: 32\n"
     ]
    }
   ],
   "source": [
    "# Logging Setup\n",
    "import os\n",
    "from log import setup_logging\n",
    "import logging\n",
    "# Create output directory base path\n",
    "output_dir_base = os.path.join(\n",
    "    args.output_dir,\n",
    "    f\"LLM_{args.LLM}_layer_{args.layer}_steer_{args.steer}_alpha_{args.alpha}_{task}_cnt_{args.topk_cnt}_mean{args.topk_mean}\"\n",
    ")\n",
    "\n",
    "# Setup logging\n",
    "setup_logging(output_dir_base)\n",
    "\n",
    "# Save hyperparameters\n",
    "hyperparams = args_dict\n",
    "\n",
    "# Log hyperparameters\n",
    "logging.info(\"Hyperparameters:\")\n",
    "for key, value in hyperparams.items():\n",
    "    logging.info(f\"  {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-14 11:42:05,802 [INFO] HF_ENDPOINT: https://hf-mirror.com\n"
     ]
    }
   ],
   "source": [
    "# Load Environment Variables\n",
    "def load_environment(env_path: str):\n",
    "    load_dotenv(env_path)\n",
    "    hf_endpoint = os.getenv('HF_ENDPOINT', 'https://hf-mirror.com')\n",
    "    logging.info(f\"HF_ENDPOINT: {hf_endpoint}\")\n",
    "load_environment(args.env_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def load_and_prepare_triple_dataset(dataset_path: str,dataset_name:str, seed: int, num_samples: int):\n",
    "    \"\"\"\n",
    "    支持positive\\neutral\\negative三元组数据类型，例如 sst5，polite数据集和multi-class数据集\n",
    "\n",
    "    Args:\n",
    "        dataset_path (str): _description_\n",
    "        dataset_name : \"sst5\",\"multiclass\",\"polite\"\n",
    "        seed (int): _description_\n",
    "        num_samples (int): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    assert dataset_name in [\"sst5\",\"multiclass\",\"polite\"]\n",
    "    if dataset_name in [\"sst5\"]:\n",
    "        neu_label=2 # 中性情感对应的label\n",
    "        assert \"sst5\" in dataset_path\n",
    "    elif  dataset_name in [\"polite\",\"multiclass\"]:\n",
    "        neu_label=1\n",
    "    logging.info(f\"Loading dataset from ****{dataset_path}***\")\n",
    "    dataset = load_dataset(dataset_path)\n",
    "    dataset[\"train\"] = dataset['train'].shuffle(seed=seed)\n",
    "\n",
    "    logging.info(\"Filtering dataset for negative, positive, and neutral samples\")\n",
    "    neg_train_set = dataset['train'].filter(lambda example: example['label'] < neu_label).select(range(num_samples))\n",
    "    pos_train_set = dataset['train'].filter(lambda example: example['label'] == neu_label).select(range(num_samples))\n",
    "    neu_train_set = dataset['train'].filter(lambda example: example['label'] > neu_label ).select(range(num_samples))\n",
    "\n",
    "    logging.info(f\"Selected {len(neg_train_set)} negative, {len(pos_train_set)} positive, and {len(neu_train_set)} neutral samples\")\n",
    "    print(dataset)\n",
    "    if dataset_name in [\"sst5\"]:\n",
    "        val_set=dataset['validation']\n",
    "    else:\n",
    "        raise ValueError(\"没写呢\")\n",
    "    test_set=dataset[\"test\"]\n",
    "    return neg_train_set, pos_train_set, neu_train_set,val_set,test_set\n",
    "def load_and_prepare_COT_dataset(dataset_path:str,seed:int,num_samples:int):\n",
    "    logging.info(f\"Loading dataset from {dataset_path}\")\n",
    "    dataset = load_dataset(dataset_path)\n",
    "    dataset[\"train\"] = dataset['train'].shuffle(seed=seed)\n",
    "    logging.info(\"Filtering dataset for COT\")\n",
    "    # 定义一个函数来提取答案\n",
    "    def extract_answer(text):\n",
    "        # 使用正则表达式提取答案\n",
    "        match = re.search(r'#### ([-+]?\\d*\\.?\\d+/?\\d*)', text)\n",
    "        if match:\n",
    "            label=match.group(1)\n",
    "            return label\n",
    "        else:\n",
    "            raise ValueError(\"Modify your re expression\")\n",
    "    def concat_QA(example,col1,col2,tag):\n",
    "        combined = f\"{example[col1]}{tag}{example[col2]}\"  # 用空格拼接\n",
    "        return combined\n",
    "    def replace_col(example,col1,target,pattern):\n",
    "        return example[col1].replace(target,pattern)\n",
    "    # 应用函数并创建新列\n",
    "    dataset = dataset.map(lambda example: {'A': extract_answer(example['response'])})\n",
    "    dataset = dataset.map(lambda example: {'Q+A': concat_QA(example,\"prompt\",\"A\",\"\")})\n",
    "    dataset = dataset.map(lambda example: {'Q+COT_A': concat_QA(example,\"prompt\",\"response\",\"\")})\n",
    "    dataset = dataset.map(lambda example: {'Q+COT_A': replace_col(example,\"Q+COT_A\",\"#### \",\"\")})\n",
    "    # 查看处理后的数据集\n",
    "    print(\"Q+A\\n\",dataset['train'][103]['Q+A'])\n",
    "    print(\"Q+COT_A\\n\",dataset['train'][103]['Q+COT_A'])\n",
    "    return dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos-neg'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.steer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-14 11:42:15,348 [INFO] dataset path /home/ckqsudo/code2024/0dataset/baseline-acl/data/sentiment/sst5\n",
      "2025-01-14 11:42:15,351 [INFO] Loading dataset from ****/home/ckqsudo/code2024/0dataset/baseline-acl/data/sentiment/sst5***\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "2025-01-14 11:42:15,353 [WARNING] Repo card metadata block was not found. Setting CardData to empty.\n",
      "2025-01-14 11:42:15,528 [INFO] Filtering dataset for negative, positive, and neutral samples\n",
      "2025-01-14 11:42:15,542 [INFO] Selected 1000 negative, 1000 positive, and 1000 neutral samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'label_text'],\n",
      "        num_rows: 8544\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label', 'label_text'],\n",
      "        num_rows: 1101\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'label_text'],\n",
      "        num_rows: 2210\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load and Prepare Dataset\n",
    "\n",
    "logging.info(\"dataset path \"+args.dataset_path)\n",
    "if \"neg\" in args.steer or \"pos\" in args.steer and args.steer==\"sentiment\":\n",
    "    neg_train_set, pos_train_set, neu_train_set,val_set,test_set = load_and_prepare_triple_dataset(\n",
    "        args.dataset_path, \"sst5\",args.seed, args.data_size\n",
    "    )\n",
    "elif \"cot\" in args.steer or \"COT\" in args.steer:\n",
    "    logging.info(\"COT \"*10)\n",
    "    all_dataset=load_and_prepare_COT_dataset(\n",
    "        args.dataset_path, args.seed, args.data_size\n",
    "    )\n",
    "elif \"polite\" in args.steer:\n",
    "    logging.info(\"polite\"*10)\n",
    "    neg_train_set, pos_train_set, neu_train_set,val_set,test_set=load_and_prepare_triple_dataset(args.dataset_path,\"polite\", args.seed, args.data_size)\n",
    "else:\n",
    "    raise ValueError(\"No Supported\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert neg_train_set[10]!=pos_train_set[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'text': 'some are fascinating and others are not , and in the end , it is almost a good movie .',\n",
       "  'label': 2,\n",
       "  'label_text': 'neutral'},\n",
       " {'text': 'the lousy lead performances ... keep the movie from ever reaching the comic heights it obviously desired .',\n",
       "  'label': 1,\n",
       "  'label_text': 'negative'})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_train_set[10],neg_train_set[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_latents(sae: SAE, model: HookedTransformer, texts: list, hook_point: str, device: str, batch_size: int) -> list:\n",
    "    \"\"\"\n",
    "    计算 latents，支持批次处理。\n",
    "\n",
    "    Args:\n",
    "        sae (SAE): SAE 实例。\n",
    "        model (HookedTransformer): Transformer 模型实例。\n",
    "        texts (list): 文本列表。\n",
    "        hook_point (str): 钩子点名称。\n",
    "        device (str): 计算设备。\n",
    "        batch_size (int): 每个批次的大小。\n",
    "\n",
    "    Returns:\n",
    "        list: 包含每个批次 latents 的张量列表。\n",
    "    \"\"\"\n",
    "    logging.info(\"Running model with cache to obtain hidden states\")\n",
    "    batch_latents = []\n",
    "\n",
    "    # 使用 tqdm 显示进度条\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Processing batches\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        sv_logits, cache = model.run_with_cache(batch_texts, prepend_bos=False, device=device)\n",
    "        batch_hidden_states = cache[hook_point]\n",
    "        logging.info(f\"Batch {i // batch_size + 1}: Hidden states shape: {batch_hidden_states.shape}\")\n",
    "\n",
    "        logging.info(f\"Encoding hidden states for batch {i // batch_size + 1}\")\n",
    "        # 假设 sae.encode 支持批量编码\n",
    "        latents = sae.encode(batch_hidden_states)  # 形状: (batch_size, latent_dim)\n",
    "        batch_latents.append(latents)\n",
    "        \n",
    "\n",
    "    logging.info(f\"Total batches processed: {len(batch_latents)}\")\n",
    "    return batch_latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_steering_vectors(sae: SAE, overlap_indices: Tensor, nz_mean: Tensor, method: str = \"val_mul\") -> Tensor:\n",
    "    logging.info(f\"Computing steering vectors using method: {method}\")\n",
    "    if method == \"mean\":\n",
    "        steering_vectors = torch.mean(sae.W_dec[overlap_indices], dim=0)\n",
    "    elif method == \"val_mul\":\n",
    "        steering_vectors = torch.zeros(sae.W_dec.shape[1], device=sae.W_dec.device)\n",
    "        for important_idx in overlap_indices:\n",
    "            steering_vectors += nz_mean[important_idx].item() * sae.W_dec[important_idx]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    logging.info(f\"Steering vectors computed with shape: {steering_vectors.shape}\")\n",
    "    return steering_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_results(output_dir: str, nz_mean: Tensor, act_cnt: Tensor, generated_texts: list, hyperparams: dict):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    # Save nz_mean and act_cnt\n",
    "    nz_stats_path = os.path.join(output_dir, 'nz_stats.pt')\n",
    "    logging.info(f\"Saving nz_mean and act_cnt to {nz_stats_path}\")\n",
    "    torch.save({\n",
    "        'nz_mean': nz_mean,\n",
    "        'act_cnt': act_cnt,\n",
    "    }, nz_stats_path)\n",
    "\n",
    "    # Save generated texts\n",
    "    generated_texts_path = os.path.join(output_dir, 'generated_texts.txt')\n",
    "    logging.info(f\"Saving generated texts to {generated_texts_path}\")\n",
    "    with open(generated_texts_path, 'w') as f:\n",
    "        for text in generated_texts:\n",
    "            f.write(text + \"\\n\")\n",
    "\n",
    "    # Save hyperparameters\n",
    "    hyperparams_path = os.path.join(output_dir, 'hyperparameters.json')\n",
    "    logging.info(f\"Saving hyperparameters to {hyperparams_path}\")\n",
    "    with open(hyperparams_path, 'w') as f:\n",
    "        json.dump(hyperparams, f, indent=4)\n",
    "\n",
    "    logging.info(\"All results saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./results/LLM_gpt2-small_layer_6_steer_pos-neg_alpha_100_cnt_100_mean100'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir_base = os.path.join(\n",
    "    args.output_dir,\n",
    "    f\"LLM_{args.LLM}_layer_{args.layer}_steer_{args.steer}_alpha_{args.alpha}_cnt_{args.topk_cnt}_mean{args.topk_mean}\"\n",
    ")\n",
    "output_dir_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-14 11:43:03,515 [INFO] non cache: ./results/LLM_gpt2-small_layer_6_steer_pos-neg_alpha_100_cnt_100_mean100/hyperparameters.json\n"
     ]
    }
   ],
   "source": [
    "def load_from_cache():\n",
    "    cache_exists = False\n",
    "    cache_file = os.path.join(output_dir_base, 'hyperparameters.json')\n",
    "    if os.path.exists(cache_file):\n",
    "        with open(cache_file, 'r') as f:\n",
    "            cached_data = json.load(f)\n",
    "        cached_hash = cached_data.get('hyperparams_hash')\n",
    "\n",
    "    if cache_exists:\n",
    "        # Load nz_mean and act_cnt from cache\n",
    "        # nz_stats_path = os.path.join(output_dir_base, 'nz_stats.pt')\n",
    "        # nz_act = torch.load(nz_stats_path)\n",
    "        # nz_mean = nz_act['nz_mean']\n",
    "        # act_cnt = nz_act['act_cnt']\n",
    "        # overlap_indices = nz_act.get('overlap_indices', None)  # If overlap_indices was saved\n",
    "        logging.info(\"load from cache\")\n",
    "    else:\n",
    "        # overlap_indices = None  # Will be computed later\n",
    "        logging.info(\"non cache: \"+cache_file)\n",
    "load_from_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup_logging(output_dir_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-14 11:43:06,271 [INFO] Hyperparameters:\n",
      "2025-01-14 11:43:06,273 [INFO]   layer: 6\n",
      "2025-01-14 11:43:06,275 [INFO]   LLM: gpt2-small\n",
      "2025-01-14 11:43:06,277 [INFO]   dataset_path: /home/ckqsudo/code2024/0dataset/baseline-acl/data/sentiment/sst5\n",
      "2025-01-14 11:43:06,279 [INFO]   prompt_path: /home/ckqsudo/code2024/0dataset/baseline-acl/data/prompts/sentiment_prompts-10k\n",
      "2025-01-14 11:43:06,280 [INFO]   output_dir: ./results\n",
      "2025-01-14 11:43:06,283 [INFO]   env_path: /home/ckqsudo/code2024/CKQ_ACL2024/Control_Infer/SAE-simple/.env\n",
      "2025-01-14 11:43:06,285 [INFO]   task: sentiment\n",
      "2025-01-14 11:43:06,287 [INFO]   seed: 42\n",
      "2025-01-14 11:43:06,288 [INFO]   data_size: 1000\n",
      "2025-01-14 11:43:06,290 [INFO]   device: cpu\n",
      "2025-01-14 11:43:06,290 [INFO]   alpha: 100\n",
      "2025-01-14 11:43:06,291 [INFO]   steer: pos-neg\n",
      "2025-01-14 11:43:06,292 [INFO]   method: val_mul\n",
      "2025-01-14 11:43:06,293 [INFO]   topk_mean: 100\n",
      "2025-01-14 11:43:06,295 [INFO]   topk_cnt: 100\n",
      "2025-01-14 11:43:06,297 [INFO]   batch_size: 32\n",
      "2025-01-14 11:43:06,300 [INFO] HF_ENDPOINT: https://hf-mirror.com\n",
      "2025-01-14 11:43:06,302 [INFO] Loading model: gpt2-small\n",
      "2025-01-14 11:43:47,109 [INFO] Loading SAE for layer 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ckqsudo/miniconda3/envs/SAE/lib/python3.11/site-packages/sae_lens/sae.py:145: UserWarning: \n",
      "This SAE has non-empty model_from_pretrained_kwargs. \n",
      "For optimal performance, load the model like so:\n",
      "model = HookedSAETransformer.from_pretrained_no_processing(..., **cfg.model_from_pretrained_kwargs)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Save hyperparameters\n",
    "hyperparams = vars(args)\n",
    "\n",
    "# Log hyperparameters\n",
    "logging.info(\"Hyperparameters:\")\n",
    "for key, value in hyperparams.items():\n",
    "    logging.info(f\"  {key}: {value}\")\n",
    "\n",
    "# Load environment\n",
    "load_environment(args.env_path)\n",
    "\n",
    "# Load model and SAE\n",
    "logging.info(f\"Loading model: {args.LLM}\")\n",
    "model = HookedTransformer.from_pretrained(args.LLM, device=args.device)\n",
    "\n",
    "logging.info(f\"Loading SAE for layer {args.layer}\")\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release=\"gpt2-small-res-jb\",\n",
    "    sae_id=f\"blocks.{args.layer}.hook_resid_pre\",\n",
    "    device=args.device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load dataset\n",
    "# all_dataset = load_and_prepare_triple_dataset(\n",
    "#     args.dataset_path, args.seed, args.data_size\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos-neg'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.steer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_latents(batch_latents: Tensor, top_k_mean: int = 100, top_k_cnt: int = 100) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "    logging.info(\"Computing non-zero element counts\")\n",
    "    act_cnt = (batch_latents != 0).sum(dim=(0, 1))\n",
    "\n",
    "    logging.info(\"Computing sum of non-zero elements\")\n",
    "    nz_sum = torch.where(batch_latents != 0, batch_latents, torch.tensor(0.0, device=batch_latents.device)).sum(dim=(0, 1))\n",
    "\n",
    "    logging.info(\"Computing mean of non-zero elements\")\n",
    "    nz_mean = torch.where(act_cnt != 0, nz_sum / act_cnt, torch.tensor(0.0, device=batch_latents.device))\n",
    "\n",
    "    logging.info(\"Selecting top-k indices based on nz_mean\")\n",
    "    nz_act_val, nz_val_indices = torch.topk(nz_mean, top_k_mean)\n",
    "    logging.info(f\"Top {top_k_mean} nz_mean values selected.\")\n",
    "\n",
    "    logging.info(\"Selecting top-k indices based on act_cnt\")\n",
    "    nz_cnt, cnt_indices = torch.topk(act_cnt, top_k_cnt)\n",
    "    logging.info(f\"Top {top_k_cnt} act_cnt values selected.\")\n",
    "\n",
    "    # logging.info(\"Finding overlapping indices between nz_mean and act_cnt top-k\")\n",
    "    # overlap_mask = torch.isin(nz_val_indices, cnt_indices)\n",
    "    # overlap_indices = nz_val_indices[overlap_mask]\n",
    "    # logging.info(f\"Number of overlapping indices: {len(overlap_indices)}\")\n",
    "    # overlap_indices=overlap_indices\n",
    "    return nz_mean, act_cnt,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a dataset steer based on steering preference\n",
    "\n",
    "def get_activation_by_steer(texts:list):\n",
    "\n",
    "    hook_point = sae.cfg.hook_name\n",
    "\n",
    "    # Compute latents with batch processing\n",
    "    batch_latents = compute_latents(sae, model, texts, hook_point, args.device, args.batch_size)\n",
    "    # 计算第二个维度的最大值\n",
    "    max_dim1 = max(latent.shape[1] for latent in batch_latents)  # 第二个维度的最大值\n",
    "    logging.info(f\"最大长度:{max_dim1}\")\n",
    "    # 对每个 Tensor 进行填充（仅填充第二个维度）\n",
    "    padded_latents_right = [\n",
    "        torch.nn.functional.pad(latent, (0, 0, 0, max_dim1 - latent.size(1)), \"constant\", 0)\n",
    "        for latent in batch_latents\n",
    "    ]\n",
    "\n",
    "    batch_latents_concatenated = torch.cat(padded_latents_right, dim=0)\n",
    "    logging.info(f\"Concatenated batch latents shape: {batch_latents_concatenated.shape}\")\n",
    "\n",
    "    # Analyze latents \n",
    "    nz_mean, act_cnt, _ = analyze_latents(batch_latents_concatenated, top_k_mean=args.topk_mean, top_k_cnt=args.topk_cnt)\n",
    "    return {\"nz_mean\":nz_mean,\"nz_cnt\":act_cnt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.steer=args.steer.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_dataset[\"train\"][\"Q+A\"][:args.data_size][193]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 26000(SAE稀疏神经元)对应的非零激活神经元激活统计信息，和激活值统计信息\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "steer_info={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-14 11:44:59,911 [INFO] pos-neg\n",
      "2025-01-14 11:44:59,938 [INFO] Running model with cache to obtain hidden states\n",
      "Processing batches:   0%|          | 0/32 [00:00<?, ?it/s]2025-01-14 11:45:00,312 [INFO] Batch 1: Hidden states shape: torch.Size([32, 47, 768])\n",
      "2025-01-14 11:45:00,313 [INFO] Encoding hidden states for batch 1\n",
      "Processing batches:   3%|▎         | 1/32 [00:00<00:12,  2.39it/s]2025-01-14 11:45:00,741 [INFO] Batch 2: Hidden states shape: torch.Size([32, 51, 768])\n",
      "2025-01-14 11:45:00,743 [INFO] Encoding hidden states for batch 2\n",
      "Processing batches:   6%|▋         | 2/32 [00:00<00:12,  2.44it/s]2025-01-14 11:45:01,054 [INFO] Batch 3: Hidden states shape: torch.Size([32, 50, 768])\n",
      "2025-01-14 11:45:01,056 [INFO] Encoding hidden states for batch 3\n",
      "Processing batches:   9%|▉         | 3/32 [00:01<00:10,  2.73it/s]2025-01-14 11:45:01,380 [INFO] Batch 4: Hidden states shape: torch.Size([32, 52, 768])\n",
      "2025-01-14 11:45:01,382 [INFO] Encoding hidden states for batch 4\n",
      "Processing batches:  12%|█▎        | 4/32 [00:01<00:09,  2.85it/s]2025-01-14 11:45:01,685 [INFO] Batch 5: Hidden states shape: torch.Size([32, 49, 768])\n",
      "2025-01-14 11:45:01,686 [INFO] Encoding hidden states for batch 5\n",
      "Processing batches:  16%|█▌        | 5/32 [00:01<00:09,  2.95it/s]2025-01-14 11:45:01,993 [INFO] Batch 6: Hidden states shape: torch.Size([32, 48, 768])\n",
      "2025-01-14 11:45:01,994 [INFO] Encoding hidden states for batch 6\n",
      "Processing batches:  19%|█▉        | 6/32 [00:02<00:08,  3.03it/s]2025-01-14 11:45:02,318 [INFO] Batch 7: Hidden states shape: torch.Size([32, 47, 768])\n",
      "2025-01-14 11:45:02,318 [INFO] Encoding hidden states for batch 7\n",
      "Processing batches:  22%|██▏       | 7/32 [00:02<00:08,  3.03it/s]2025-01-14 11:45:02,644 [INFO] Batch 8: Hidden states shape: torch.Size([32, 34, 768])\n",
      "2025-01-14 11:45:02,645 [INFO] Encoding hidden states for batch 8\n",
      "Processing batches:  25%|██▌       | 8/32 [00:02<00:07,  3.14it/s]2025-01-14 11:45:02,919 [INFO] Batch 9: Hidden states shape: torch.Size([32, 45, 768])\n",
      "2025-01-14 11:45:02,921 [INFO] Encoding hidden states for batch 9\n",
      "Processing batches:  28%|██▊       | 9/32 [00:02<00:07,  3.26it/s]2025-01-14 11:45:03,244 [INFO] Batch 10: Hidden states shape: torch.Size([32, 50, 768])\n",
      "2025-01-14 11:45:03,245 [INFO] Encoding hidden states for batch 10\n",
      "Processing batches:  31%|███▏      | 10/32 [00:03<00:07,  3.11it/s]2025-01-14 11:45:03,569 [INFO] Batch 11: Hidden states shape: torch.Size([32, 49, 768])\n",
      "2025-01-14 11:45:03,570 [INFO] Encoding hidden states for batch 11\n",
      "Processing batches:  34%|███▍      | 11/32 [00:03<00:06,  3.10it/s]2025-01-14 11:45:03,933 [INFO] Batch 12: Hidden states shape: torch.Size([32, 56, 768])\n",
      "2025-01-14 11:45:03,935 [INFO] Encoding hidden states for batch 12\n",
      "Processing batches:  38%|███▊      | 12/32 [00:04<00:06,  2.97it/s]2025-01-14 11:45:04,302 [INFO] Batch 13: Hidden states shape: torch.Size([32, 53, 768])\n",
      "2025-01-14 11:45:04,303 [INFO] Encoding hidden states for batch 13\n",
      "Processing batches:  41%|████      | 13/32 [00:04<00:06,  2.89it/s]2025-01-14 11:45:04,602 [INFO] Batch 14: Hidden states shape: torch.Size([32, 47, 768])\n",
      "2025-01-14 11:45:04,604 [INFO] Encoding hidden states for batch 14\n",
      "Processing batches:  44%|████▍     | 14/32 [00:04<00:05,  3.09it/s]2025-01-14 11:45:04,859 [INFO] Batch 15: Hidden states shape: torch.Size([32, 41, 768])\n",
      "2025-01-14 11:45:04,861 [INFO] Encoding hidden states for batch 15\n",
      "Processing batches:  47%|████▋     | 15/32 [00:04<00:05,  3.31it/s]2025-01-14 11:45:05,083 [INFO] Batch 16: Hidden states shape: torch.Size([32, 35, 768])\n",
      "2025-01-14 11:45:05,084 [INFO] Encoding hidden states for batch 16\n",
      "Processing batches:  50%|█████     | 16/32 [00:05<00:04,  3.61it/s]2025-01-14 11:45:05,317 [INFO] Batch 17: Hidden states shape: torch.Size([32, 44, 768])\n",
      "2025-01-14 11:45:05,318 [INFO] Encoding hidden states for batch 17\n",
      "Processing batches:  53%|█████▎    | 17/32 [00:05<00:03,  3.77it/s]2025-01-14 11:45:05,573 [INFO] Batch 18: Hidden states shape: torch.Size([32, 46, 768])\n",
      "2025-01-14 11:45:05,575 [INFO] Encoding hidden states for batch 18\n",
      "Processing batches:  56%|█████▋    | 18/32 [00:05<00:03,  3.75it/s]2025-01-14 11:45:05,849 [INFO] Batch 19: Hidden states shape: torch.Size([32, 45, 768])\n",
      "2025-01-14 11:45:05,850 [INFO] Encoding hidden states for batch 19\n",
      "Processing batches:  59%|█████▉    | 19/32 [00:05<00:03,  3.77it/s]2025-01-14 11:45:06,177 [INFO] Batch 20: Hidden states shape: torch.Size([32, 60, 768])\n",
      "2025-01-14 11:45:06,178 [INFO] Encoding hidden states for batch 20\n",
      "Processing batches:  62%|██████▎   | 20/32 [00:06<00:03,  3.39it/s]2025-01-14 11:45:06,520 [INFO] Batch 21: Hidden states shape: torch.Size([32, 51, 768])\n",
      "2025-01-14 11:45:06,521 [INFO] Encoding hidden states for batch 21\n",
      "Processing batches:  66%|██████▌   | 21/32 [00:06<00:03,  3.26it/s]2025-01-14 11:45:06,861 [INFO] Batch 22: Hidden states shape: torch.Size([32, 56, 768])\n",
      "2025-01-14 11:45:06,863 [INFO] Encoding hidden states for batch 22\n",
      "Processing batches:  69%|██████▉   | 22/32 [00:06<00:03,  3.14it/s]2025-01-14 11:45:07,144 [INFO] Batch 23: Hidden states shape: torch.Size([32, 42, 768])\n",
      "2025-01-14 11:45:07,145 [INFO] Encoding hidden states for batch 23\n",
      "Processing batches:  72%|███████▏  | 23/32 [00:07<00:02,  3.35it/s]2025-01-14 11:45:07,403 [INFO] Batch 24: Hidden states shape: torch.Size([32, 44, 768])\n",
      "2025-01-14 11:45:07,405 [INFO] Encoding hidden states for batch 24\n",
      "Processing batches:  75%|███████▌  | 24/32 [00:07<00:02,  3.47it/s]2025-01-14 11:45:07,705 [INFO] Batch 25: Hidden states shape: torch.Size([32, 53, 768])\n",
      "2025-01-14 11:45:07,706 [INFO] Encoding hidden states for batch 25\n",
      "Processing batches:  78%|███████▊  | 25/32 [00:07<00:02,  3.34it/s]2025-01-14 11:45:08,045 [INFO] Batch 26: Hidden states shape: torch.Size([32, 53, 768])\n",
      "2025-01-14 11:45:08,046 [INFO] Encoding hidden states for batch 26\n",
      "Processing batches:  81%|████████▏ | 26/32 [00:08<00:01,  3.21it/s]2025-01-14 11:45:08,334 [INFO] Batch 27: Hidden states shape: torch.Size([32, 44, 768])\n",
      "2025-01-14 11:45:08,336 [INFO] Encoding hidden states for batch 27\n",
      "Processing batches:  84%|████████▍ | 27/32 [00:08<00:01,  3.37it/s]2025-01-14 11:45:08,626 [INFO] Batch 28: Hidden states shape: torch.Size([32, 52, 768])\n",
      "2025-01-14 11:45:08,628 [INFO] Encoding hidden states for batch 28\n",
      "Processing batches:  88%|████████▊ | 28/32 [00:08<00:01,  3.34it/s]2025-01-14 11:45:08,896 [INFO] Batch 29: Hidden states shape: torch.Size([32, 44, 768])\n",
      "2025-01-14 11:45:08,898 [INFO] Encoding hidden states for batch 29\n",
      "Processing batches:  91%|█████████ | 29/32 [00:08<00:00,  3.42it/s]2025-01-14 11:45:09,202 [INFO] Batch 30: Hidden states shape: torch.Size([32, 49, 768])\n",
      "2025-01-14 11:45:09,204 [INFO] Encoding hidden states for batch 30\n",
      "Processing batches:  94%|█████████▍| 30/32 [00:09<00:00,  3.36it/s]2025-01-14 11:45:09,772 [INFO] Batch 31: Hidden states shape: torch.Size([32, 48, 768])\n",
      "2025-01-14 11:45:09,774 [INFO] Encoding hidden states for batch 31\n",
      "Processing batches:  97%|█████████▋| 31/32 [00:09<00:00,  2.64it/s]2025-01-14 11:45:09,952 [INFO] Batch 32: Hidden states shape: torch.Size([8, 40, 768])\n",
      "2025-01-14 11:45:09,954 [INFO] Encoding hidden states for batch 32\n",
      "Processing batches: 100%|██████████| 32/32 [00:10<00:00,  3.19it/s]\n",
      "2025-01-14 11:45:09,965 [INFO] Total batches processed: 32\n",
      "2025-01-14 11:45:09,968 [INFO] 最大长度:60\n",
      "2025-01-14 11:45:10,400 [INFO] Concatenated batch latents shape: torch.Size([1000, 60, 24576])\n",
      "2025-01-14 11:45:10,401 [INFO] Computing non-zero element counts\n",
      "2025-01-14 11:45:11,389 [INFO] Computing sum of non-zero elements\n",
      "2025-01-14 11:45:11,897 [INFO] Computing mean of non-zero elements\n",
      "2025-01-14 11:45:11,899 [INFO] Selecting top-k indices based on nz_mean\n",
      "2025-01-14 11:45:11,900 [INFO] Top 100 nz_mean values selected.\n",
      "2025-01-14 11:45:11,901 [INFO] Selecting top-k indices based on act_cnt\n",
      "2025-01-14 11:45:11,902 [INFO] Top 100 act_cnt values selected.\n",
      "2025-01-14 11:45:12,321 [INFO] Running model with cache to obtain hidden states\n",
      "Processing batches:   0%|          | 0/32 [00:00<?, ?it/s]2025-01-14 11:45:12,589 [INFO] Batch 1: Hidden states shape: torch.Size([32, 55, 768])\n",
      "2025-01-14 11:45:12,590 [INFO] Encoding hidden states for batch 1\n",
      "Processing batches:   3%|▎         | 1/32 [00:00<00:09,  3.31it/s]2025-01-14 11:45:12,890 [INFO] Batch 2: Hidden states shape: torch.Size([32, 51, 768])\n",
      "2025-01-14 11:45:12,892 [INFO] Encoding hidden states for batch 2\n",
      "Processing batches:   6%|▋         | 2/32 [00:00<00:09,  3.32it/s]2025-01-14 11:45:13,171 [INFO] Batch 3: Hidden states shape: torch.Size([32, 48, 768])\n",
      "2025-01-14 11:45:13,173 [INFO] Encoding hidden states for batch 3\n",
      "Processing batches:   9%|▉         | 3/32 [00:00<00:08,  3.37it/s]2025-01-14 11:45:13,488 [INFO] Batch 4: Hidden states shape: torch.Size([32, 52, 768])\n",
      "2025-01-14 11:45:13,490 [INFO] Encoding hidden states for batch 4\n",
      "Processing batches:  12%|█▎        | 4/32 [00:01<00:08,  3.26it/s]2025-01-14 11:45:13,757 [INFO] Batch 5: Hidden states shape: torch.Size([32, 37, 768])\n",
      "2025-01-14 11:45:13,758 [INFO] Encoding hidden states for batch 5\n",
      "Processing batches:  16%|█▌        | 5/32 [00:01<00:07,  3.47it/s]2025-01-14 11:45:14,030 [INFO] Batch 6: Hidden states shape: torch.Size([32, 45, 768])\n",
      "2025-01-14 11:45:14,032 [INFO] Encoding hidden states for batch 6\n",
      "Processing batches:  19%|█▉        | 6/32 [00:01<00:07,  3.55it/s]2025-01-14 11:45:14,301 [INFO] Batch 7: Hidden states shape: torch.Size([32, 45, 768])\n",
      "2025-01-14 11:45:14,302 [INFO] Encoding hidden states for batch 7\n",
      "Processing batches:  22%|██▏       | 7/32 [00:02<00:07,  3.55it/s]2025-01-14 11:45:14,611 [INFO] Batch 8: Hidden states shape: torch.Size([32, 51, 768])\n",
      "2025-01-14 11:45:14,612 [INFO] Encoding hidden states for batch 8\n",
      "Processing batches:  25%|██▌       | 8/32 [00:02<00:07,  3.42it/s]2025-01-14 11:45:14,954 [INFO] Batch 9: Hidden states shape: torch.Size([32, 53, 768])\n",
      "2025-01-14 11:45:14,955 [INFO] Encoding hidden states for batch 9\n",
      "Processing batches:  28%|██▊       | 9/32 [00:02<00:07,  3.25it/s]2025-01-14 11:45:15,291 [INFO] Batch 10: Hidden states shape: torch.Size([32, 54, 768])\n",
      "2025-01-14 11:45:15,293 [INFO] Encoding hidden states for batch 10\n",
      "Processing batches:  31%|███▏      | 10/32 [00:03<00:06,  3.16it/s]2025-01-14 11:45:15,600 [INFO] Batch 11: Hidden states shape: torch.Size([32, 50, 768])\n",
      "2025-01-14 11:45:15,601 [INFO] Encoding hidden states for batch 11\n",
      "Processing batches:  34%|███▍      | 11/32 [00:03<00:06,  3.19it/s]2025-01-14 11:45:15,903 [INFO] Batch 12: Hidden states shape: torch.Size([32, 51, 768])\n",
      "2025-01-14 11:45:15,905 [INFO] Encoding hidden states for batch 12\n",
      "Processing batches:  38%|███▊      | 12/32 [00:03<00:06,  3.22it/s]2025-01-14 11:45:16,187 [INFO] Batch 13: Hidden states shape: torch.Size([32, 46, 768])\n",
      "2025-01-14 11:45:16,188 [INFO] Encoding hidden states for batch 13\n",
      "Processing batches:  41%|████      | 13/32 [00:03<00:05,  3.31it/s]2025-01-14 11:45:16,478 [INFO] Batch 14: Hidden states shape: torch.Size([32, 44, 768])\n",
      "2025-01-14 11:45:16,480 [INFO] Encoding hidden states for batch 14\n",
      "Processing batches:  44%|████▍     | 14/32 [00:04<00:05,  3.40it/s]2025-01-14 11:45:16,742 [INFO] Batch 15: Hidden states shape: torch.Size([32, 44, 768])\n",
      "2025-01-14 11:45:16,743 [INFO] Encoding hidden states for batch 15\n",
      "Processing batches:  47%|████▋     | 15/32 [00:04<00:04,  3.46it/s]2025-01-14 11:45:17,033 [INFO] Batch 16: Hidden states shape: torch.Size([32, 48, 768])\n",
      "2025-01-14 11:45:17,034 [INFO] Encoding hidden states for batch 16\n",
      "Processing batches:  50%|█████     | 16/32 [00:04<00:04,  3.44it/s]2025-01-14 11:45:17,312 [INFO] Batch 17: Hidden states shape: torch.Size([32, 42, 768])\n",
      "2025-01-14 11:45:17,314 [INFO] Encoding hidden states for batch 17\n",
      "Processing batches:  53%|█████▎    | 17/32 [00:05<00:04,  3.50it/s]2025-01-14 11:45:17,616 [INFO] Batch 18: Hidden states shape: torch.Size([32, 51, 768])\n",
      "2025-01-14 11:45:17,617 [INFO] Encoding hidden states for batch 18\n",
      "Processing batches:  56%|█████▋    | 18/32 [00:05<00:04,  3.42it/s]2025-01-14 11:45:17,930 [INFO] Batch 19: Hidden states shape: torch.Size([32, 53, 768])\n",
      "2025-01-14 11:45:17,931 [INFO] Encoding hidden states for batch 19\n",
      "Processing batches:  59%|█████▉    | 19/32 [00:05<00:03,  3.34it/s]2025-01-14 11:45:18,296 [INFO] Batch 20: Hidden states shape: torch.Size([32, 58, 768])\n",
      "2025-01-14 11:45:18,298 [INFO] Encoding hidden states for batch 20\n",
      "Processing batches:  62%|██████▎   | 20/32 [00:06<00:03,  3.11it/s]2025-01-14 11:45:18,587 [INFO] Batch 21: Hidden states shape: torch.Size([32, 44, 768])\n",
      "2025-01-14 11:45:18,588 [INFO] Encoding hidden states for batch 21\n",
      "Processing batches:  66%|██████▌   | 21/32 [00:06<00:03,  3.25it/s]2025-01-14 11:45:18,857 [INFO] Batch 22: Hidden states shape: torch.Size([32, 45, 768])\n",
      "2025-01-14 11:45:18,859 [INFO] Encoding hidden states for batch 22\n",
      "Processing batches:  69%|██████▉   | 22/32 [00:06<00:02,  3.36it/s]2025-01-14 11:45:19,214 [INFO] Batch 23: Hidden states shape: torch.Size([32, 59, 768])\n",
      "2025-01-14 11:45:19,215 [INFO] Encoding hidden states for batch 23\n",
      "Processing batches:  72%|███████▏  | 23/32 [00:06<00:02,  3.13it/s]2025-01-14 11:45:19,521 [INFO] Batch 24: Hidden states shape: torch.Size([32, 49, 768])\n",
      "2025-01-14 11:45:19,522 [INFO] Encoding hidden states for batch 24\n",
      "Processing batches:  75%|███████▌  | 24/32 [00:07<00:02,  3.20it/s]2025-01-14 11:45:19,828 [INFO] Batch 25: Hidden states shape: torch.Size([32, 53, 768])\n",
      "2025-01-14 11:45:19,830 [INFO] Encoding hidden states for batch 25\n",
      "Processing batches:  78%|███████▊  | 25/32 [00:07<00:02,  3.20it/s]2025-01-14 11:45:20,109 [INFO] Batch 26: Hidden states shape: torch.Size([32, 44, 768])\n",
      "2025-01-14 11:45:20,111 [INFO] Encoding hidden states for batch 26\n",
      "Processing batches:  81%|████████▏ | 26/32 [00:07<00:01,  3.32it/s]2025-01-14 11:45:20,394 [INFO] Batch 27: Hidden states shape: torch.Size([32, 48, 768])\n",
      "2025-01-14 11:45:20,395 [INFO] Encoding hidden states for batch 27\n",
      "Processing batches:  84%|████████▍ | 27/32 [00:08<00:01,  3.37it/s]2025-01-14 11:45:20,665 [INFO] Batch 28: Hidden states shape: torch.Size([32, 43, 768])\n",
      "2025-01-14 11:45:20,666 [INFO] Encoding hidden states for batch 28\n",
      "Processing batches:  88%|████████▊ | 28/32 [00:08<00:01,  3.47it/s]2025-01-14 11:45:20,924 [INFO] Batch 29: Hidden states shape: torch.Size([32, 40, 768])\n",
      "2025-01-14 11:45:20,926 [INFO] Encoding hidden states for batch 29\n",
      "Processing batches:  91%|█████████ | 29/32 [00:08<00:00,  3.59it/s]2025-01-14 11:45:21,179 [INFO] Batch 30: Hidden states shape: torch.Size([32, 40, 768])\n",
      "2025-01-14 11:45:21,180 [INFO] Encoding hidden states for batch 30\n",
      "Processing batches:  94%|█████████▍| 30/32 [00:08<00:00,  3.68it/s]2025-01-14 11:45:21,485 [INFO] Batch 31: Hidden states shape: torch.Size([32, 54, 768])\n",
      "2025-01-14 11:45:21,486 [INFO] Encoding hidden states for batch 31\n",
      "Processing batches:  97%|█████████▋| 31/32 [00:09<00:00,  3.51it/s]2025-01-14 11:45:21,732 [INFO] Batch 32: Hidden states shape: torch.Size([8, 45, 768])\n",
      "2025-01-14 11:45:21,734 [INFO] Encoding hidden states for batch 32\n",
      "Processing batches: 100%|██████████| 32/32 [00:09<00:00,  3.40it/s]\n",
      "2025-01-14 11:45:21,743 [INFO] Total batches processed: 32\n",
      "2025-01-14 11:45:21,746 [INFO] 最大长度:59\n",
      "2025-01-14 11:45:22,175 [INFO] Concatenated batch latents shape: torch.Size([1000, 59, 24576])\n",
      "2025-01-14 11:45:22,176 [INFO] Computing non-zero element counts\n",
      "2025-01-14 11:45:23,252 [INFO] Computing sum of non-zero elements\n",
      "2025-01-14 11:45:23,765 [INFO] Computing mean of non-zero elements\n",
      "2025-01-14 11:45:23,766 [INFO] Selecting top-k indices based on nz_mean\n",
      "2025-01-14 11:45:23,767 [INFO] Top 100 nz_mean values selected.\n",
      "2025-01-14 11:45:23,768 [INFO] Selecting top-k indices based on act_cnt\n",
      "2025-01-14 11:45:23,769 [INFO] Top 100 act_cnt values selected.\n",
      "2025-01-14 11:45:24,181 [INFO] Running model with cache to obtain hidden states\n",
      "Processing batches:   0%|          | 0/32 [00:00<?, ?it/s]2025-01-14 11:45:24,515 [INFO] Batch 1: Hidden states shape: torch.Size([32, 76, 768])\n",
      "2025-01-14 11:45:24,516 [INFO] Encoding hidden states for batch 1\n",
      "Processing batches:   3%|▎         | 1/32 [00:00<00:12,  2.52it/s]2025-01-14 11:45:24,882 [INFO] Batch 2: Hidden states shape: torch.Size([32, 60, 768])\n",
      "2025-01-14 11:45:24,883 [INFO] Encoding hidden states for batch 2\n",
      "Processing batches:   6%|▋         | 2/32 [00:00<00:11,  2.68it/s]2025-01-14 11:45:25,177 [INFO] Batch 3: Hidden states shape: torch.Size([32, 46, 768])\n",
      "2025-01-14 11:45:25,178 [INFO] Encoding hidden states for batch 3\n",
      "Processing batches:   9%|▉         | 3/32 [00:01<00:09,  3.00it/s]2025-01-14 11:45:25,467 [INFO] Batch 4: Hidden states shape: torch.Size([32, 48, 768])\n",
      "2025-01-14 11:45:25,469 [INFO] Encoding hidden states for batch 4\n",
      "Processing batches:  12%|█▎        | 4/32 [00:01<00:08,  3.16it/s]2025-01-14 11:45:25,747 [INFO] Batch 5: Hidden states shape: torch.Size([32, 46, 768])\n",
      "2025-01-14 11:45:25,748 [INFO] Encoding hidden states for batch 5\n",
      "Processing batches:  16%|█▌        | 5/32 [00:01<00:08,  3.30it/s]2025-01-14 11:45:26,042 [INFO] Batch 6: Hidden states shape: torch.Size([32, 50, 768])\n",
      "2025-01-14 11:45:26,044 [INFO] Encoding hidden states for batch 6\n",
      "Processing batches:  19%|█▉        | 6/32 [00:01<00:07,  3.32it/s]2025-01-14 11:45:26,346 [INFO] Batch 7: Hidden states shape: torch.Size([32, 49, 768])\n",
      "2025-01-14 11:45:26,348 [INFO] Encoding hidden states for batch 7\n",
      "Processing batches:  22%|██▏       | 7/32 [00:02<00:07,  3.31it/s]2025-01-14 11:45:26,622 [INFO] Batch 8: Hidden states shape: torch.Size([32, 43, 768])\n",
      "2025-01-14 11:45:26,624 [INFO] Encoding hidden states for batch 8\n",
      "Processing batches:  25%|██▌       | 8/32 [00:02<00:07,  3.42it/s]2025-01-14 11:45:26,916 [INFO] Batch 9: Hidden states shape: torch.Size([32, 46, 768])\n",
      "2025-01-14 11:45:26,917 [INFO] Encoding hidden states for batch 9\n",
      "Processing batches:  28%|██▊       | 9/32 [00:02<00:06,  3.42it/s]2025-01-14 11:45:27,214 [INFO] Batch 10: Hidden states shape: torch.Size([32, 52, 768])\n",
      "2025-01-14 11:45:27,215 [INFO] Encoding hidden states for batch 10\n",
      "Processing batches:  31%|███▏      | 10/32 [00:03<00:06,  3.38it/s]2025-01-14 11:45:27,478 [INFO] Batch 11: Hidden states shape: torch.Size([32, 39, 768])\n",
      "2025-01-14 11:45:27,479 [INFO] Encoding hidden states for batch 11\n",
      "Processing batches:  34%|███▍      | 11/32 [00:03<00:05,  3.53it/s]2025-01-14 11:45:27,800 [INFO] Batch 12: Hidden states shape: torch.Size([32, 58, 768])\n",
      "2025-01-14 11:45:27,803 [INFO] Encoding hidden states for batch 12\n",
      "Processing batches:  38%|███▊      | 12/32 [00:03<00:06,  3.33it/s]2025-01-14 11:45:28,100 [INFO] Batch 13: Hidden states shape: torch.Size([32, 48, 768])\n",
      "2025-01-14 11:45:28,102 [INFO] Encoding hidden states for batch 13\n",
      "Processing batches:  41%|████      | 13/32 [00:03<00:05,  3.36it/s]2025-01-14 11:45:28,393 [INFO] Batch 14: Hidden states shape: torch.Size([32, 47, 768])\n",
      "2025-01-14 11:45:28,394 [INFO] Encoding hidden states for batch 14\n",
      "Processing batches:  44%|████▍     | 14/32 [00:04<00:05,  3.38it/s]2025-01-14 11:45:28,677 [INFO] Batch 15: Hidden states shape: torch.Size([32, 45, 768])\n",
      "2025-01-14 11:45:28,679 [INFO] Encoding hidden states for batch 15\n",
      "Processing batches:  47%|████▋     | 15/32 [00:04<00:04,  3.43it/s]2025-01-14 11:45:29,006 [INFO] Batch 16: Hidden states shape: torch.Size([32, 58, 768])\n",
      "2025-01-14 11:45:29,008 [INFO] Encoding hidden states for batch 16\n",
      "Processing batches:  50%|█████     | 16/32 [00:04<00:04,  3.27it/s]2025-01-14 11:45:29,309 [INFO] Batch 17: Hidden states shape: torch.Size([32, 47, 768])\n",
      "2025-01-14 11:45:29,310 [INFO] Encoding hidden states for batch 17\n",
      "Processing batches:  53%|█████▎    | 17/32 [00:05<00:04,  3.31it/s]2025-01-14 11:45:29,592 [INFO] Batch 18: Hidden states shape: torch.Size([32, 46, 768])\n",
      "2025-01-14 11:45:29,593 [INFO] Encoding hidden states for batch 18\n",
      "Processing batches:  56%|█████▋    | 18/32 [00:05<00:04,  3.37it/s]2025-01-14 11:45:29,904 [INFO] Batch 19: Hidden states shape: torch.Size([32, 53, 768])\n",
      "2025-01-14 11:45:29,905 [INFO] Encoding hidden states for batch 19\n",
      "Processing batches:  59%|█████▉    | 19/32 [00:05<00:03,  3.30it/s]2025-01-14 11:45:30,189 [INFO] Batch 20: Hidden states shape: torch.Size([32, 45, 768])\n",
      "2025-01-14 11:45:30,191 [INFO] Encoding hidden states for batch 20\n",
      "Processing batches:  62%|██████▎   | 20/32 [00:06<00:03,  3.38it/s]2025-01-14 11:45:30,524 [INFO] Batch 21: Hidden states shape: torch.Size([32, 60, 768])\n",
      "2025-01-14 11:45:30,526 [INFO] Encoding hidden states for batch 21\n",
      "Processing batches:  66%|██████▌   | 21/32 [00:06<00:03,  3.21it/s]2025-01-14 11:45:30,805 [INFO] Batch 22: Hidden states shape: torch.Size([32, 42, 768])\n",
      "2025-01-14 11:45:30,807 [INFO] Encoding hidden states for batch 22\n",
      "Processing batches:  69%|██████▉   | 22/32 [00:06<00:02,  3.36it/s]2025-01-14 11:45:31,147 [INFO] Batch 23: Hidden states shape: torch.Size([32, 59, 768])\n",
      "2025-01-14 11:45:31,148 [INFO] Encoding hidden states for batch 23\n",
      "Processing batches:  72%|███████▏  | 23/32 [00:07<00:02,  3.17it/s]2025-01-14 11:45:31,455 [INFO] Batch 24: Hidden states shape: torch.Size([32, 46, 768])\n",
      "2025-01-14 11:45:31,457 [INFO] Encoding hidden states for batch 24\n",
      "Processing batches:  75%|███████▌  | 24/32 [00:07<00:02,  3.23it/s]2025-01-14 11:45:31,736 [INFO] Batch 25: Hidden states shape: torch.Size([32, 45, 768])\n",
      "2025-01-14 11:45:31,737 [INFO] Encoding hidden states for batch 25\n",
      "Processing batches:  78%|███████▊  | 25/32 [00:07<00:02,  3.33it/s]2025-01-14 11:45:32,033 [INFO] Batch 26: Hidden states shape: torch.Size([32, 51, 768])\n",
      "2025-01-14 11:45:32,035 [INFO] Encoding hidden states for batch 26\n",
      "Processing batches:  81%|████████▏ | 26/32 [00:07<00:01,  3.32it/s]2025-01-14 11:45:32,315 [INFO] Batch 27: Hidden states shape: torch.Size([32, 43, 768])\n",
      "2025-01-14 11:45:32,317 [INFO] Encoding hidden states for batch 27\n",
      "Processing batches:  84%|████████▍ | 27/32 [00:08<00:01,  3.40it/s]2025-01-14 11:45:32,613 [INFO] Batch 28: Hidden states shape: torch.Size([32, 52, 768])\n",
      "2025-01-14 11:45:32,615 [INFO] Encoding hidden states for batch 28\n",
      "Processing batches:  88%|████████▊ | 28/32 [00:08<00:01,  3.37it/s]2025-01-14 11:45:32,899 [INFO] Batch 29: Hidden states shape: torch.Size([32, 45, 768])\n",
      "2025-01-14 11:45:32,901 [INFO] Encoding hidden states for batch 29\n",
      "Processing batches:  91%|█████████ | 29/32 [00:08<00:00,  3.42it/s]2025-01-14 11:45:33,204 [INFO] Batch 30: Hidden states shape: torch.Size([32, 53, 768])\n",
      "2025-01-14 11:45:33,206 [INFO] Encoding hidden states for batch 30\n",
      "Processing batches:  94%|█████████▍| 30/32 [00:09<00:00,  3.37it/s]2025-01-14 11:45:33,474 [INFO] Batch 31: Hidden states shape: torch.Size([32, 42, 768])\n",
      "2025-01-14 11:45:33,475 [INFO] Encoding hidden states for batch 31\n",
      "Processing batches:  97%|█████████▋| 31/32 [00:09<00:00,  3.45it/s]2025-01-14 11:45:33,667 [INFO] Batch 32: Hidden states shape: torch.Size([8, 60, 768])\n",
      "2025-01-14 11:45:33,668 [INFO] Encoding hidden states for batch 32\n",
      "Processing batches: 100%|██████████| 32/32 [00:09<00:00,  3.37it/s]\n",
      "2025-01-14 11:45:33,680 [INFO] Total batches processed: 32\n",
      "2025-01-14 11:45:33,683 [INFO] 最大长度:76\n",
      "2025-01-14 11:45:34,224 [INFO] Concatenated batch latents shape: torch.Size([1000, 76, 24576])\n",
      "2025-01-14 11:45:34,225 [INFO] Computing non-zero element counts\n",
      "2025-01-14 11:45:35,471 [INFO] Computing sum of non-zero elements\n",
      "2025-01-14 11:45:36,141 [INFO] Computing mean of non-zero elements\n",
      "2025-01-14 11:45:36,142 [INFO] Selecting top-k indices based on nz_mean\n",
      "2025-01-14 11:45:36,143 [INFO] Top 100 nz_mean values selected.\n",
      "2025-01-14 11:45:36,144 [INFO] Selecting top-k indices based on act_cnt\n",
      "2025-01-14 11:45:36,145 [INFO] Top 100 act_cnt values selected.\n"
     ]
    }
   ],
   "source": [
    "steer_info={}\n",
    "if args.steer=='polite-impolite' or args.steer==\"pos-neg\":\n",
    "    logging.info(args.steer)\n",
    "    text=pos_train_set[\"text\"][:args.data_size]\n",
    "    steer_info[\"pos\"]=get_activation_by_steer(text)\n",
    "    text=neg_train_set[\"text\"][:args.data_size]\n",
    "    steer_info[\"neg\"]=get_activation_by_steer(text)\n",
    "    text=neu_train_set[\"text\"][:args.data_size]\n",
    "    steer_info[\"neu\"]=get_activation_by_steer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if \"cot\" in args.steer:\n",
    "    if args.steer in \"cot-direct\":\n",
    "        texts=all_dataset[\"train\"][\"Q+A\"][:args.data_size]\n",
    "        print(type(texts))\n",
    "        steer_info[\"direct\"]=get_activation_by_steer(texts)\n",
    "        \n",
    "        texts=all_dataset[\"train\"][\"Q+COT_A\"][:args.data_size]\n",
    "        print(type(texts))\n",
    "        steer_info[\"cot\"]=get_activation_by_steer(texts)\n",
    "        # print(texts[123])\n",
    "    else:\n",
    "        raise ValueError(\"????\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos-neg'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.steer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_train_set[\"text\"][:args.data_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_train_set[\"text\"][:args.data_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for steer_key in [\"pos\",\"neu\",\"neg\"]:\n",
    "#     if steer_key == \"pos\":\n",
    "#         selected_set = pos_train_set\n",
    "#     elif steer_key == \"neg\":\n",
    "#         selected_set = neg_train_set\n",
    "#     elif steer_key==\"neu\":\n",
    "#         selected_set = neu_train_set\n",
    "\n",
    "#     texts = selected_set[\"text\"][:args.data_size]\n",
    "#     a=get_activation_by_steer(texts)\n",
    "#     steer_info[steer_key]=a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nz_mean': tensor([ 6.6345, 11.8903,  4.9516,  ...,  0.9575,  1.0143,  0.0000],\n",
       "        grad_fn=<WhereBackward0>),\n",
       " 'nz_cnt': tensor([ 7,  6,  1,  ..., 64, 29,  0])}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steer_info[\"pos\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24576])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steer_info[\"pos\"][\"nz_mean\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# steer_info[\"dif_neg_pos\"]={\"steer\":\"dif_neg_pos\",\"nz_cnt\":steer_info[\"pos\"][\"nz_cnt\"]-steer_info[\"neg\"][\"nz_cnt\"],\"nz_mean\":steer_info[\"pos\"][\"nz_mean\"]-steer_info[\"neg\"][\"nz_mean\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# steer_info[\"dif_neg_pos_relu\"]={\"nz_cnt\":torch.relu(steer_info[\"pos\"][\"nz_cnt\"]-steer_info[\"neg\"][\"nz_cnt\"]),\"nz_mean\":torch.relu(steer_info[\"pos\"][\"nz_mean\"]-steer_info[\"neg\"][\"nz_mean\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# steer_info[\"dif_neg_pos_relu\"],steer_info[\"dif_neg_pos\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceource=\"cot\"\n",
    "target=\"direct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "source=\"pos\"\n",
    "target=\"neg\"\n",
    "# 调整样本正负性在这里调整 从负样本到正样本还是从正样本()到负样本\n",
    "# pos 代表积极情绪\n",
    "# neg 代表消极情绪\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert bool(torch.all((steer_info[\"pos\"][\"nz_mean\"]-steer_info[\"neg\"][\"nz_mean\"])==0))==False,\"数据库读取有问题\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.all(steer_info[target][\"nz_mean\"]>=0),\"所有SAE的激活需要大于d等于0（maybe）\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 泽凯在这里做mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "steer_info[f\"dif_{target}-{source}_relu\"]={\"nz_cnt\":torch.relu(steer_info[target][\"nz_cnt\"]-steer_info[source][\"nz_cnt\"]),\"nz_mean\":torch.relu(steer_info[target][\"nz_mean\"]-steer_info[source][\"nz_mean\"]),\"target_nz_mean\":torch.relu(steer_info[target][\"nz_mean\"])}\n",
    "\"\"\"\n",
    "nz_cnt: 神经元被激活的次数\n",
    "nz_mean: 神经元被激活后的平均值\n",
    "nz_mean_pos: 正样本神经元被激活后的平均值\n",
    "\"\"\"\n",
    "top_k=100\n",
    "_,steer_indices=torch.topk(steer_info[f\"dif_{target}-{source}_relu\"][\"nz_cnt\"],top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# steer_info[f\"dif_{a}-{b}_relu\"]={\"nz_cnt\":torch.relu(steer_info[a][\"nz_cnt\"]-steer_info[b][\"nz_cnt\"]),\"nz_mean\":torch.relu(steer_info[a][\"nz_mean\"]-steer_info[b][\"nz_mean\"])}\n",
    "# top_k=100\n",
    "# steering_vectors,steer_indices=torch.topk(steer_info[f\"dif_{a}-{b}_relu\"][\"nz_cnt\"],top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 假设 steer_info[f\"dif_{b}-{a}_relu\"][\"nz_cnt\"] 是一个 NumPy 数组\n",
    "nz_cnt = steer_info[f\"dif_{target}-{source}_relu\"][\"nz_cnt\"]\n",
    "\n",
    "# 先获取非零元素的索引\n",
    "nz_indices = np.nonzero(nz_cnt)\n",
    "torch.all(nz_cnt == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24576])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steer_info[f\"dif_{target}-{source}_relu\"][\"nz_cnt\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([976, 976, 950, 866, 803, 697, 670, 634, 618, 582, 576, 570, 556, 549,\n",
       "         534, 533, 526, 523, 506, 471, 470, 462, 456, 456, 445, 416, 415, 390,\n",
       "         377, 377, 370, 367, 366, 360, 348, 347, 344, 320, 312, 307, 303, 302,\n",
       "         291, 289, 288, 287, 273, 268, 263, 263, 260, 257, 256, 255, 249, 241,\n",
       "         237, 235, 233, 233, 232, 231, 230, 228, 224, 219, 216, 216, 216, 215,\n",
       "         212, 210, 205, 204, 201, 201, 200, 195, 195, 194, 194, 189, 187, 186,\n",
       "         183, 182, 179, 177, 176, 175, 174, 173, 173, 172, 169, 167, 166, 165,\n",
       "         163, 163]),\n",
       " tensor([14899, 16053, 14950, 18318, 21264,  3857, 17462,  5318, 17539, 11413,\n",
       "         17227,  3290, 23494,   970,  6103, 11779, 21409,  4075,  4283, 18398,\n",
       "         18739,  8847,  1188, 20725, 16383, 21950,  4989, 18604, 22703, 14404,\n",
       "         10516, 16101, 21762,  4978, 15489,  3639,  8272, 23000, 17560, 13611,\n",
       "         10045, 10512,  4886,  8064,  8989,  5842,  5455,  8212, 14168, 10188,\n",
       "         11185,  1167, 18722,  9082, 12797,   222, 18146,  4163,  7417,  8896,\n",
       "          5203, 13252, 21943, 11859,  2282, 13817, 12488, 23355, 20830,  3219,\n",
       "          6861, 24114, 16460,  5303,  9148, 12460,  1374, 10526,  5741,  7467,\n",
       "          6545, 22906, 16318, 17732,  4173,   502, 22288, 23847, 17504,  2211,\n",
       "         15256,  4063,  4994, 15262,  5330, 10282, 14954, 14440,  7383,    98]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_,steer_indices,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14899, 16053, 14950, 18318, 21264,  3857, 17462,  5318, 17539, 11413,\n",
       "        17227,  3290, 23494,   970,  6103, 11779, 21409,  4075,  4283, 18398,\n",
       "        18739,  8847,  1188, 20725, 16383, 21950,  4989, 18604, 22703, 14404,\n",
       "        10516, 16101, 21762,  4978, 15489,  3639,  8272, 23000, 17560, 13611,\n",
       "        10045, 10512,  4886,  8064,  8989,  5842,  5455,  8212, 14168, 10188,\n",
       "        11185,  1167, 18722,  9082, 12797,   222, 18146,  4163,  7417,  8896,\n",
       "         5203, 13252, 21943, 11859,  2282, 13817, 12488, 23355, 20830,  3219,\n",
       "         6861, 24114, 16460,  5303,  9148, 12460,  1374, 10526,  5741,  7467,\n",
       "         6545, 22906, 16318, 17732,  4173,   502, 22288, 23847, 17504,  2211,\n",
       "        15256,  4063,  4994, 15262,  5330, 10282, 14954, 14440,  7383,    98])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steer_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.9328e-02, 3.8317e-02, 2.0146e-02, 2.3163e-02, 2.8212e-02, 0.0000e+00,\n",
       "        5.1741e-02, 4.0420e-02, 0.0000e+00, 2.9353e-02, 2.1048e-02, 0.0000e+00,\n",
       "        0.0000e+00, 1.4081e-02, 7.1191e-02, 4.8021e-02, 1.1438e-04, 0.0000e+00,\n",
       "        4.0448e-02, 4.5769e-02, 7.2892e-02, 4.8401e-03, 0.0000e+00, 9.2183e-03,\n",
       "        0.0000e+00, 3.1627e-02, 1.8980e-02, 2.9644e-02, 1.5619e-02, 1.5609e-02,\n",
       "        0.0000e+00, 2.3832e-02, 2.7588e-02, 0.0000e+00, 4.8306e-02, 1.6971e-02,\n",
       "        4.3642e-02, 0.0000e+00, 2.7315e-02, 2.3624e-02, 0.0000e+00, 0.0000e+00,\n",
       "        2.6217e-02, 6.4675e-02, 6.5274e-03, 0.0000e+00, 6.5705e-02, 3.3928e-03,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.1977e-03, 6.2665e-03,\n",
       "        2.9225e-03, 7.8505e-02, 3.3161e-02, 0.0000e+00, 1.0959e-02, 6.2663e-02,\n",
       "        3.1515e-02, 1.6875e-02, 3.7916e-02, 4.5186e-02, 2.7820e-02, 0.0000e+00,\n",
       "        1.0531e-02, 1.1299e-01, 4.5587e-02, 0.0000e+00, 0.0000e+00, 3.3021e-02,\n",
       "        8.4106e-02, 5.1196e-02, 0.0000e+00, 0.0000e+00, 3.3756e-02, 1.4343e-02,\n",
       "        2.8543e-01, 0.0000e+00, 1.4199e-02, 0.0000e+00, 1.7252e-02, 5.2944e-03,\n",
       "        0.0000e+00, 5.2148e-02, 1.2817e-02, 1.3522e-02, 1.8805e-03, 4.0168e-02,\n",
       "        2.5569e-02, 4.0237e-01, 8.3818e-02, 5.7092e-01, 0.0000e+00, 1.1682e-01,\n",
       "        2.3015e-03, 7.1480e-02, 0.0000e+00, 4.2096e-03],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steer_info[f\"dif_{target}-{source}_relu\"][\"nz_mean\"][steer_indices]# 这里有0,没有负数比较正常\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# steer_info[\"dif_-pos\"][\"nz_mean\"][steer_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14899, 16053, 14950, 18318, 21264,  3857, 17462,  5318, 17539, 11413,\n",
       "        17227,  3290, 23494,   970,  6103, 11779, 21409,  4075,  4283, 18398,\n",
       "        18739,  8847,  1188, 20725, 16383, 21950,  4989, 18604, 22703, 14404,\n",
       "        10516, 16101, 21762,  4978, 15489,  3639,  8272, 23000, 17560, 13611,\n",
       "        10045, 10512,  4886,  8064,  8989,  5842,  5455,  8212, 14168, 10188,\n",
       "        11185,  1167, 18722,  9082, 12797,   222, 18146,  4163,  7417,  8896,\n",
       "         5203, 13252, 21943, 11859,  2282, 13817, 12488, 23355, 20830,  3219,\n",
       "         6861, 24114, 16460,  5303,  9148, 12460,  1374, 10526,  5741,  7467,\n",
       "         6545, 22906, 16318, 17732,  4173,   502, 22288, 23847, 17504,  2211,\n",
       "        15256,  4063,  4994, 15262,  5330, 10282, 14954, 14440,  7383,    98])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steer_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-14 11:57:51,496 [INFO] Computing steering vectors using method: val_mul\n",
      "2025-01-14 11:57:51,508 [INFO] Steering vectors computed with shape: torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "# mean_type=\"dif_mean\"\n",
    "mean_type=\"tar_mean\"\n",
    "def compute_steering_vectors(sae: SAE, indices: Tensor, nz_mean_val: Tensor, method: str = \"val_mul\") -> Tensor:\n",
    "    logging.info(f\"Computing steering vectors using method: {method}\")\n",
    "    if method == \"mean\":\n",
    "        steering_vectors = torch.mean(sae.W_dec[indices], dim=0)\n",
    "    elif method == \"val_mul\":\n",
    "        steering_vectors = torch.zeros(sae.W_dec.shape[1], device=sae.W_dec.device)\n",
    "        for idx in indices:\n",
    "            steering_vectors += nz_mean_val[idx].item() * sae.W_dec[idx]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    logging.info(f\"Steering vectors computed with shape: {steering_vectors.shape}\")\n",
    "    return steering_vectors\n",
    "if mean_type==\"dif_mean\":\n",
    "    delta_matrix=compute_steering_vectors(sae,indices=steer_indices,nz_mean_val=steer_info[f\"dif_{target}-{source}_relu\"][\"nz_mean\"],method=\"val_mul\")\n",
    "else:\n",
    "    delta_matrix=compute_steering_vectors(sae,indices=steer_indices,nz_mean_val=steer_info[f\"dif_{target}-{source}_relu\"][\"target_nz_mean\"],method=\"val_mul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50256, 50256]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.to_tokens(\"<|endoftext|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.6214e-01,  1.0613e+00, -2.0561e+00,  7.6985e-02, -2.2114e-01,\n",
       "        -1.1145e+00,  1.0092e-01,  3.1080e+00, -1.4369e+00, -4.4066e-02,\n",
       "        -2.4463e-01, -5.4689e-01, -3.2318e-01,  2.2005e-01, -1.0918e+00,\n",
       "        -3.3018e-02, -6.1457e-03, -4.1507e-01,  9.3305e-01,  1.2855e+00,\n",
       "         1.3368e+00, -6.3565e-01,  4.4228e-02,  9.8607e-01, -2.0987e-01,\n",
       "         4.9163e-01, -6.4610e-02, -1.7473e+00, -5.9216e-01, -5.9732e-01,\n",
       "        -1.3186e+00, -9.7763e-01,  1.6091e-01, -1.6140e+00,  9.4070e-01,\n",
       "         2.3314e+00,  6.6513e-01,  4.7095e-01, -2.1278e+00,  7.3241e-01,\n",
       "         9.2688e-02, -3.1276e-01, -3.0850e-01,  1.1526e-01, -4.6604e-01,\n",
       "         2.2835e-02,  3.8812e-01, -8.7149e-01, -1.2737e-01, -4.7759e-01,\n",
       "        -9.2506e-01, -1.7304e-01, -1.0689e+00,  9.0440e-01, -1.1013e-01,\n",
       "         2.5222e-01,  8.8352e-01, -1.8106e-02, -1.2686e+00, -4.0374e-01,\n",
       "         7.9995e-01,  8.5598e-01, -4.1962e-01,  3.7982e-01,  1.8890e-01,\n",
       "         5.1330e-01, -8.1306e-01,  3.2115e-02, -7.1212e-01, -8.3575e-01,\n",
       "        -5.4462e-01,  8.7986e-01, -1.3901e-01, -1.2146e+00, -2.1511e-01,\n",
       "        -1.3739e+00, -5.1858e-01,  1.6421e-01, -8.9849e-01, -9.0946e-01,\n",
       "        -1.8775e+00, -7.0653e-01,  1.4620e+00, -1.1587e-01,  2.5670e-01,\n",
       "         4.8903e-01, -1.7800e+00,  3.3453e+00,  1.1732e+00, -5.9974e-01,\n",
       "         2.6189e-01, -7.1271e-01, -3.7458e-01, -1.3684e-01, -3.1339e-01,\n",
       "         5.6076e-01, -7.1380e-02, -6.9841e-01,  1.1125e+00,  1.6227e+00,\n",
       "         7.8083e-01, -3.3653e-01,  7.0053e-03,  3.5148e-01, -9.9458e-01,\n",
       "        -4.3093e-01,  5.6173e-01,  2.3951e-01,  7.2175e-02,  1.8494e-01,\n",
       "         6.1563e-01, -1.0080e+00, -1.7413e-01,  6.2424e-01,  4.1126e-01,\n",
       "         3.1152e-02,  2.7262e-01, -3.3060e-01,  8.6712e-01, -2.5729e-01,\n",
       "        -1.8405e-01, -4.6448e-01, -4.1175e-01,  4.9847e-01,  2.8877e-01,\n",
       "        -5.9981e-01, -3.4002e-01,  7.6493e-01,  9.6655e-02, -1.2180e-01,\n",
       "        -1.4048e+00, -2.1060e-01, -3.1889e-02,  1.1732e+00,  6.0155e-01,\n",
       "         1.1178e-01, -7.9781e-03,  1.7682e-01, -5.6895e-01,  6.3714e-01,\n",
       "         6.0983e-01,  9.4296e-01, -1.0982e-01, -1.1535e+00,  1.0269e+00,\n",
       "        -1.1110e+00, -2.0220e+00,  1.0568e+00,  2.8541e-01, -1.3500e+00,\n",
       "         1.8357e-01, -4.9995e-02,  1.7806e-01,  6.3765e-01, -6.9886e-01,\n",
       "         9.1867e-01,  3.6342e-01, -1.7558e+00,  5.8172e-01,  2.3202e-01,\n",
       "         1.5285e+00, -4.5652e-01, -7.3152e-01,  3.4207e-01, -4.1667e-01,\n",
       "        -1.0813e+00,  2.2247e-01, -9.0551e-01,  1.4163e-01, -9.9642e-01,\n",
       "        -7.2621e-01, -4.3411e-01,  9.8740e-01, -3.9392e-01,  6.4221e-01,\n",
       "         5.3057e-02, -3.9681e-01, -5.2593e-01,  1.5343e+00,  5.5837e-01,\n",
       "        -1.1580e+00, -6.1918e-01, -1.9935e-01, -4.1112e-01,  1.0666e-01,\n",
       "         1.9338e-01,  2.4494e-01, -1.2633e+00, -3.9379e-01, -7.1163e-01,\n",
       "         6.6487e-01,  2.2389e-01, -1.9577e-01, -5.1762e-01,  2.1378e-01,\n",
       "        -1.2186e+00, -1.2834e+00,  7.2237e-01,  2.1767e-01,  8.4781e-01,\n",
       "         5.6732e-01, -7.4655e-01,  1.1795e-01,  1.4060e-01, -4.7544e-01,\n",
       "        -5.5048e-01,  4.2246e-01, -4.3145e-01, -6.8089e-02, -8.6342e-01,\n",
       "        -5.5475e-03,  1.8505e-02,  3.0295e-01, -4.5182e-01, -1.1390e+00,\n",
       "         1.4206e-01, -6.3671e-01, -1.9528e-01, -8.0229e-01, -1.7300e+00,\n",
       "        -1.0491e-01,  7.0226e-01,  2.4803e-01,  3.7956e-01, -1.5713e+00,\n",
       "         2.8848e-02,  5.1656e-01, -6.1320e-01,  8.8014e-01, -7.2840e-01,\n",
       "         2.1338e-01,  6.3356e-03, -3.8885e-01,  4.0748e-01,  8.6848e-01,\n",
       "         4.3140e-01,  1.1037e+00, -4.6375e-01, -5.9241e-01,  4.7617e-03,\n",
       "        -1.3316e+00, -4.6259e-01,  4.1499e-01,  1.6594e+00, -6.6379e-01,\n",
       "        -2.2708e-01,  2.5239e-01,  8.9007e-01, -1.0428e+00, -1.4749e-01,\n",
       "        -6.9731e-01, -5.6403e-02, -2.7095e-01, -5.4011e-01, -3.4124e-01,\n",
       "         9.1402e-02,  9.2757e-02,  3.4135e-01,  3.6392e-01,  3.7408e-01,\n",
       "         6.5203e-01,  6.4482e-01,  4.2162e-01, -8.3708e-01,  1.7473e-01,\n",
       "        -9.9084e-01,  9.3733e+00, -1.5919e+00, -7.0725e-01,  1.1679e-01,\n",
       "         1.1447e-01, -7.7406e-01,  2.4281e-01,  3.5574e-01, -4.7572e-01,\n",
       "        -8.1292e-01,  1.8348e+00, -5.6938e-01,  1.7928e-04,  6.1144e-01,\n",
       "        -7.3973e-01, -2.8367e-01,  3.4929e-01,  2.1758e-01,  4.8534e-01,\n",
       "        -7.6249e-01, -1.4777e+00,  6.9886e-01,  2.7993e+00, -3.2036e-01,\n",
       "        -2.9493e-01,  2.4170e-01, -7.5427e-01,  4.6130e-01, -3.2240e-01,\n",
       "        -7.0439e-01, -7.5298e-02, -6.0211e-01,  1.2948e+00,  3.1179e-01,\n",
       "         6.5660e-01, -9.5681e-02,  8.7410e-02, -3.8855e-01, -8.0772e-01,\n",
       "        -7.5382e-01, -1.0241e-01, -3.8309e-01, -2.1980e-01,  2.7376e-02,\n",
       "        -2.5918e-01, -1.7046e+00,  1.0632e-01,  4.2443e-01,  1.5528e-01,\n",
       "         4.3773e-01, -5.2517e-01, -7.1774e-01, -3.6934e-01, -2.5098e-01,\n",
       "        -5.9138e-01, -7.9071e-01, -2.2984e-01,  1.9467e-02, -1.5867e-01,\n",
       "        -1.1575e-01,  1.3020e+00, -1.0756e+00, -5.1966e-01, -8.9000e-02,\n",
       "        -2.9131e-01, -3.3476e-02, -6.8127e-01, -5.9791e-01,  6.6757e-02,\n",
       "        -2.1419e-01, -9.1084e-01, -6.3360e-01, -4.0909e-01, -3.4648e-01,\n",
       "         3.9860e-02, -7.3980e-01,  6.8588e-01, -1.1373e+00,  1.6011e-01,\n",
       "        -8.9552e-01,  9.6369e-02, -1.5211e-01,  4.7858e-01, -5.1806e-01,\n",
       "        -9.9792e-02,  3.0634e+00,  2.1608e-01, -4.7677e-01,  9.7526e-02,\n",
       "         7.5622e-01, -7.4981e-01, -8.3673e-01, -8.3473e-02, -1.1743e+00,\n",
       "        -1.4149e+00,  8.5381e-01,  1.0935e+00,  1.3640e-01, -2.8327e-01,\n",
       "         8.1343e-01, -3.1049e-01,  1.6778e+00, -8.7977e-01,  5.5931e-01,\n",
       "         9.5101e-02,  2.6866e-01,  6.7152e-01, -9.5183e+00, -8.9696e-01,\n",
       "        -1.0399e+00,  6.5927e-01,  1.7154e+00,  3.2751e-01, -6.5327e-01,\n",
       "        -1.1578e+00,  8.7732e-01,  2.0481e-01,  2.0940e-01, -1.2440e+00,\n",
       "        -1.6405e+00,  5.2264e-01, -4.8821e-01,  3.1152e-01,  1.2385e-01,\n",
       "        -4.8918e-01, -1.1811e+00, -2.0811e-01,  1.8497e-01, -9.8650e-01,\n",
       "        -5.0076e-01, -4.9580e-01,  1.5408e+00, -3.5522e-01, -5.3478e-01,\n",
       "         6.9217e-01,  1.0313e+00,  1.3390e-01, -5.9680e-02,  3.6873e-01,\n",
       "        -5.8753e-01,  1.4074e-01, -1.3591e+00, -6.9461e-01, -3.9405e-01,\n",
       "        -7.6976e-01, -3.8774e-01, -1.3900e+00, -4.2076e-01,  4.5698e-02,\n",
       "        -1.5914e+00, -6.9123e-02,  1.1752e+00, -5.2448e-02, -5.8776e-01,\n",
       "         1.9124e+00,  3.3538e-02,  1.9035e-01,  8.5806e-02, -4.5438e-01,\n",
       "        -3.1127e-01, -6.3286e-01,  7.6609e-01, -2.6679e-01, -9.2942e-01,\n",
       "        -1.7317e-01, -2.5766e-01, -6.5515e-01,  3.0310e-01, -4.1499e-01,\n",
       "         2.7529e-01, -4.9098e-01, -9.2528e-01,  3.2123e-01, -1.4088e+00,\n",
       "        -1.2860e+00, -8.0376e-02, -1.2730e+00, -1.0263e+00, -6.3761e-01,\n",
       "         5.0859e-01, -2.7718e-01, -8.9982e-02, -5.5366e-01, -5.3388e-01,\n",
       "        -1.2426e+00,  3.8269e-01, -1.0890e+00, -6.3606e-02, -8.7201e-01,\n",
       "        -5.8967e-01, -1.2107e+00, -3.7839e-01,  6.2969e-02, -5.5116e+00,\n",
       "         1.0739e-01, -1.3853e+00,  5.0315e-02, -8.4180e-02, -3.6141e-01,\n",
       "         1.9907e-01,  2.9288e-01, -6.5752e-01, -5.8052e-01, -3.6900e-01,\n",
       "        -4.1835e-01,  1.6906e-01, -1.0054e+00,  5.2748e-02,  4.3676e-01,\n",
       "         1.1609e+00, -1.2374e+00,  7.8382e-01, -2.5165e-01, -2.1164e-01,\n",
       "         3.4855e+01, -2.6810e+00,  1.1303e-01, -1.4217e+00, -1.0648e+00,\n",
       "        -2.8753e-02,  9.2622e-04,  4.5374e-04,  1.7530e-01,  6.2454e-01,\n",
       "         5.5520e-01, -4.0647e-01,  2.0348e-01, -2.3695e-01, -1.7052e+00,\n",
       "         1.1067e+00, -1.1186e+00, -3.2772e+00,  4.1270e-01,  1.5292e-01,\n",
       "        -1.0367e+00, -2.1342e-01, -4.8680e-01, -8.7016e-01, -6.8271e-01,\n",
       "         1.3836e-01, -3.0892e-01,  1.1047e-01, -5.6493e-01, -1.5084e+00,\n",
       "        -2.4734e-01,  7.0924e-01,  1.3091e+00,  4.3669e-01, -7.0601e-01,\n",
       "        -4.0839e-01,  1.1865e+00, -4.2018e-01, -1.4975e-01, -1.0958e+00,\n",
       "         2.8758e-01, -9.5801e-02, -2.1212e-03, -6.2043e-02, -8.8867e-01,\n",
       "         1.4821e-01,  1.9880e+00, -1.1934e+00, -1.2452e+00,  1.7475e-01,\n",
       "         8.9009e-01, -3.9088e-01,  5.1995e-01,  1.0831e-01, -1.7430e+00,\n",
       "        -7.4274e-01, -4.6487e-01, -5.3048e-01, -1.1910e+00, -8.2417e-01,\n",
       "         5.4136e-01,  7.5251e-01,  7.2427e-02,  9.7076e-01, -4.8578e-01,\n",
       "         5.9469e-01,  5.3931e-01, -5.1393e-01,  3.0162e-01,  2.9088e-01,\n",
       "         7.1565e-01,  2.9828e-01, -7.8222e-01,  8.8363e-01, -8.8488e-01,\n",
       "        -8.4103e-01, -9.3230e-01,  1.4608e-01, -5.1190e-01, -7.8632e-01,\n",
       "        -1.7821e+00, -7.3655e-01, -1.5319e-01, -5.5985e-01, -2.2443e-01,\n",
       "         8.5083e-02, -1.5218e-01,  1.2431e-01, -7.3490e-02,  2.7490e-01,\n",
       "        -4.4203e-01, -2.3355e-02,  6.5820e-02, -3.1322e-01, -7.2251e-01,\n",
       "         1.8604e+00, -7.6461e-01,  2.6899e-01, -1.4897e-02,  2.9716e-01,\n",
       "        -3.6302e-01, -8.4787e-03, -9.8115e-01,  6.6510e-02,  7.3338e-01,\n",
       "        -1.3283e+00, -2.5821e-01,  1.3657e-01,  9.4105e-01,  9.7438e-01,\n",
       "         6.9034e-02, -3.1811e-01, -9.8586e-01,  5.7475e-01, -3.9900e-01,\n",
       "        -1.4990e+00, -3.0143e-01, -5.3605e-01, -3.2785e-01, -6.3772e-01,\n",
       "         3.0833e-01, -2.8879e-01, -1.3228e-01, -7.2793e-02,  6.8588e-01,\n",
       "         5.3570e-01,  2.9699e-01, -5.8107e-01, -1.5228e-01,  2.7229e-01,\n",
       "         4.4138e-01, -8.7694e-01, -2.6644e-01,  7.2867e-01,  1.5207e-01,\n",
       "        -4.7182e-01, -8.0666e-02,  6.2151e-01, -3.5342e-01,  6.2723e-01,\n",
       "         1.5403e+00, -2.1193e-01, -8.9465e-01,  1.5909e+00, -1.2423e+00,\n",
       "         9.3989e-02,  1.0330e-01, -9.6541e-01,  2.7225e-01, -5.4018e-01,\n",
       "        -2.5789e-01,  1.5265e-02,  3.3801e-01, -5.5150e-01, -4.0238e-01,\n",
       "         1.9616e-01,  2.4825e-01, -6.6444e-01, -7.8030e-01,  4.9200e-01,\n",
       "        -2.5450e+00,  1.5203e+00,  2.1516e+00, -5.8437e-01, -4.6646e-01,\n",
       "        -1.0201e+00,  4.4974e-01, -6.2405e-01,  1.3555e-01,  4.1376e-01,\n",
       "        -3.3737e-01,  2.3418e-01,  2.8605e-01, -9.9380e-01,  3.8569e-03,\n",
       "         1.3879e-01, -1.2921e+00, -7.8572e-02, -3.0667e-01, -2.1343e+00,\n",
       "         7.8495e-01, -1.3092e+00, -9.5212e-01, -1.1185e+00, -1.8833e-01,\n",
       "         8.3367e-01, -2.6775e-01, -7.5806e-01, -1.0392e+00, -9.3643e-01,\n",
       "        -4.6711e-01,  4.2722e-01, -3.3990e-01, -8.1974e-01, -1.6635e+00,\n",
       "         2.8757e-01,  3.6023e-01,  7.6366e-02,  9.5074e-01, -7.7214e-01,\n",
       "        -2.3330e+00,  1.0322e+00,  5.4446e-01,  6.9573e-01,  9.9197e-02,\n",
       "        -1.7682e+00,  2.1494e-01, -4.0238e-01,  3.6029e-01, -4.5152e-01,\n",
       "         9.2139e-01, -2.2338e-01,  3.5382e-01,  2.6560e-01, -3.4781e-01,\n",
       "         6.8419e-02, -3.9824e-01, -2.9864e-01,  2.3766e-01,  5.4736e-01,\n",
       "        -1.2708e+00, -5.0241e-01, -1.1735e+00, -2.6316e-01,  5.7464e-01,\n",
       "         9.0462e-01,  1.2101e-01, -2.6173e-01, -6.7200e-01,  5.3758e-01,\n",
       "         3.3082e-01,  1.3396e-01, -2.7151e-01,  2.0154e-01, -2.7552e-02,\n",
       "        -1.3182e+00,  3.6340e-01, -6.2987e-01, -4.4785e-01, -7.4880e-02,\n",
       "         5.1524e-02, -2.8799e-01, -8.9838e-02, -3.1166e-01,  1.4516e+00,\n",
       "         4.9174e-02, -1.5385e+00, -1.0350e+00,  6.7072e-01, -1.4411e+00,\n",
       "        -5.5981e-01, -1.3666e-01, -1.3880e+00,  3.2819e-01, -1.2055e+00,\n",
       "         4.4521e-01, -7.9083e-02,  1.0663e-01,  1.0047e+00, -1.0776e+00,\n",
       "        -2.2458e-01, -7.7955e-01, -1.1798e+00,  6.8898e-01, -8.4233e-01,\n",
       "        -7.3299e-01, -9.4864e-01, -5.3286e-01, -1.1437e+00,  1.1115e+00,\n",
       "        -5.6215e-01,  6.4756e-01,  5.3657e-01, -2.8270e-01, -8.6476e-01,\n",
       "        -3.4373e-01, -7.7135e+00, -1.2665e+00, -9.1331e-02, -7.7653e-01,\n",
       "        -5.2655e-01, -1.6232e-01,  1.2938e+00,  2.9040e-02,  1.8694e-01,\n",
       "        -7.1535e-01,  4.4159e-01, -4.5073e-01], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_matrix #理论上这里有正有负比较正常"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 这里得到的就是delta_matricx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'blocks.6.hook_resid_pre'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae.cfg.hook_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'blocks.6.hook_resid_post'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"blocks.{args.layer}.hook_resid_post\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0111, 0.0796, 0.3247, 0.7548, 1.0000, 0.7548, 0.3247, 0.0796])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0095, 0.0680, 0.2774, 0.6451])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "def half_gaussian_kernel(half_len):\n",
    "    # 设置均值和标准差\n",
    "    cov_len=2*half_len\n",
    "    mean = cov_len // 2  # 正态分布的均值\n",
    "    std = cov_len / 6    # 设置标准差，可以根据需要调整\n",
    "\n",
    "    # 创建正态分布\n",
    "    x = torch.arange(cov_len, dtype=torch.float32)\n",
    "    kernel = torch.exp(-0.5 * ((x - mean) / std) ** 2)\n",
    "    print(kernel)\n",
    "\n",
    "    # 仅保留正态分布的前半部分（右侧值设置为0）\n",
    "    kernel[int(cov_len // 2):] = 0  # 保留前半部分，右半部分置为零\n",
    "\n",
    "    # 归一化，确保总和为 1\n",
    "    kernel = kernel / kernel.sum()\n",
    "    return kernel[:half_len]\n",
    "\n",
    "gauss=half_gaussian_kernel(4)\n",
    "gauss\n",
    "# k_gau=torch.cat([gauss, torch.tensor([0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0095, 0.0680, 0.2774, 0.6451])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gauss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 3]), torch.Size([6]))"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import einops\n",
    "test=torch.ones(2, 6,3)\n",
    "# test=einops.rearrange(test,\"b s d->b d s\")\n",
    "test.shape,k_gau.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_gau=einops.repeat(k_gau,\"s -> b s h\",b=2,h=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0070, 0.0070, 0.0070],\n",
       "          [0.0354, 0.0354, 0.0354],\n",
       "          [0.1247, 0.1247, 0.1247],\n",
       "          [0.3067, 0.3067, 0.3067],\n",
       "          [0.5263, 0.5263, 0.5263],\n",
       "          [0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0070, 0.0070, 0.0070],\n",
       "          [0.0354, 0.0354, 0.0354],\n",
       "          [0.1247, 0.1247, 0.1247],\n",
       "          [0.3067, 0.3067, 0.3067],\n",
       "          [0.5263, 0.5263, 0.5263],\n",
       "          [0.0000, 0.0000, 0.0000]]]),\n",
       " tensor([[[1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.]]]))"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test*re_gau,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-14 15:54:00,315 [INFO] Example prompt:  Aha, we have good result!\n",
      "2025-01-14 15:54:00,317 [INFO] Generating texts **without** steering... \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96fbe7c273e442c5a5cd3eeadedbc99b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-14 15:54:02,039 [INFO] Generated Text: 1:\n",
      " Aha, we have good result!\n",
      "\n",
      "We are now in the final stages of our journey to get to the next level. We will be able to start building up our team and build a strong foundation for future success. We will also be able to take on more challenges and make\n",
      "2025-01-14 15:54:02,040 [INFO] Generated Text: 2:\n",
      " Aha, we have good result!\n",
      "\n",
      "I'm sure you've heard of the \"Giant's Song\" by The Beatles. It was a song about a giant who had to be saved from death by his own hand. It was also about the way that humans are created and\n",
      "2025-01-14 15:54:02,040 [INFO] Generated Text: 3:\n",
      " Aha, we have good result!\n",
      "\n",
      "\"We are going to be the first team in the league to win a title. We will be the first team in Europe to win a European Cup. We will be able to play at home and play against other teams.\"<|endoftext|><|endoftext|><|endoftext|>\n",
      "2025-01-14 15:54:02,040 [INFO] 干预之后的结果\n",
      "2025-01-14 15:54:02,041 [INFO] 干预方向pos->neg,礼貌任务下，neg=impolite，情感任务下 pos=积极情感\n",
      "2025-01-14 15:54:02,041 [INFO] ** Generating texts with steering... Target **\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a75a7e536c534f81beb646522078b808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-14 15:54:02,075 [INFO] 干预类型：高斯\n",
      "2025-01-14 15:54:02,076 [INFO] 干预矩阵: tensor([[[ -0.1662,   0.4870,  -0.9435,  ...,  -0.3283,   0.2026,  -0.2068],\n",
      "         [ -0.5484,   1.6071,  -3.1135,  ...,  -1.0832,   0.6687,  -0.6825],\n",
      "         [ -1.5059,   4.4132,  -8.5501,  ...,  -2.9746,   1.8363,  -1.8743],\n",
      "         ...,\n",
      "         [ -6.5456,  19.1824, -37.1638,  ..., -12.9295,   7.9816,  -8.1466],\n",
      "         [-10.3602,  30.3616, -58.8221,  ..., -20.4646,  12.6331, -12.8944],\n",
      "         [-13.6465,  39.9923, -77.4807,  ..., -26.9561,  16.6404, -16.9845]],\n",
      "\n",
      "        [[ -0.1662,   0.4870,  -0.9435,  ...,  -0.3283,   0.2026,  -0.2068],\n",
      "         [ -0.5484,   1.6071,  -3.1135,  ...,  -1.0832,   0.6687,  -0.6825],\n",
      "         [ -1.5059,   4.4132,  -8.5501,  ...,  -2.9746,   1.8363,  -1.8743],\n",
      "         ...,\n",
      "         [ -6.5456,  19.1824, -37.1638,  ..., -12.9295,   7.9816,  -8.1466],\n",
      "         [-10.3602,  30.3616, -58.8221,  ..., -20.4646,  12.6331, -12.8944],\n",
      "         [-13.6465,  39.9923, -77.4807,  ..., -26.9561,  16.6404, -16.9845]],\n",
      "\n",
      "        [[ -0.1662,   0.4870,  -0.9435,  ...,  -0.3283,   0.2026,  -0.2068],\n",
      "         [ -0.5484,   1.6071,  -3.1135,  ...,  -1.0832,   0.6687,  -0.6825],\n",
      "         [ -1.5059,   4.4132,  -8.5501,  ...,  -2.9746,   1.8363,  -1.8743],\n",
      "         ...,\n",
      "         [ -6.5456,  19.1824, -37.1638,  ..., -12.9295,   7.9816,  -8.1466],\n",
      "         [-10.3602,  30.3616, -58.8221,  ..., -20.4646,  12.6331, -12.8944],\n",
      "         [-13.6465,  39.9923, -77.4807,  ..., -26.9561,  16.6404, -16.9845]]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0111, 0.0367, 0.1007, 0.2301, 0.4376, 0.6926, 0.9123, 1.0000, 0.9123,\n",
      "        0.6926, 0.4376, 0.2301, 0.1007, 0.0367])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-14 15:54:03,807 [INFO] Generated Text: 1:\n",
      " Aha, we have good result!\n",
      "\n",
      "But I'm not sure if it's because of the fact that I'm a fan of this series or because I've never read it. It's just that there are so many things in this book that make me want to read more.\n",
      "2025-01-14 15:54:03,808 [INFO] Generated Text: 2:\n",
      " Aha, we have good result!\n",
      "\n",
      "The story of the film is that of a young man who has been kidnapped by his father and taken to a mysterious place. The film begins with the boy being taken to an unknown location where he is forced to watch as his father's body\n",
      "2025-01-14 15:54:03,808 [INFO] Generated Text: 3:\n",
      " Aha, we have good result!\n",
      "\n",
      "The game is a simple puzzle with no branching. The player must solve the puzzles to get the most out of it. It's a fun game and I'm sure you'll enjoy it too!<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# # Define steering hook\n",
    "# steering_on = True  # This will be toggled in run_generate\n",
    "# alpha = args.alpha\n",
    "# method = args.method  # Store method for clarity\n",
    "from functools import partial\n",
    "steer_cnt=0\n",
    "\n",
    "\n",
    "def steering_hook(resid_pre, hook, steer_on, alpha, steer_type=\"last\"):\n",
    "    # 如果 seq_len 只有 1，则直接返回，不进行操作\n",
    "    d_m=torch.clone(delta_matrix)\n",
    "    s = resid_pre[:, :-1, :].shape[1]\n",
    "    b=resid_pre[:, :-1, :].shape[0]\n",
    "    h=resid_pre[:, :-1, :].shape[2]\n",
    "    if resid_pre.shape[1] == 1:\n",
    "        return\n",
    "    # 判断是否进行干预\n",
    "    if steer_on:\n",
    "        if steer_type == \"last\":\n",
    "            # 对最后一个token前的部分应用干预，使用给定的 delta_matrix\n",
    "            d_m_repeat=einops.repeat(d_m,\"h -> b s h\",b=b,s=s)\n",
    "            resid_pre[:, :-1, :] += alpha * d_m_repeat\n",
    "            \n",
    "            logging.info(f\"干预类型：last\")\n",
    "            logging.info(f\"干预矩阵: {alpha * d_m_repeat}\")\n",
    "        elif steer_type == \"gaussian\":\n",
    "            # 使用高斯卷积对输入进行干预\n",
    "            # s_idx=-1\n",
    "\n",
    "            h_gauss = half_gaussian_kernel(s)  # 获取高斯卷积核\n",
    "            # k_gauss=torch.cat([h_gauss, torch.tensor([0])])\n",
    "            k_gauss=h_gauss\n",
    "            k_gau_repeat=einops.repeat(k_gauss,\"s -> b s h\",b=b,h=h)\n",
    "            d_m_repeat=einops.repeat(d_m,\"h -> b s h\",b=b,s=s)\n",
    "            # 根据卷积结果更新 resid_pre（注意：保留其他维度不变）,逐一元素相乘\n",
    "            resid_pre[:, :-1, :] += alpha * d_m_repeat* k_gau_repeat\n",
    "            logging.info(f\"干预类型：高斯\")\n",
    "            logging.info(f\"干预矩阵: {alpha * d_m_repeat* k_gau_repeat}\")\n",
    "        else:\n",
    "            raise ValueError(\"Unknown steering type\")\n",
    "\n",
    "\n",
    "        # elif steer_type==\"last2\":\n",
    "        #     resid_pre[:, :-2, :] += args.alpha * steering_vectors\n",
    "        # elif steer_type==\"gaussian\":\n",
    "        #     # 高斯卷积的方式放缩干预矩阵，\n",
    "        #     # 这里需要一个高斯核，然后对steering_vectors进行卷积\n",
    "        #     gaussian_kernel = torch.tensor([1,2,1])\n",
    "        #     steering_vectors = torch.conv1d(steering_vectors, gaussian_kernel, padding=1)\n",
    "        #     resid_pre[:, :-1, :] += args.alpha * steering_vectors\n",
    "        # else:\n",
    "        #     raise ValueError(\"Unknown steering type\")\n",
    "        # 修改这里的干预方式，增加干预的选择，例如从倒数第一个token开始干预，或者从倒数第二个token开始干预，或者使用高斯卷积的方式放缩干预矩阵，这里是干预调整的关键，很有意思的是，如果提前干预效果会更好更连贯，还没尝试高斯卷积的方法\n",
    "\n",
    "def hooked_generate(prompt_batch, fwd_hooks=[], seed=None, **kwargs):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    with model.hooks(fwd_hooks=fwd_hooks):\n",
    "        tokenized = model.to_tokens(prompt_batch,prepend_bos=False)\n",
    "        result = model.generate(\n",
    "            stop_at_eos=True,  # avoids a bug on MPS\n",
    "            input=tokenized,\n",
    "            max_new_tokens=50,\n",
    "            do_sample=True,\n",
    "            **kwargs,\n",
    "        )\n",
    "    return result\n",
    "\n",
    "def run_generate(example_prompt,sampling_kwargs,steer_on,alpha,steer_type=\"last\",repeat_num=3,show_res=False):\n",
    "    model.reset_hooks()\n",
    "    if steer_on:\n",
    "        steering_hook_fn=partial(steering_hook,steer_on=steer_on,alpha=alpha,steer_type=steer_type)\n",
    "        editing_hooks = [(f\"blocks.{args.layer}.hook_resid_post\", steering_hook_fn)]\n",
    "    else:\n",
    "        editing_hooks=[]\n",
    "    res = hooked_generate(\n",
    "        [example_prompt] * repeat_num, editing_hooks, seed=None, **sampling_kwargs\n",
    "    )\n",
    "\n",
    "    # Print results, removing the ugly beginning of sequence token\n",
    "    res_str = model.to_string(res[:, :])\n",
    "    # print((\"\\n\\n\" + \"-\" * 80 + \"\\n\\n\").join(res_str))\n",
    "    # generated_texts = res_str\n",
    "    if show_res:\n",
    "        for idx, text in enumerate(res_str):\n",
    "            logging.info(f\"Generated Text: {idx+1}:\\n{text}\")\n",
    "        \n",
    "    return res_str\n",
    "\n",
    "# Define sampling parameters\n",
    "sampling_kwargs = dict(temperature=1.0, top_p=0.1, freq_penalty=1.0)\n",
    "\n",
    "# Example prompt from the selected set\n",
    "example_prompt = \" Aha, we have good result!\"\n",
    "# example_prompt=model.tokenizer\n",
    "# print( all_dataset[\"val\"][\"prompt\"][:3],all_dataset[\"val\"][\"A\"][:3])\n",
    "logging.info(f\"Example prompt: {example_prompt}\")\n",
    "\n",
    "# Generate without steering\n",
    "steer_on = False\n",
    "alpha = 0\n",
    "logging.info(\"Generating texts **without** steering... \")\n",
    "generated_texts_no_steer = run_generate(example_prompt, sampling_kwargs,steer_on=steer_on,alpha=alpha,show_res=True)\n",
    "logging.info(\"干预之后的结果\")\n",
    "# bef,aft=args.steer.split(\"-\")\n",
    "logging.info(f\"干预方向{source}->{target},礼貌任务下，neg=impolite，情感任务下 pos=积极情感\")\n",
    "# Generate with steering\n",
    "steer_on = True\n",
    "alpha = 100\n",
    "# alpha=args.aplha\n",
    "logging.info(\"** Generating texts with steering... Target **\")\n",
    "generated_texts_with_steer = run_generate(\n",
    "    example_prompt, \n",
    "    sampling_kwargs,\n",
    "    steer_on=steer_on,\n",
    "    alpha=alpha,\n",
    "    steer_type=\"gaussian\",\n",
    "    show_res=True)\n",
    "\n",
    "# Combine generated texts\n",
    "# all_generated_texts = generated_texts_no_steer + generated_texts_with_steer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 理论上来讲，\n",
    "* 不礼貌的输出应该有很多疑问句？例如what？ ha？why？\n",
    "* 而礼貌的输出应该有很多正常的词语\n",
    "* 积极情感和不积极情感同理\n",
    "* 从目前的实验来看，负向情感干预+礼貌情感干预表现比较好，可以拿这个做可解释性\n",
    "* 频率很重要，我选取的latents选了前100频次的激活神经元\n",
    "* 如果对[0:-1]的区间进行干预，效果异常优秀，生成比较连贯，但是如果对[-1:length]的区间进行干预，效果就很差，生成的词语很零碎"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 下面进行的是扭转实验，使用prompt对模型进行诱导，再进行转向"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ckqsudo/code2024/0dataset/baseline-acl/data/prompts/sentiment_prompts-10k'"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.prompt_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-14 15:08:28,571 [INFO] Loading prompt_path from /home/ckqsudo/code2024/0dataset/baseline-acl/data/prompts/sentiment_prompts-10k\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    neg: Dataset({\n",
      "        features: ['md5_hash', 'prompt', 'continuation', 'num_positive'],\n",
      "        num_rows: 2500\n",
      "    })\n",
      "    pos: Dataset({\n",
      "        features: ['md5_hash', 'prompt', 'continuation', 'num_positive'],\n",
      "        num_rows: 2500\n",
      "    })\n",
      "    neu: Dataset({\n",
      "        features: ['md5_hash', 'prompt', 'continuation', 'num_positive'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def load_and_prepare_sentiment_prompts(prompt_path:str,seed:int,num_samples:int):\n",
    "    logging.info(f\"Loading prompt_path from {prompt_path}\")\n",
    "    data_files = {\"neg\": \"negative_prompts.jsonl\", \"pos\": \"positive_prompts.jsonl\",\"neu\":\"neutral_prompts.jsonl\"}\n",
    "    \n",
    "    prompts= load_dataset(\"/home/ckqsudo/code2024/0refer_ACL/LM-Steer/data/data/prompts/sentiment_prompts-10k\",data_files=data_files)\n",
    "    print(prompts)\n",
    "    return prompts\n",
    "prompts=load_and_prepare_sentiment_prompts(args.prompt_path,args.seed,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['md5_hash', 'prompt', 'continuation', 'num_positive'],\n",
       "    num_rows: 2500\n",
       "})"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts[\"pos\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts[\"pos\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experience the lifelike thrill of virtual\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "run_generate() missing 2 required positional arguments: 'steer_on' and 'alpha'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[245], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# logging.info(\"Generating texts **without** steering... \")\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m generated_texts_no_steer \u001b[38;5;241m=\u001b[39m \u001b[43mrun_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# logging.info(\"干预之后的结果\")\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# bef,aft=args.steer.split(\"-\")\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# logging.info(f\"干预方向{source}->{target},礼貌任务下，neg=impolite，情感任务下 pos=积极情感\")# Generate with steering\u001b[39;00m\n\u001b[1;32m     22\u001b[0m steer_on \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: run_generate() missing 2 required positional arguments: 'steer_on' and 'alpha'"
     ]
    }
   ],
   "source": [
    "\n",
    "import copy\n",
    "# Example prompt from the selected set\n",
    "res=[]\n",
    "params={}\n",
    "params[\"params\"]=vars(args)\n",
    "params[\"alpha\"]=100\n",
    "res.append(params)\n",
    "\n",
    "no_steer_res=[]\n",
    "steer_res=[]\n",
    "\n",
    "prompt_type=\"pos\"\n",
    "assert prompt_type!=target,\"prompt和转向的方向是一致的\"\n",
    "\n",
    "for idx,item in tqdm(enumerate(prompts[prompt_type][1000:2000])):\n",
    "    prompt_=item[\"prompt\"][\"text\"]\n",
    "    label=prompts[prompt_type]['label'][idx]\n",
    "    print(prompt_)\n",
    "    # 没转向的结果\n",
    "    steer_on = False\n",
    "    alpha = 0\n",
    "    # logging.info(\"Generating texts **without** steering... \")\n",
    "    generated_texts_no_steer = run_generate(\n",
    "        prompt_, \n",
    "        sampling_kwargs,\n",
    "        steer_on=steer_on,\n",
    "        alpha=alpha,\n",
    "        repeat_num=2,\n",
    "        steer_type=\"gaussian\",\n",
    "        show_res=True)\n",
    "    no_steer_item=copy.deepcopy(prompt_)\n",
    "    \n",
    "    # 转向的结果\n",
    "    steer_on = True\n",
    "    alpha = 100\n",
    "    \n",
    "    generated_texts_with_steer = run_generate(prompt_, sampling_kwargs,steer_on=steer_on,\n",
    "    alpha=alpha,\n",
    "    steer_type=\"gaussian\",\n",
    "    repeat_num=3,\n",
    "    show_res=True)\n",
    "    item[\"no_steer\"]=generated_texts_no_steer\n",
    "    item[\"steer\"]=generated_texts_with_steer\n",
    "    res.append(copy.deepcopy(item))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"/home/ckqsudo/code2024/CKQ_ACL2024/Control_Infer/SAE-simple/src/evaluations/res.json\",mode=\"w\",encoding=\"utf-8\") as res_f:\n",
    "    res_f.write(json.dumps(res,ensure_ascii=False))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
